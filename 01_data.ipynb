{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad6909d-00fb-45d5-ab8e-ff33648bd6a0",
   "metadata": {},
   "source": [
    "# Pretraining on .py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86871b90-fe77-4290-89ed-a5329c7f8d17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from git.repo import Repo\n",
    "from pathlib import Path\n",
    "from rich.markdown import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c4a3af-d593-4416-84ab-3538bfabf408",
   "metadata": {},
   "source": [
    "Let's clone vllm: \n",
    "\n",
    "```bash\n",
    "git clone https://github.com/vllm-project/vllm.git\n",
    "```\n",
    "\n",
    "\n",
    "let's bring some text in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cec3c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_path = Path(\"vllm/\")\n",
    "vllm_repo = Repo(vllm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29d53b88-5f51-480f-8b70-e32ed1bfaa7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bc0644574ca12d754a031596bdcfe8e1f0e6ab39\n"
     ]
    }
   ],
   "source": [
    "# Get the latest commit\n",
    "last_commit = vllm_repo.commit()\n",
    "\n",
    "# Print the commit message\n",
    "print(last_commit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d707080-cca0-4341-91b8-a3786c5e8db5",
   "metadata": {},
   "source": [
    "Naively we can bring all MarkDown files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbe2c9ed-3dc0-4a97-b54d-36133447d9cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_files(directory, extension=\"*.py\"):\n",
    "    \"Find all files of a given `extension` in a directory and return their content and path\"\n",
    "    md_files = []\n",
    "    for file in Path(directory).rglob(extension):\n",
    "        with open(file, 'r', encoding='utf-8') as md_file:\n",
    "            content = md_file.read()\n",
    "        md_files.append((file.relative_to(directory), content))\n",
    "    return md_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97cdd38b-f6c2-477e-b5d4-d1eed6d98fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import copy\n",
      "import time\n",
      "from functools import partial\n",
      "from typing import TYPE_CHECKING, Any, Iterable, List, Optional, Tuple, Union\n",
      "\n",
      "from vllm.config import (CacheConfig, ModelConfig, ParallelConfig,\n",
      "                         SchedulerConfig)\n",
      "from vllm.core.scheduler import Scheduler, SchedulerOutputs\n",
      "from vllm.engine.arg_utils import EngineArgs\n",
      "from vllm.engine.ray_utils import RayWorker, initialize_cluster, ray\n",
      "from vllm.logger import init_logger\n",
      "from vllm.outputs import RequestOutput\n",
      "from vllm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(96, None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_files = find_files(vllm_path)\n",
    "\n",
    "file = random.choice(py_files)\n",
    "\n",
    "len(py_files), print(file[1][0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79897931-ddac-4423-b34e-f204fd824994",
   "metadata": {},
   "source": [
    "We should stack the file with some metadata, for instance, the path where the files is coming from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4e621d8-a3ce-4ef2-9520-8d41cc7ba98f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff = \"\"\"<<Begin file>>\n",
    "Path: {path}\n",
    "---------\n",
    "Content:\n",
    "{content}\n",
    "<<End File>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95db238a-6479-46d2-917e-a3316559c0cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<Begin file>>\n",
      "Path: vllm/engine/llm_engine.py\n",
      "---------\n",
      "Content:\n",
      "import copy\n",
      "import time\n",
      "from functools import partial\n",
      "from typing import TYPE_CHECKING, Any, Iterable, List, Optional, Tuple, Union\n",
      "\n",
      "from vllm.config import (CacheConfig, ModelConfig, ParallelConfig,\n",
      "                         SchedulerConfig)\n",
      "from vllm.core.scheduler import Scheduler, SchedulerOutputs\n",
      "from vllm.engine.arg_utils import EngineArgs\n",
      "from vllm.engine.ray_utils import RayWorker, initialize_cluster, ray\n",
      "from vllm.logger import init_logger\n",
      "from vllm.outputs import RequestOutput\n",
      "from vllm\n",
      "<<End File>>\n"
     ]
    }
   ],
   "source": [
    "print(stuff.format(path=file[0], content=file[1][0:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c54825f-2838-4c7d-9ba6-78c069f7cd00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93826f174df403eb00e9e8a14e1c848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "with open('vllm_python.jsonl', 'w') as json_file:\n",
    "    for path, content in tqdm(py_files):\n",
    "        data = stuff.format(path=path, content=content)\n",
    "        json.dump({\"text\":data}, json_file)\n",
    "        json_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85057fc",
   "metadata": {},
   "source": [
    "## How much data do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08c1ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57a7f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "OS_MODEL = \"codellama/CodeLlama-7b-Python-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3217af43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(OS_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3f9ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = tokenizer.encode(\"def hello_world():\\n\\tprint('Hello World!')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2eb411b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af772119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 822, 22172, 29918, 11526, 7295, 13, 12, 2158, 877, 10994, 2787, 29991, 1495]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d198044",
   "metadata": {},
   "source": [
    "Let's save each file on a separate line of a JSONL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7979915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_jsonl(fname):\n",
    "    \"Read a .jsonl file and return a list of dicts\"\n",
    "    with open(fname, 'r') as json_file:\n",
    "        return [json.loads(line) for line in json_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df66bd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_jsonl(\"vllm_python.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a6700e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus = \"\\n\".join([d['text'] for d in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ee90375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "604130"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92c3d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenizer.encode(raw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7773e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLLM .py files total tokens: 0.184048M\n"
     ]
    }
   ],
   "source": [
    "tokens = len(tokenized_data)\n",
    "print(f\"VLLM .py files total tokens: {tokens/1_000_000}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1241354b",
   "metadata": {},
   "source": [
    "That's not a lot of tokens :P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c487645d",
   "metadata": {},
   "source": [
    "## Save to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa2b4dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tcapelle/work/vllm_llm/wandb/run-20230918_212420-i4lcpbvk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/capecape/vllm_llm/runs/i4lcpbvk' target=\"_blank\">golden-water-6</a></strong> to <a href='https://wandb.ai/capecape/vllm_llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/capecape/vllm_llm' target=\"_blank\">https://wandb.ai/capecape/vllm_llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/capecape/vllm_llm/runs/i4lcpbvk' target=\"_blank\">https://wandb.ai/capecape/vllm_llm/runs/i4lcpbvk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682e4042e68d4751a31e72cc6ab1bcaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.645 MB of 0.660 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.978074…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">golden-water-6</strong> at: <a href='https://wandb.ai/capecape/vllm_llm/runs/i4lcpbvk' target=\"_blank\">https://wandb.ai/capecape/vllm_llm/runs/i4lcpbvk</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230918_212420-i4lcpbvk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "with wandb.init(project=\"vllm_llm\"):\n",
    "    at = wandb.Artifact(name=\"vllm_python\", \n",
    "                        description=\"The .py files from the vllm library\",\n",
    "                        type=\"dataset\",\n",
    "                        metadata={\n",
    "                            \"url\": \"https://github.com/vllm-project/vllm.git\",\n",
    "                            \"commit\":last_commit,\n",
    "                            \"remote\": vllm_repo.remote().url,\n",
    "                            \"tokens\": tokens})\n",
    "    at.add_file(\"vllm_python.jsonl\")\n",
    "    \n",
    "    wandb.log_artifact(at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3fd120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
