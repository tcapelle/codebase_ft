{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad6909d-00fb-45d5-ab8e-ff33648bd6a0",
   "metadata": {},
   "source": [
    "# Pretraining on .py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86871b90-fe77-4290-89ed-a5329c7f8d17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from git.repo import Repo\n",
    "from pathlib import Path\n",
    "from rich.markdown import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c4a3af-d593-4416-84ab-3538bfabf408",
   "metadata": {},
   "source": [
    "Let's clone vllm: \n",
    "\n",
    "```bash\n",
    "git clone https://github.com/vllm-project/vllm.git\n",
    "```\n",
    "\n",
    "\n",
    "let's bring some text in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cec3c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_path = Path(\"vllm/\")\n",
    "vllm_repo = Repo(vllm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d53b88-5f51-480f-8b70-e32ed1bfaa7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90979c38f87c17d53a7cd0eb430373ecb0b64b9a\n"
     ]
    }
   ],
   "source": [
    "# Get the latest commit\n",
    "last_commit = vllm_repo.commit()\n",
    "\n",
    "# Print the commit message\n",
    "print(last_commit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d707080-cca0-4341-91b8-a3786c5e8db5",
   "metadata": {},
   "source": [
    "Naively we can bring all MarkDown files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbe2c9ed-3dc0-4a97-b54d-36133447d9cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_files(directory, extension=\"*.py\"):\n",
    "    \"Find all files of a given `extension` in a directory and return their content and path\"\n",
    "    md_files = []\n",
    "    for file in Path(directory).rglob(extension):\n",
    "        with open(file, 'r', encoding='utf-8') as md_file:\n",
    "            content = md_file.read()\n",
    "        md_files.append((file.relative_to(directory), content))\n",
    "    return md_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97cdd38b-f6c2-477e-b5d4-d1eed6d98fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"Compare the outputs of HF and vLLM when using greedy sampling.\n",
      "\n",
      "Run `pytest tests/models/test_models.py --forked`.\n",
      "\"\"\"\n",
      "import pytest\n",
      "\n",
      "MODELS = [\n",
      "    \"facebook/opt-125m\",\n",
      "    \"gpt2\",\n",
      "    \"bigcode/tiny_starcoder_py\",\n",
      "    \"EleutherAI/gpt-j-6b\",\n",
      "    \"EleutherAI/pythia-70m\",\n",
      "    \"bigscience/bloom-560m\",\n",
      "    \"mosaicml/mpt-7b\",\n",
      "    \"tiiuae/falcon-7b\",\n",
      "    \"meta-llama/Llama-2-7b-hf\",\n",
      "]\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"model\", MODELS)\n",
      "@pytest.mark.parametrize(\"dtype\", [\"half\"])\n",
      "@pytest.mark.parametrize(\"ma\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(96, None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_files = find_files(vllm_path)\n",
    "\n",
    "file = random.choice(py_files)\n",
    "\n",
    "len(py_files), print(file[1][0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79897931-ddac-4423-b34e-f204fd824994",
   "metadata": {},
   "source": [
    "We should stack the file with some metadata, for instance, the path where the files is coming from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4e621d8-a3ce-4ef2-9520-8d41cc7ba98f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stuff(file):\n",
    "    path, content = file\n",
    "    return f\"<<Begin file>>\\nPath:\\n{path}\\n---------\\nContent:\\n{content}<<End File>>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95db238a-6479-46d2-917e-a3316559c0cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<Begin file>>\n",
      "Path:\n",
      "tests/models/test_models.py\n",
      "---------\n",
      "Content:\n",
      "\"\"\"Compare the outputs of HF and vLLM when using greedy sampling.\n",
      "\n",
      "Run `pytest tests/models/test_models.py --forked`.\n",
      "\"\"\"\n",
      "import pytest\n",
      "\n",
      "MODELS = [\n",
      "    \"facebook/opt-125m\",\n",
      "    \"gpt2\",\n",
      "    \"bigcode/tiny_starcoder_py\",\n",
      "    \"EleutherAI/gpt-j-6b\",\n",
      "    \"EleutherAI/pythia-70m\",\n",
      "    \"bigscience/bloom-560m\",\n",
      "    \"mosaicml/mpt-7b\",\n",
      "    \"tiiuae/falcon-7b\",\n",
      "    \"meta-llama/Llama-2-7b-hf\",\n",
      "]\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"model\", MODELS)\n",
      "@pytest.mark.parametrize(\"dtype\", [\"half\"])\n",
      "@pytest.mark.parametrize(\"max_tokens\", [128])\n",
      "def test_models(\n",
      "    hf_runner,\n",
      "    vllm_runner,\n",
      "    example_prompts,\n",
      "    model: str,\n",
      "    dtype: str,\n",
      "    max_tokens: int,\n",
      ") -> None:\n",
      "    hf_model = hf_runner(model, dtype=dtype)\n",
      "    hf_outputs = hf_model.generate_greedy(example_prompts, max_tokens)\n",
      "    del hf_model\n",
      "\n",
      "    vllm_model = vllm_runner(model, dtype=dtype)\n",
      "    vllm_outputs = vllm_model.generate_greedy(example_prompts, max_tokens)\n",
      "    del vllm_model\n",
      "\n",
      "    for i in range(len(example_prompts)):\n",
      "        hf_output_ids, hf_output_str = hf_outputs[i]\n",
      "        vllm_output_ids, vllm_output_str = vllm_outputs[i]\n",
      "        assert hf_output_str == vllm_output_str, (\n",
      "            f\"Test{i}:\\nHF: {hf_output_str!r}\\nvLLM: {vllm_output_str!r}\")\n",
      "        assert hf_output_ids == vllm_output_ids, (\n",
      "            f\"Test{i}:\\nHF: {hf_output_ids}\\nvLLM: {vllm_output_ids}\")\n",
      "<<End File>>\n"
     ]
    }
   ],
   "source": [
    "print(stuff(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c54825f-2838-4c7d-9ba6-78c069f7cd00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780d7de2f8e240d28d1558996d177afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "with open('vllm_python.jsonl', 'w') as json_file:\n",
    "    for file in tqdm(py_files):\n",
    "        data = stuff(file)\n",
    "        json.dump({\"text\":data}, json_file)\n",
    "        json_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85057fc",
   "metadata": {},
   "source": [
    "## How much data do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08c1ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57a7f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "OS_MODEL = \"codellama/CodeLlama-7b-Python-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3217af43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(OS_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3f9ff71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 822,\n",
       " 22172,\n",
       " 29918,\n",
       " 11526,\n",
       " 7295,\n",
       " 13,\n",
       " 12,\n",
       " 2158,\n",
       " 877,\n",
       " 10994,\n",
       " 2787,\n",
       " 29991,\n",
       " 1495]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"def hello_world():\\n\\tprint('Hello World!')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d198044",
   "metadata": {},
   "source": [
    "Let's save each file on a separate line of a JSONL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7979915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_jsonl(fname):\n",
    "    \"Read a .jsonl file and return a list of dicts\"\n",
    "    with open(fname, 'r') as json_file:\n",
    "        return [json.loads(line) for line in json_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df66bd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_jsonl(\"vllm_python.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a6700e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus = \"\\n\".join([d['text'] for d in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ee90375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<<Begin file>>\\nPath:\\nsetup.py\\n---------\\nContent:\\nimport io\\nimport os\\nimport re\\nimport subprocess\\nfrom typing import List, Set\\n\\nfrom packaging.version import parse, Version\\nimport setuptools\\nimport torch\\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension, CUDA_HOME\\n\\nROOT_DIR = os.path.dirname(__file__)\\n\\n# Compiler flags.\\nCXX_FLAGS = [\"-g\", \"-O2\", \"-std=c++17\"]\\n# TODO(woosuk): Should we use -O3?\\nNVCC_FLAGS = [\"-O2\", \"-std=c++17\"]\\n\\nABI = 1 if torch._C._GLIBCXX_USE_CXX11_ABI else 0\\nCXX_FLAGS += [f\"-D_GLIBCXX_USE_CXX11_ABI={ABI}\"]\\nNVCC_FLAGS += [f\"-D_GLIBCXX_USE_CXX11_ABI={ABI}\"]\\n\\nif CUDA_HOME is None:\\n    raise RuntimeError(\\n        \"Cannot find CUDA_HOME. CUDA must be available to build the package.\")\\n\\n\\ndef get_nvcc_cuda_version(cuda_dir: str) -> Version:\\n    \"\"\"Get the CUDA version from nvcc.\\n\\n    Adapted from https://github.com/NVIDIA/apex/blob/8b7a1ff183741dd8f9b87e7bafd04cfde99cea28/setup.py\\n    \"\"\"\\n    nvcc_output = subprocess.check_output([cuda_dir + \"/bin/nvcc\", \"-V\"],\\n                                          universal_newlines=True)\\n    output = nvcc_output.split()\\n    release_idx = output.index(\"release\") + 1\\n    nvcc_cuda_version = parse(output[release_idx].split(\",\")[0])\\n    return nvcc_cuda_version\\n\\n\\n# Collect the compute capabilities of all available GPUs.\\ndevice_count = torch.cuda.device_count()\\ncompute_capabilities: Set[int] = set()\\nfor i in range(device_count):\\n    major, minor = torch.cuda.get_device_capability(i)\\n    if major < 7:\\n        raise RuntimeError(\\n            \"GPUs with compute capability less than 7.0 are not supported.\")\\n    compute_capabilities.add(major * 10 + minor)\\n\\n# Validate the NVCC CUDA version.\\nnvcc_cuda_version = get_nvcc_cuda_version(CUDA_HOME)\\nif nvcc_cuda_version < Version(\"11.0\"):\\n    raise RuntimeError(\"CUDA 11.0 or higher is required to build the package.\")\\nif 86 in compute_capabilities and nvcc_cuda_version < Version(\"11.1\"):\\n    raise RuntimeError(\\n        \"CUDA 11.1 or higher is required for GPUs with compute capability 8.6.\"\\n    )\\nif 89 in compute_capabilities and nvcc_cuda_version < Version(\"11.8\"):\\n    # CUDA 11.8 is required to generate the code targeting compute capability 8.9.\\n    # However, GPUs with compute capability 8.9 can also run the code generated by\\n    # the previous versions of CUDA 11 and targeting compute capability 8.0.\\n    # Therefore, if CUDA 11.8 is not available, we target compute capability 8.0\\n    # instead of 8.9.\\n    compute_capabilities.remove(89)\\n    compute_capabilities.add(80)\\nif 90 in compute_capabilities and nvcc_cuda_version < Version(\"11.8\"):\\n    raise RuntimeError(\\n        \"CUDA 11.8 or higher is required for GPUs with compute capability 9.0.\"\\n    )\\n\\n# If no GPU is available, add all supported compute capabilities.\\nif not compute_capabilities:\\n    compute_capabilities = {70, 75, 80}\\n    if nvcc_cuda_version >= Version(\"11.1\"):\\n        compute_capabilities.add(86)\\n    if nvcc_cuda_version >= Version(\"11.8\"):\\n        compute_capabilities.add(89)\\n        compute_capabilities.add(90)\\n\\n# Add target compute capabilities to NVCC flags.\\nfor capability in compute_capabilities:\\n    NVCC_FLAGS += [\\n        \"-gencode\", f\"arch=compute_{capability},code=sm_{capability}\"\\n    ]\\n\\n# Use NVCC threads to parallelize the build.\\nif nvcc_cuda_version >= Version(\"11.2\"):\\n    num_threads = min(os.cpu_count(), 8)\\n    NVCC_FLAGS += [\"--threads\", str(num_threads)]\\n\\next_modules = []\\n\\n# Cache operations.\\ncache_extension = CUDAExtension(\\n    name=\"vllm.cache_ops\",\\n    sources=[\"csrc/cache.cpp\", \"csrc/cache_kernels.cu\"],\\n    extra_compile_args={\\n        \"cxx\": CXX_FLAGS,\\n        \"nvcc\": NVCC_FLAGS,\\n    },\\n)\\next_modules.append(cache_extension)\\n\\n# Attention kernels.\\nattention_extension = CUDAExtension(\\n    name=\"vllm.attention_ops\",\\n    sources=[\"csrc/attention.cpp\", \"csrc/attention/attention_kernels.cu\"],\\n    extra_compile_args={\\n        \"cxx\": CXX_FLAGS,\\n        \"nvcc\": NVCC_FLAGS,\\n    },\\n)\\next_modules.append(attention_extension)\\n\\n# Positional encoding kernels.\\npositional_encoding_extension = CUDAExtension(\\n    name=\"vllm.pos_encoding_ops\",\\n    sources=[\"csrc/pos_encoding.cpp\", \"csrc/pos_encoding_kernels.cu\"],\\n    extra_compile_args={\\n        \"cxx\": CXX_FLAGS,\\n        \"nvcc\": NVCC_FLAGS,\\n    },\\n)\\next_modules.append(positional_encoding_extension)\\n\\n# Layer normalization kernels.\\nlayernorm_extension = CUDAExtension(\\n    name=\"vllm.layernorm_ops\",\\n    sources=[\"csrc/layernorm.cpp\", \"csrc/layernorm_kernels.cu\"],\\n    extra_compile_args={\\n        \"cxx\": CXX_FLAGS,\\n        \"nvcc\": NVCC_FLAGS,\\n    },\\n)\\next_modules.append(layernorm_extension)\\n\\n# Activation kernels.\\nactivation_extension = CUDAExtension(\\n    name=\"vllm.activation_ops\",\\n    sources=[\"csrc/activation.cpp\", \"csrc/activation_kernels.cu\"],\\n    extra_compile_args={\\n        \"cxx\": CXX_FLAGS,\\n        \"nvcc\": NVCC_FLAGS,\\n    },\\n)\\next_modules.append(activation_extension)\\n\\n# Quantization kernels.\\nquantization_extension = CUDAExtension(\\n    name=\"vllm.quantization_ops\",\\n    sources=[\\n        \"csrc/quantization.cpp\",\\n        \"csrc/quantization/awq/gemm_kernels.cu\",\\n    ],\\n    extra_compile_args={\\n        \"cxx\": CXX_FLAGS,\\n        \"nvcc\": NVCC_FLAGS,\\n    },\\n)\\next_modules.append(quantization_extension)\\n\\n\\ndef get_path(*filepath) -> str:\\n    return os.path.join(ROOT_DIR, *filepath)\\n\\n\\ndef find_version(filepath: str):\\n    \"\"\"Extract version information from the given filepath.\\n\\n    Adapted from https://github.com/ray-project/ray/blob/0b190ee1160eeca9796bc091e07eaebf4c85b511/python/setup.py\\n    \"\"\"\\n    with open(filepath) as fp:\\n        version_match = re.search(r\"^__version__ = [\\'\\\\\"]([^\\'\\\\\"]*)[\\'\\\\\"]\",\\n                                  fp.read(), re.M)\\n        if version_match:\\n            return version_match.group(1)\\n        raise RuntimeError(\"Unable to find version string.\")\\n\\n\\ndef read_readme() -> str:\\n    \"\"\"Read the README file.\"\"\"\\n    return io.open(get_path(\"README.md\"), \"r\", encoding=\"utf-8\").read()\\n\\n\\ndef get_requirements() -> List[str]:\\n    \"\"\"Get Python package dependencies from requirements.txt.\"\"\"\\n    with open(get_path(\"requirements.txt\")) as f:\\n        requirements = f.read().strip().split(\"\\\\n\")\\n    return requirements\\n\\n\\nsetuptools.setup(\\n    name=\"vllm\",\\n    version=find_version(get_path(\"vllm\", \"__init__.py\")),\\n    author=\"vLLM Team\",\\n    license=\"Apache 2.0\",\\n    description=(\"A high-throughput and memory-efficient inference and \"\\n                 \"serving engine for LLMs\"),\\n    long_description=read_readme(),\\n    long_description_content_type=\"text/markdown\",\\n    url=\"https://github.com/vllm-project/vllm\",\\n    project_urls={\\n        \"Homepage\": \"https://github.com/vllm-project/vllm\",\\n        \"Documentation\": \"https://vllm.readthedocs.io/en/latest/\",\\n    },\\n    classifiers=[\\n        \"Programming Language :: Python :: 3.8\",\\n        \"Programming Language :: Python :: 3.9\",\\n        \"Programming Language :: Python :: 3.10\",\\n        \"Programming Language :: Python :: 3.11\",\\n        \"License :: OSI Approved :: Apache Software License\",\\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\\n    ],\\n    packages=setuptools.find_packages(exclude=(\"benchmarks\", \"csrc\", \"docs\",\\n                                               \"examples\", \"tests\")),\\n    python_requires=\">=3.8\",\\n    install_requires=get_requirements(),\\n    ext_modules=ext_modules,\\n    cmdclass={\"build_ext\": BuildExtension},\\n)\\n<<End File>>\\n<<Begin file>>\\nPath:\\ntests/conftest.py\\n---------\\nContent:\\nfrom typing import List, Optional, Tuple\\n\\nimport pytest\\nimport torch\\nfrom transformers import AutoModelForCausalLM\\n\\nfrom vllm import LLM, SamplingParams\\nfrom vllm.transformers_utils.tokenizer import get_tokenizer\\n\\n_TEST_PROMPTS = [\\n    \"vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\",\\n    \"Briefly describe the major milestones in the development of artificial intelligence from 1950 to 2020.\",\\n    \"Compare and contrast artificial intelligence with human intelligence in terms of processing information.\",\\n    \"Describe the basic components of a neural network and how it can be trained.\",\\n    \"Write a short story about a robot that dreams for the first time.\",\\n    \"Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\",\\n    \"Explain the cultural significance of the Mona Lisa painting, and how its perception might vary in Western versus Eastern societies.\",\\n    \"Translate the following English sentence into Japanese, French, and Swahili: \\'The early bird catches the worm.\\'\",\\n]\\n\\n\\n@pytest.fixture\\ndef example_prompts() -> List[str]:\\n    return _TEST_PROMPTS\\n\\n\\n_STR_DTYPE_TO_TORCH_DTYPE = {\\n    \"half\": torch.half,\\n    \"bfloat16\": torch.bfloat16,\\n    \"float\": torch.float,\\n}\\n\\n\\nclass HfRunner:\\n\\n    def __init__(\\n        self,\\n        model_name: str,\\n        tokenizer_name: Optional[str] = None,\\n        dtype: str = \"half\",\\n    ) -> None:\\n        assert dtype in _STR_DTYPE_TO_TORCH_DTYPE\\n        torch_dtype = _STR_DTYPE_TO_TORCH_DTYPE[dtype]\\n        self.model = AutoModelForCausalLM.from_pretrained(\\n            model_name,\\n            torch_dtype=torch_dtype,\\n            trust_remote_code=True,\\n        ).cuda()\\n        if tokenizer_name is None:\\n            tokenizer_name = model_name\\n        self.tokenizer = get_tokenizer(tokenizer_name, trust_remote_code=True)\\n\\n    def generate(\\n        self,\\n        prompts: List[str],\\n        **kwargs,\\n    ) -> List[Tuple[List[int], str]]:\\n        outputs: List[Tuple[List[int], str]] = []\\n        for prompt in prompts:\\n            input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\\n            output_ids = self.model.generate(\\n                input_ids.cuda(),\\n                use_cache=True,\\n                **kwargs,\\n            )\\n            output_str = self.tokenizer.batch_decode(\\n                output_ids,\\n                skip_special_tokens=True,\\n                clean_up_tokenization_spaces=False,\\n            )\\n            output_ids = output_ids.cpu().tolist()\\n            outputs.append((output_ids, output_str))\\n        return outputs\\n\\n    def generate_greedy(\\n        self,\\n        prompts: List[str],\\n        max_tokens: int,\\n    ) -> List[Tuple[List[int], str]]:\\n        outputs = self.generate(prompts,\\n                                do_sample=False,\\n                                max_new_tokens=max_tokens)\\n        for i in range(len(outputs)):\\n            output_ids, output_str = outputs[i]\\n            outputs[i] = (output_ids[0], output_str[0])\\n        return outputs\\n\\n    def generate_beam_search(\\n        self,\\n        prompts: List[str],\\n        beam_width: int,\\n        max_tokens: int,\\n    ) -> List[Tuple[List[int], str]]:\\n        outputs = self.generate(prompts,\\n                                do_sample=False,\\n                                max_new_tokens=max_tokens,\\n                                num_beams=beam_width,\\n                                num_return_sequences=beam_width)\\n        for i in range(len(outputs)):\\n            output_ids, output_str = outputs[i]\\n            for j in range(len(output_ids)):\\n                output_ids[j] = [\\n                    x for x in output_ids[j]\\n                    if x != self.tokenizer.pad_token_id\\n                ]\\n            outputs[i] = (output_ids, output_str)\\n        return outputs\\n\\n\\n@pytest.fixture\\ndef hf_runner():\\n    return HfRunner\\n\\n\\nclass VllmRunner:\\n\\n    def __init__(\\n        self,\\n        model_name: str,\\n        tokenizer_name: Optional[str] = None,\\n        dtype: str = \"half\",\\n    ) -> None:\\n        self.model = LLM(\\n            model=model_name,\\n            tokenizer=tokenizer_name,\\n            trust_remote_code=True,\\n            dtype=dtype,\\n            swap_space=0,\\n        )\\n\\n    def generate(\\n        self,\\n        prompts: List[str],\\n        sampling_params: SamplingParams,\\n    ) -> List[Tuple[List[int], str]]:\\n        req_outputs = self.model.generate(prompts,\\n                                          sampling_params=sampling_params)\\n        outputs = []\\n        for req_output in req_outputs:\\n            prompt_str = req_output.prompt\\n            prompt_ids = req_output.prompt_token_ids\\n            req_sample_output_ids = []\\n            req_sample_output_strs = []\\n            for sample in req_output.outputs:\\n                output_str = sample.text\\n                output_ids = sample.token_ids\\n                req_sample_output_ids.append(prompt_ids + output_ids)\\n                req_sample_output_strs.append(prompt_str + output_str)\\n            outputs.append((req_sample_output_ids, req_sample_output_strs))\\n        return outputs\\n\\n    def generate_greedy(\\n        self,\\n        prompts: List[str],\\n        max_tokens: int,\\n    ) -> List[Tuple[List[int], str]]:\\n        greedy_params = SamplingParams(temperature=0.0, max_tokens=max_tokens)\\n        outputs = self.generate(prompts, greedy_params)\\n        return [(output_ids[0], output_str[0])\\n                for output_ids, output_str in outputs]\\n\\n    def generate_beam_search(\\n        self,\\n        prompts: List[str],\\n        beam_width: int,\\n        max_tokens: int,\\n    ) -> List[Tuple[List[int], str]]:\\n        beam_search_params = SamplingParams(n=beam_width,\\n                                            use_beam_search=True,\\n                                            temperature=0.0,\\n                                            max_tokens=max_tokens)\\n        outputs = self.generate(prompts, beam_search_params)\\n        return outputs\\n\\n\\n@pytest.fixture\\ndef vllm_runner():\\n    return VllmRunner\\n<<End File>>\\n<<Begin file>>\\nPath:\\ntests/kernels/conftest.py\\n---------\\nContent:\\nfrom typing import List, Tuple\\n\\nimport pytest\\nimport torch\\n\\n\\ndef create_kv_caches(\\n    num_blocks: int,\\n    block_size: int,\\n    num_layers: int,\\n    num_heads: int,\\n    head_size: int,\\n    dtype: torch.dtype,\\n    seed: int,\\n) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\\n    torch.random.manual_seed(seed)\\n    torch.cuda.manual_seed(seed)\\n\\n    scale = head_size**-0.5\\n    x = 16 // torch.tensor([], dtype=dtype).element_size()\\n    key_cache_shape = (num_blocks, num_heads, head_size // x, block_size, x)\\n    key_caches = []\\n    for _ in range(num_layers):\\n        key_cache = torch.empty(size=key_cache_shape,\\n                                dtype=dtype,\\n                                device=\\'cuda\\')\\n        key_cache.uniform_(-scale, scale)\\n        key_caches.append(key_cache)\\n\\n    value_cache_shape = (num_blocks, num_heads, head_size, block_size)\\n    value_caches = []\\n    for _ in range(num_layers):\\n        value_cache = torch.empty(size=value_cache_shape,\\n                                  dtype=dtype,\\n                                  device=\\'cuda\\')\\n        value_cache.uniform_(-scale, scale)\\n        value_caches.append(value_cache)\\n    return key_caches, value_caches\\n\\n\\n@pytest.fixture()\\ndef kv_cache_factory():\\n    return create_kv_caches\\n<<End File>>\\n<<Begin file>>\\nPath:\\ntests/kernels/test_pos_encoding.py\\n---------\\nContent:\\nfrom typing import Optional, Tuple\\n\\nimport pytest\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nfrom vllm import pos_encoding_ops\\n\\nIS_NEOX_STYLE = [True, False]\\nDTYPES = [torch.half, torch.bfloat16, torch.float]\\nHEAD_SIZES = [64, 80, 96, 112, 128, 256]\\nROTARY_DIMS = [None, 32]  # None means rotary dim == head size\\nNUM_HEADS = [7, 12, 40, 52]  # Arbitrary values for testing\\nNUM_TOKENS = [11, 83, 2048]  # Arbitrary values for testing\\nSEEDS = [0]\\n\\n\\ndef rotate_neox(x: torch.Tensor) -> torch.Tensor:\\n    x1 = x[..., :x.shape[-1] // 2]\\n    x2 = x[..., x.shape[-1] // 2:]\\n    return torch.cat((-x2, x1), dim=-1)\\n\\n\\ndef rotate_gptj(x: torch.Tensor) -> torch.Tensor:\\n    x1 = x[..., ::2]\\n    x2 = x[..., 1::2]\\n    x = torch.stack((-x2, x1), dim=-1)\\n    return x.flatten(-2)\\n\\n\\ndef apply_rope(\\n    q: torch.Tensor,\\n    k: torch.Tensor,\\n    cos: torch.Tensor,\\n    sin: torch.Tensor,\\n    is_neox_style: bool,\\n) -> Tuple[torch.Tensor, torch.Tensor]:\\n    rotate_fn = rotate_neox if is_neox_style else rotate_gptj\\n    q_embed = (q * cos) + (rotate_fn(q) * sin)\\n    k_embed = (k * cos) + (rotate_fn(k) * sin)\\n    return q_embed, k_embed\\n\\n\\nclass RefRotaryEmbedding(nn.Module):\\n    \"\"\"Reference implementation of rotary embedding.\"\"\"\\n\\n    def __init__(\\n        self,\\n        dim: int,\\n        is_neox_style: bool,\\n        max_position_embeddings: int = 8192,\\n        base: int = 10000,\\n    ) -> None:\\n        super().__init__()\\n        self.rotary_dim = dim\\n        self.is_neox_style = is_neox_style\\n        self.max_position_embeddings = max_position_embeddings\\n\\n        # Create cos and sin embeddings.\\n        inv_freq = 1.0 / (base**(torch.arange(0, dim, 2) / dim))\\n        t = torch.arange(max_position_embeddings).float()\\n        freqs = torch.einsum(\"i,j->ij\", t, inv_freq.float())\\n        if is_neox_style:\\n            emb = torch.cat((freqs, freqs), dim=-1)\\n        else:\\n            emb = torch.repeat_interleave(freqs, 2, -1)\\n        cos = emb.cos().to(dtype=inv_freq.dtype)\\n        sin = emb.sin().to(dtype=inv_freq.dtype)\\n        self.register_buffer(\"cos_cached\", cos, persistent=False)\\n        self.register_buffer(\"sin_cached\", sin, persistent=False)\\n\\n    def forward(\\n        self,\\n        positions: torch.Tensor,  # [num_tokens]\\n        query: torch.Tensor,  # [num_tokens, num_heads, head_size]\\n        key: torch.Tensor,  # [num_tokens, num_heads, head_size]\\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\\n        query_rot = query[..., :self.rotary_dim]\\n        query_pass = query[..., self.rotary_dim:]\\n        key_rot = key[..., :self.rotary_dim]\\n        key_pass = key[..., self.rotary_dim:]\\n\\n        query_rot = query_rot.transpose(0, 1)\\n        key_rot = key_rot.transpose(0, 1)\\n        cos = F.embedding(positions, self.cos_cached)\\n        sin = F.embedding(positions, self.sin_cached)\\n\\n        query_rot, key_rot = apply_rope(query_rot, key_rot, cos, sin,\\n                                        self.is_neox_style)\\n        query_rot = query_rot.transpose(0, 1).contiguous()\\n        key_rot = key_rot.transpose(0, 1).contiguous()\\n\\n        query = torch.cat((query_rot, query_pass), dim=-1)\\n        key = torch.cat((key_rot, key_pass), dim=-1)\\n\\n        # Output query/key shape: [num_tokens, num_tokens, head_size]\\n        return query, key\\n\\n\\n@pytest.mark.parametrize(\"is_neox_style\", IS_NEOX_STYLE)\\n@pytest.mark.parametrize(\"num_tokens\", NUM_TOKENS)\\n@pytest.mark.parametrize(\"num_heads\", NUM_HEADS)\\n@pytest.mark.parametrize(\"head_size\", HEAD_SIZES)\\n@pytest.mark.parametrize(\"rotary_dim\", ROTARY_DIMS)\\n@pytest.mark.parametrize(\"dtype\", DTYPES)\\n@pytest.mark.parametrize(\"seed\", SEEDS)\\n@torch.inference_mode()\\ndef test_rotary_embedding(\\n    is_neox_style: bool,\\n    num_tokens: int,\\n    num_heads: int,\\n    head_size: int,\\n    rotary_dim: Optional[int],\\n    dtype: torch.dtype,\\n    seed: int,\\n    max_position: int = 8192,\\n    base: int = 10000,\\n) -> None:\\n    if rotary_dim is None:\\n        rotary_dim = head_size\\n    torch.random.manual_seed(seed)\\n    torch.cuda.manual_seed(seed)\\n\\n    positions = torch.randint(0, max_position, (num_tokens, ), device=\"cuda\")\\n    query = torch.randn(num_tokens,\\n                        num_heads * head_size,\\n                        dtype=dtype,\\n                        device=\"cuda\")\\n    key = torch.randn(num_tokens,\\n                      num_heads * head_size,\\n                      dtype=dtype,\\n                      device=\"cuda\")\\n\\n    # Create the rotary embedding.\\n    inv_freq = 1.0 / (base**(\\n        torch.arange(0, rotary_dim, 2, dtype=torch.float) / rotary_dim))\\n    t = torch.arange(max_position).float()\\n    freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\\n    cos = freqs.cos()\\n    sin = freqs.sin()\\n    cos_sin_cache = torch.cat((cos, sin), dim=-1)\\n    cos_sin_cache = cos_sin_cache.to(dtype=dtype, device=\\'cuda\\')\\n\\n    # Run the kernel. The kernel is in-place, so we need to clone the inputs.\\n    out_query = query.clone()\\n    out_key = key.clone()\\n    pos_encoding_ops.rotary_embedding(\\n        positions,\\n        out_query,\\n        out_key,\\n        head_size,\\n        cos_sin_cache,\\n        is_neox_style,\\n    )\\n\\n    # Run the reference implementation.\\n    ref_rotary_embedding = RefRotaryEmbedding(\\n        dim=rotary_dim,\\n        is_neox_style=is_neox_style,\\n        max_position_embeddings=max_position,\\n        base=base,\\n    ).to(dtype=dtype, device=\"cuda\")\\n    ref_query, ref_key = ref_rotary_embedding(\\n        positions,\\n        query.view(num_tokens, num_heads, head_size),\\n        key.view(num_tokens, num_heads, head_size),\\n    )\\n    ref_query = ref_query.view(num_tokens, num_heads * head_size)\\n    ref_key = ref_key.view(num_tokens, num_heads * head_size)\\n\\n    # Compare the results.\\n    assert torch.allclose(out_query, ref_query, atol=1e-5, rtol=1e-5)\\n    assert torch.allclose(out_key, ref_key, atol=1e-5, rtol=1e-5)\\n<<End File>>\\n<<Begin file>>\\nPath:\\ntests/kernels/test_activation.py\\n---------\\nContent:\\nimport pytest\\nimport torch\\nimport torch.nn.functional as F\\nfrom transformers.activations import get_activation\\n\\nfrom vllm import activation_ops\\n\\nDTYPES = [torch.half, torch.bfloat16, torch.float]\\nNUM_TOKENS = [7, 83, 2048]  # Arbitrary values for testing\\nD = [512, 4096, 5120, 13824]  # Arbitrary values for testing\\nSEEDS = [0]\\n\\n\\ndef ref_silu_and_mul(x: torch.Tensor) -> torch.Tensor:\\n    x1, x2 = x.chunk(chunks=2, dim=1)\\n    return F.silu(x1) * x2\\n\\n\\n@pytest.mark.parametrize(\"num_tokens\", NUM_TOKENS)\\n@pytest.mark.parametrize(\"d\", D)\\n@pytest.mark.parametrize(\"dtype\", DTYPES)\\n@pytest.mark.parametrize(\"seed\", SEEDS)\\n@torch.inference_mode()\\ndef test_silu_and_mul(\\n    num_tokens: int,\\n    d: int,\\n    dtype: torch.dtype,\\n    seed: int,\\n) -> None:\\n    torch.random.manual_seed(seed)\\n    torch.cuda.manual_seed(seed)\\n    x = torch.randn(num_tokens, 2 * d, dtype=dtype, device=\\'cuda\\')\\n    out = torch.empty(num_tokens, d, dtype=dtype, device=\\'cuda\\')\\n    activation_ops.silu_and_mul(out, x)\\n    ref_out = ref_silu_and_mul(x)\\n    assert torch.allclose(out, ref_out, atol=1e-5, rtol=1e-5)\\n\\n\\n@pytest.mark.parametrize(\"num_tokens\", NUM_TOKENS)\\n@pytest.mark.parametrize(\"d\", D)\\n@pytest.mark.parametrize(\"dtype\", DTYPES)\\n@pytest.mark.parametrize(\"seed\", SEEDS)\\n@torch.inference_mode()\\ndef test_gelu_new(\\n    num_tokens: int,\\n    d: int,\\n    dtype: torch.dtype,\\n    seed: int,\\n) -> None:\\n    torch.random.manual_seed(seed)\\n    torch.cuda.manual_seed(seed)\\n    x = torch.randn(num_tokens, d, dtype=dtype, device=\\'cuda\\')\\n    out = torch.empty(num_tokens, d, dtype=dtype, device=\\'cuda\\')\\n    activation_ops.gelu_new(out, x)\\n    ref_out = get_activation(\"gelu_new\")(x)\\n    assert torch.allclose(out, ref_out, atol=1e-5, rtol=1e-5)\\n\\n\\n@pytest.mark.parametrize(\"num_tokens\", NUM_TOKENS)\\n@pytest.mark.parametrize(\"d\", D)\\n@pytest.mark.parametrize(\"dtype\", DTYPES)\\n@pytest.mark.parametrize(\"seed\", SEEDS)\\ndef test_gelu_fast(\\n    num_tokens: int,\\n    d: int,\\n    dtype: torch.dtype,\\n    seed: int,\\n) -> None:\\n    torch.random.manual_seed(seed)\\n    torch.cuda.manual_seed(seed)\\n    x = torch.randn(num_tokens, d, dtype=dtype, device=\\'cuda\\')\\n    out = torch.empty(num_tokens, d, dtype=dtype, device=\\'cuda\\')\\n    activation_ops.gelu_fast(out, x)\\n    ref_out = get_activation(\"gelu_fast\")(x)\\n    assert torch.allclose(out, ref_out, atol=1e-5, rtol=1e-5)\\n<<End File>>\\n<<Begin file>>\\nPath:\\ntests/kernels/test_layernorm.py\\n---------\\nContent:\\nimport pytest\\nimport torch\\nimport torch.nn as nn\\n\\nfrom vllm import layernorm_ops\\n\\nDTYPES = [torch.half, torch.bfloat16, torch.float]\\nHIDDEN_SIZES = [67, 768, 2048, 5120, 8192]  # Arbitrary values for testing\\nNUM_TOKENS = [7, 83, 4096]  # Arbitrary values for testing\\nSEEDS = [0]\\n\\n\\nclass RefRMSNorm(nn.Module):\\n\\n    def __init__(self, hidden_size, eps=1e-6):\\n        super().__init__()\\n        weight = torch.empty(hidden_size)\\n        weight.normal_(mean=1.0, std=0.1)\\n        self.weight = nn.Parameter(weight)\\n        self.variance_epsilon = eps\\n\\n    def forward(self, hidden_states):\\n        input_dtype = hidden_states.dtype\\n        hidden_states = hidden_states.to(torch.float32)\\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\\n        hidden_states = hidden_states * torch.rsqrt(variance +\\n                                                    self.variance_epsilon)\\n        return self.weight * hidden_states.to(input_dtype)\\n\\n\\n@pytest.mark.parametrize(\"num_tokens\", NUM_TOKENS)\\n@pytest.mark.parametrize(\"hidden_size\", HIDDEN_SIZES)\\n@pytest.mark.parametrize(\"dtype\", DTYPES)\\n@pytest.mark.parametrize(\"seed\", SEEDS)\\n@torch.inference_mode()\\ndef test_rms_norm(\\n    num_tokens: int,\\n    hidden_size: int,\\n    dtype: torch.dtype,\\n    seed: int,\\n) -> None:\\n    torch.random.manual_seed(seed)\\n    torch.cuda.manual_seed(seed)\\n\\n    scale = float(hidden_size**-0.5)\\n    x = torch.empty(num_tokens, hidden_size, dtype=dtype, device=\"cuda\")\\n    x.uniform_(-scale, scale)\\n    ref = RefRMSNorm(hidden_size).to(dtype).cuda()\\n\\n    out = torch.empty_like(x)\\n    layernorm_ops.rms_norm(\\n        out,\\n        x,\\n        ref.weight.data,\\n        ref.variance_epsilon,\\n    )\\n    ref_out = ref(x)\\n    assert torch.allclose(out, ref_out, atol=1e-2, rtol=1e-5)\\n<<End File>>\\n<<Begin file>>\\nPath:\\ntests/kernels/test_attention.py\\n---------\\nContent:\\nimport random\\nfrom typing import List, Optional, Tuple\\n\\nimport pytest\\nimport torch\\nfrom xformers import ops as xops\\nfrom xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\\n\\nfrom vllm import attention_ops\\n\\nMAX_SEQ_LEN = 8192\\nNUM_BLOCKS = 128  # Arbitrary values for testing\\n\\nDTYPES = [torch.half, torch.bfloat16, torch.float]\\nNUM_GEN_SEQS = [7]  # Arbitrary values for testing\\nNUM_PREFILL_SEQS = [1, 3, 7]  # Arbitrary values for testing\\nNUM_HEADS = [(40, 40), (64, 8)]  # Arbitrary values for testing\\nHEAD_SIZES = [64, 80, 96, 112, 128, 256]\\nBLOCK_SIZES = [8, 16, 32]\\nUSE_ALIBI = [False, True]\\nSEEDS = [0]\\n\\n\\ndef ref_masked_attention(\\n    query: torch.Tensor,\\n    key: torch.Tensor,\\n    value: torch.Tensor,\\n    scale: float,\\n    attn_mask: Optional[torch.Tensor] = None,\\n) -> torch.Tensor:\\n    attn_weights = scale * torch.einsum(\"qhd,khd->hqk\", query, key).float()\\n    if attn_mask is not None:\\n        attn_weights = attn_weights + attn_mask.float()\\n    attn_weights = torch.softmax(attn_weights, dim=-1).to(value.dtype)\\n    out = torch.einsum(\"hqk,khd->qhd\", attn_weights, value)\\n    return out\\n\\n\\ndef ref_single_query_cached_kv_attention(\\n    output: torch.Tensor,\\n    query: torch.Tensor,\\n    num_queries_per_kv: int,\\n    key_cache: torch.Tensor,\\n    value_cache: torch.Tensor,\\n    block_tables: torch.Tensor,\\n    context_lens: torch.Tensor,\\n    scale: float,\\n    alibi_slopes: Optional[torch.Tensor],\\n) -> None:\\n    num_query_heads = query.shape[1]\\n    num_kv_heads = value_cache.shape[1]\\n    head_size = value_cache.shape[2]\\n    block_size = value_cache.shape[3]\\n    num_seqs = query.shape[0]\\n\\n    block_tables = block_tables.cpu().tolist()\\n    context_lens = context_lens.cpu().tolist()\\n    for i in range(num_seqs):\\n        q = query[i].unsqueeze(0)\\n        block_table = block_tables[i]\\n        context_len = int(context_lens[i])\\n\\n        keys = []\\n        values = []\\n        for j in range(context_len):\\n            block_number = int(block_table[j // block_size])\\n            block_offset = j % block_size\\n\\n            k = key_cache[block_number, :, :, block_offset, :]\\n            k = k.reshape(num_kv_heads, head_size)\\n            keys.append(k)\\n\\n            v = value_cache[block_number, :, :, block_offset]\\n            values.append(v)\\n        keys = torch.stack(keys, dim=0)\\n        values = torch.stack(values, dim=0)\\n        if num_queries_per_kv > 1:\\n            # Handle MQA and GQA\\n            keys = torch.repeat_interleave(keys, num_queries_per_kv, dim=1)\\n            values = torch.repeat_interleave(values, num_queries_per_kv, dim=1)\\n\\n        alibi_bias = None\\n        if alibi_slopes is not None:\\n            # Create the ALiBi bias used in the paged attention kernel.\\n            position_ids = torch.arange(context_len, device=\"cuda\").int()\\n            alibi_bias = (position_ids - context_len + 1).float()\\n            alibi_bias = alibi_slopes.view(-1, 1, 1) * alibi_bias.view(\\n                1, 1, -1)\\n\\n        out = ref_masked_attention(q, keys, values, scale, alibi_bias)\\n        out = out.view(num_query_heads, head_size)\\n        output[i].copy_(out, non_blocking=True)\\n\\n\\n@pytest.mark.parametrize(\"num_seqs\", NUM_GEN_SEQS)\\n@pytest.mark.parametrize(\"num_heads\", NUM_HEADS)\\n@pytest.mark.parametrize(\"head_size\", HEAD_SIZES)\\n@pytest.mark.parametrize(\"use_alibi\", USE_ALIBI)\\n@pytest.mark.parametrize(\"block_size\", BLOCK_SIZES)\\n@pytest.mark.parametrize(\"dtype\", DTYPES)\\n@pytest.mark.parametrize(\"seed\", SEEDS)\\n@torch.inference_mode()\\ndef test_single_query_cached_kv_attention(\\n    kv_cache_factory,\\n    num_seqs: int,\\n    num_heads: Tuple[int, int],\\n    head_size: int,\\n    use_alibi: bool,\\n    block_size: int,\\n    dtype: torch.dtype,\\n    seed: int,\\n) -> None:\\n    random.seed(seed)\\n    torch.random.manual_seed(seed)\\n    torch.cuda.manual_seed(seed)\\n\\n    scale = float(1.0 / (head_size**0.5))\\n    num_query_heads, num_kv_heads = num_heads\\n    query = torch.empty(num_seqs,\\n                        num_query_heads,\\n                        head_size,\\n                        dtype=dtype,\\n                        device=\"cuda\")\\n    query.uniform_(-scale, scale)\\n\\n    assert num_query_heads % num_kv_heads == 0\\n    num_queries_per_kv = num_query_heads // num_kv_heads\\n    head_mapping = torch.repeat_interleave(\\n        torch.arange(num_kv_heads, dtype=torch.int32, device=\"cuda\"),\\n        num_queries_per_kv)\\n    alibi_slopes = None\\n    if use_alibi:\\n        alibi_slopes = torch.randn(num_query_heads,\\n                                   dtype=torch.float,\\n                                   device=\"cuda\")\\n\\n    context_lens = [random.randint(1, MAX_SEQ_LEN) for _ in range(num_seqs)]\\n    max_context_len = max(context_lens)\\n    context_lens = torch.tensor(context_lens, dtype=torch.int, device=\"cuda\")\\n\\n    # Create the block tables.\\n    max_num_blocks_per_seq = (max_context_len + block_size - 1) // block_size\\n    block_tables = []\\n    for _ in range(num_seqs):\\n        block_table = [\\n            random.randint(0, NUM_BLOCKS - 1)\\n            for _ in range(max_num_blocks_per_seq)\\n        ]\\n        block_tables.append(block_table)\\n    block_tables = torch.tensor(block_tables, dtype=torch.int, device=\"cuda\")\\n\\n    # Create the KV caches.\\n    key_caches, value_caches = kv_cache_factory(NUM_BLOCKS, block_size, 1,\\n                                                num_kv_heads, head_size, dtype,\\n                                                seed)\\n    key_cache, value_cache = key_caches[0], value_caches[0]\\n\\n    # Call the paged attention kernel.\\n    output = torch.empty_like(query)\\n    attention_ops.single_query_cached_kv_attention(\\n        output,\\n        query,\\n        key_cache,\\n        value_cache,\\n        head_mapping,\\n        scale,\\n        block_tables,\\n        context_lens,\\n        block_size,\\n        max_context_len,\\n        alibi_slopes,\\n    )\\n\\n    # Run the reference implementation.\\n    ref_output = torch.empty_like(query)\\n    ref_single_query_cached_kv_attention(\\n        ref_output,\\n        query,\\n        num_queries_per_kv,\\n        key_cache,\\n        value_cache,\\n        block_tables,\\n        context_lens,\\n        scale,\\n        alibi_slopes,\\n    )\\n\\n    # NOTE(woosuk): Due to the kernel-level differences in the two\\n    # implementations, there is a small numerical difference in the two\\n    # outputs. Thus, we use a relaxed tolerance for the test.\\n    assert torch.allclose(output, ref_output, atol=1e-3, rtol=1e-5)\\n\\n\\ndef ref_multi_query_kv_attention(\\n    cu_seq_lens: List[int],\\n    query: torch.Tensor,\\n    key: torch.Tensor,\\n    value: torch.Tensor,\\n    scale: float,\\n    dtype: torch.dtype,\\n) -> torch.Tensor:\\n    num_seqs = len(cu_seq_lens) - 1\\n    ref_outputs = []\\n    for i in range(num_seqs):\\n        start_idx = cu_seq_lens[i]\\n        end_idx = cu_seq_lens[i + 1]\\n        seq_len = end_idx - start_idx\\n\\n        # Create attention mask.\\n        attn_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=dtype),\\n                               diagonal=1)\\n        attn_mask = attn_mask * torch.finfo(dtype).min\\n        attn_mask = attn_mask.to(dtype=dtype, device=\"cuda\")\\n\\n        ref_output = ref_masked_attention(\\n            query[start_idx:end_idx],\\n            key[start_idx:end_idx],\\n            value[start_idx:end_idx],\\n            scale,\\n            attn_mask=attn_mask,\\n        )\\n        ref_outputs.append(ref_output)\\n    ref_output = torch.cat(ref_outputs, dim=0)\\n    return ref_output\\n\\n\\n# TODO(woosuk): Add tests for USE_ALIBI=True.\\n@pytest.mark.parametrize(\"num_seqs\", NUM_PREFILL_SEQS)\\n@pytest.mark.parametrize(\"num_heads\", NUM_HEADS)\\n@pytest.mark.parametrize(\"head_size\", HEAD_SIZES)\\n@pytest.mark.parametrize(\"dtype\", DTYPES)\\n@pytest.mark.parametrize(\"seed\", SEEDS)\\n@torch.inference_mode()\\ndef test_multi_query_kv_attention(\\n    num_seqs: int,\\n    num_heads: Tuple[int, int],\\n    head_size: int,\\n    dtype: torch.dtype,\\n    seed: int,\\n) -> None:\\n    random.seed(seed)\\n    torch.random.manual_seed(seed)\\n    torch.cuda.manual_seed(seed)\\n\\n    seq_lens = random.sample(range(1, MAX_SEQ_LEN), num_seqs)\\n    num_tokens = sum(seq_lens)\\n\\n    scale = float(1.0 / (head_size**0.5))\\n    num_query_heads, num_kv_heads = num_heads\\n    qkv = torch.empty(num_tokens,\\n                      num_query_heads + 2 * num_kv_heads,\\n                      head_size,\\n                      dtype=dtype,\\n                      device=\"cuda\")\\n    qkv.uniform_(-scale, scale)\\n    query, key, value = qkv.split(\\n        [num_query_heads, num_kv_heads, num_kv_heads], dim=1)\\n\\n    num_queries_per_kv = num_query_heads // num_kv_heads\\n    if num_queries_per_kv > 1:\\n        # Handle MQA and GQA\\n        key = torch.repeat_interleave(key, num_queries_per_kv, dim=1)\\n        value = torch.repeat_interleave(value, num_queries_per_kv, dim=1)\\n    attn_bias = BlockDiagonalCausalMask.from_seqlens(seq_lens)\\n    output = xops.memory_efficient_attention_forward(\\n        query.unsqueeze(0),\\n        key.unsqueeze(0),\\n        value.unsqueeze(0),\\n        attn_bias=attn_bias,\\n        p=0.0,\\n        scale=scale,\\n    )\\n    output = output.squeeze(0)\\n\\n    cu_seq_lens = [0]\\n    for seq_len in seq_lens:\\n        cu_seq_lens.append(cu_seq_lens[-1] + seq_len)\\n    ref_output = ref_multi_query_kv_attention(\\n        cu_seq_lens,\\n        query,\\n        key,\\n        value,\\n        scale,\\n        dtype,\\n    )\\n    assert torch.allclose(output, ref_output, atol=1e-3, rtol=1e-5)\\n<<End File>>\\n<<Begin file>>\\nPath:\\ntests/kernels/test_cache.py\\n---------\\nContent:\\nimport random\\n\\nimport pytest\\nimport torch\\n\\nfrom vllm import cache_ops\\n\\nDTYPES = [torch.half, torch.bfloat16, torch.float]\\nNUM_TOKENS = [7, 83, 2048]  # Arbitrary values for testing\\nNUM_LAYERS = [5]  # Arbitrary values for testing\\nNUM_HEADS = [8]  # Arbitrary values for testing\\nHEAD_SIZES = [64, 80, 96, 112, 128, 256]\\nBLOCK_SIZES = [8, 16, 32]\\nNUM_BLOCKS = [1024]  # Arbitrary values for testing\\nNUM_MAPPINGS = [32, 256]  # Arbitrary values for testing\\nSEEDS = [0]\\n\\n\\n@pytest.mark.parametrize(\"num_mappings\", NUM_MAPPINGS)\\n@pytest.mark.parametrize(\"num_layers\", NUM_LAYERS)\\n@pytest.mark.parametrize(\"num_heads\", NUM_HEADS)\\n@pytest.mark.parametrize(\"head_size\", HEAD_SIZES)\\n@pytest.mark.parametrize(\"block_size\", BLOCK_SIZES)\\n@pytest.mark.parametrize(\"num_blocks\", NUM_BLOCKS)\\n@pytest.mark.parametrize(\"dtype\", DTYPES)\\n@pytest.mark.parametrize(\"seed\", SEEDS)\\n@torch.inference_mode()\\ndef test_copy_blocks(\\n    kv_cache_factory,\\n    num_mappings: int,\\n    num_layers: int,\\n    num_heads: int,\\n    head_size: int,\\n    block_size: int,\\n    num_blocks: int,\\n    dtype: torch.dtype,\\n    seed: int,\\n) -> None:\\n    random.seed(seed)\\n    torch.random.manual_seed(seed)\\n    torch.cuda.manual_seed(seed)\\n\\n    # Generate random block mappings where each source block is mapped to two\\n    # destination blocks.\\n    assert 2 * num_mappings <= num_blocks\\n    src_blocks = random.sample(range(num_blocks), num_mappings)\\n    remainig_blocks = list(set(range(num_blocks)) - set(src_blocks))\\n    dst_blocks = random.sample(remainig_blocks, 2 * num_mappings)\\n    block_mapping = {}\\n    for i in range(num_mappings):\\n        src = src_blocks[i]\\n        dst1 = dst_blocks[2 * i]\\n        dst2 = dst_blocks[2 * i + 1]\\n        block_mapping[src] = [dst1, dst2]\\n\\n    # Create the KV caches.\\n    key_caches, value_caches = kv_cache_factory(num_blocks, block_size,\\n                                                num_layers, num_heads,\\n                                                head_size, dtype, seed)\\n\\n    # Clone the KV caches.\\n    cloned_key_caches = [key_cache.clone() for key_cache in key_caches]\\n    cloned_value_caches = [value_cache.clone() for value_cache in value_caches]\\n\\n    # Call the copy blocks kernel.\\n    cache_ops.copy_blocks(key_caches, value_caches, block_mapping)\\n\\n    # Run the reference implementation.\\n    for src, dsts in block_mapping.items():\\n        for dst in dsts:\\n            for cloned_key_cache in cloned_key_caches:\\n                cloned_key_cache[dst] = cloned_key_cache[src]\\n            for cloned_value_cache in cloned_value_caches:\\n                cloned_value_cache[dst] = cloned_value_cache[src]\\n\\n    # Compare the results.\\n    for key_cache, cloned_key_cache in zip(key_caches, cloned_key_caches):\\n        assert torch.allclose(key_cache, cloned_key_cache)\\n    for value_cache, cloned_value_cache in zip(value_caches,\\n                                               cloned_value_caches):\\n        assert torch.allclose(value_cache, cloned_value_cache)\\n\\n\\n@pytest.mark.parametrize(\"num_tokens\", NUM_TOKENS)\\n@pytest.mark.parametrize(\"num_heads\", NUM_HEADS)\\n@pytest.mark.parametrize(\"head_size\", HEAD_SIZES)\\n@pytest.mark.parametrize(\"block_size\", BLOCK_SIZES)\\n@pytest.mark.parametrize(\"num_blocks\", NUM_BLOCKS)\\n@pytest.mark.parametrize(\"dtype\", DTYPES)\\n@pytest.mark.parametrize(\"seed\", SEEDS)\\n@torch.inference_mode()\\ndef test_reshape_and_cache(\\n    kv_cache_factory,\\n    num_tokens: int,\\n    num_heads: int,\\n    head_size: int,\\n    block_size: int,\\n    num_blocks: int,\\n    dtype: torch.dtype,\\n    seed: int,\\n) -> None:\\n    random.seed(seed)\\n    torch.random.manual_seed(seed)\\n    torch.cuda.manual_seed(seed)\\n\\n    # Create a random slot mapping.\\n    num_slots = block_size * num_blocks\\n    slot_mapping = random.sample(range(num_slots), num_tokens)\\n    slot_mapping = torch.tensor(slot_mapping, dtype=torch.int, device=\\'cuda\\')\\n\\n    qkv = torch.randn(num_tokens,\\n                      3,\\n                      num_heads,\\n                      head_size,\\n                      dtype=dtype,\\n                      device=\\'cuda\\')\\n    _, key, value = qkv.unbind(dim=1)\\n\\n    # Create the KV caches.\\n    key_caches, value_caches = kv_cache_factory(num_blocks, block_size, 1,\\n                                                num_heads, head_size, dtype,\\n                                                seed)\\n    key_cache, value_cache = key_caches[0], value_caches[0]\\n\\n    # Clone the KV caches.\\n    cloned_key_cache = key_cache.clone()\\n    cloned_value_cache = value_cache.clone()\\n\\n    # Call the reshape_and_cache kernel.\\n    cache_ops.reshape_and_cache(key, value, key_cache, value_cache,\\n                                slot_mapping)\\n\\n    # Run the reference implementation.\\n    reshaped_key = key.reshape(num_tokens, *key_cache[0, :, :, 0, :].shape)\\n    block_indicies = torch.div(slot_mapping, block_size, rounding_mode=\\'floor\\')\\n    block_indicies = block_indicies.cpu().tolist()\\n    block_offsets = slot_mapping % block_size\\n    block_offsets = block_offsets.cpu().tolist()\\n    for i in range(num_tokens):\\n        block_idx = block_indicies[i]\\n        block_offset = block_offsets[i]\\n        cloned_key_cache[block_idx, :, :, block_offset, :] = reshaped_key[i]\\n        cloned_value_cache[block_idx, :, :, block_offset] = value[i]\\n\\n    assert torch.allclose(key_cache, cloned_key_cache)\\n    assert torch.allclose(value_cache, cloned_value_cache)\\n<<End File>>\\n<<Begin file>>\\nPath:\\ntests/models/test_models.py\\n---------\\nContent:\\n\"\"\"Compare the outputs of HF and vLLM when using greedy sampling.\\n\\nRun `pytest tests/models/test_models.py --forked`.\\n\"\"\"\\nimport pytest\\n\\nMODELS = [\\n    \"facebook/opt-125m\",\\n    \"gpt2\",\\n    \"bigcode/tiny_starcoder_py\",\\n    \"EleutherAI/gpt-j-6b\",\\n    \"EleutherAI/pythia-70m\",\\n    \"bigscience/bloom-560m\",\\n    \"mosaicml/mpt-7b\",\\n    \"tiiuae/falcon-7b\",\\n    \"meta-llama/Llama-2-7b-hf\",\\n]\\n\\n\\n@pytest.mark.parametrize(\"model\", MODELS)\\n@pytest.mark.parametrize(\"dtype\", [\"half\"])\\n@pytest.mark.parametrize(\"max_tokens\", [128])\\ndef test_models(\\n    hf_runner,\\n    vllm_runner,\\n    example_prompts,\\n    model: str,\\n    dtype: str,\\n    max_tokens: int,\\n) -> None:\\n    hf_model = hf_runner(model, dtype=dtype)\\n    hf_outputs = hf_model.generate_greedy(example_prompts, max_tokens)\\n    del hf_model\\n\\n    vllm_model = vllm_runner(model, dtype=dtype)\\n    vllm_outputs = vllm_model.generate_greedy(example_prompts, max_tokens)\\n    del vllm_model\\n\\n    for i in range(len(example_prompts)):\\n        hf_output_ids, hf_output_str = hf_outputs[i]\\n        vllm_output_ids, vllm_output_str = vllm_outputs[i]\\n        assert hf_output_str == vllm_output_str, (\\n            f\"Test{i}:\\\\nHF: {hf_output_str!r}\\\\nvLLM: {vllm_output_str!r}\")\\n        assert hf_output_ids == vllm_output_ids, (\\n            f\"Test{i}:\\\\nHF: {hf_output_ids}\\\\nvLLM: {vllm_output_ids}\")\\n<<End File>>\\n<<Begin file>>\\nPath:\\ntests/async_engine/test_async_llm_engine.py\\n---------\\nContent:\\nimport asyncio\\nfrom dataclasses import dataclass\\n\\nimport pytest\\n\\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine\\n\\n\\n@dataclass\\nclass RequestOutput:\\n    request_id: int\\n    finished: bool = False\\n\\n\\nclass MockEngine:\\n\\n    def __init__(self):\\n        self.step_calls = 0\\n        self.add_request_calls = 0\\n        self.abort_request_calls = 0\\n        self.request_id = None\\n\\n    async def step_async(self):\\n        self.step_calls += 1\\n        return [RequestOutput(\\n            request_id=self.request_id)] if self.request_id else []\\n\\n    def generate(self, request_id):\\n        self.request_id = request_id\\n\\n    def stop_generating(self):\\n        self.request_id = None\\n\\n    def add_request(self, **kwargs):\\n        self.add_request_calls += 1\\n        return\\n\\n    def abort_request(self, request_id):\\n        self.abort_request_calls += 1\\n        return\\n\\n\\nclass MockAsyncLLMEngine(AsyncLLMEngine):\\n\\n    def _init_engine(self, *args, **kwargs):\\n        return MockEngine()\\n\\n\\n@pytest.mark.asyncio\\nasync def test_new_requests_event():\\n    engine = MockAsyncLLMEngine(worker_use_ray=False, engine_use_ray=False)\\n    engine.start_background_loop()\\n    await asyncio.sleep(0.01)\\n    assert engine.engine.step_calls == 0\\n\\n    await engine.add_request(\"1\", \"\", None)\\n    await asyncio.sleep(0.01)\\n    assert engine.engine.add_request_calls == 1\\n    assert engine.engine.step_calls == 1\\n\\n    await engine.add_request(\"2\", \"\", None)\\n    engine.engine.generate(\"2\")\\n    await asyncio.sleep(0)\\n    assert engine.engine.add_request_calls == 2\\n    assert engine.engine.step_calls == 2\\n    await asyncio.sleep(0)\\n    assert engine.engine.step_calls == 3\\n    engine.engine.stop_generating()\\n    await asyncio.sleep(0)\\n    assert engine.engine.step_calls == 4\\n    await asyncio.sleep(0)\\n    assert engine.engine.step_calls == 4\\n\\n    await engine.add_request(\"3\", \"\", None)\\n    await asyncio.sleep(0.01)\\n    assert engine.engine.add_request_calls == 3\\n    assert engine.engine.step_calls == 5\\n    await asyncio.sleep(0.01)\\n    assert engine.engine.add_request_calls == 3\\n    assert engine.engine.step_calls == 5\\n<<End File>>\\n<<Begin file>>\\nPath:\\ntests/async_engine/test_api_server.py\\n---------\\nContent:\\nimport subprocess\\nimport sys\\nimport time\\nfrom multiprocessing import Pool\\nfrom pathlib import Path\\n\\nimport pytest\\nimport requests\\n\\n\\ndef _query_server(prompt: str) -> dict:\\n    response = requests.post(\"http://localhost:8000/generate\",\\n                             json={\\n                                 \"prompt\": prompt,\\n                                 \"max_tokens\": 100,\\n                                 \"temperature\": 0,\\n                                 \"ignore_eos\": True\\n                             })\\n    response.raise_for_status()\\n    return response.json()\\n\\n\\n@pytest.fixture\\ndef api_server():\\n    script_path = Path(__file__).parent.joinpath(\\n        \"api_server_async_engine.py\").absolute()\\n    uvicorn_process = subprocess.Popen([\\n        sys.executable, \"-u\",\\n        str(script_path), \"--model\", \"facebook/opt-125m\"\\n    ])\\n    yield\\n    uvicorn_process.terminate()\\n\\n\\ndef test_api_server(api_server):\\n    \"\"\"\\n    Run the API server and test it.\\n\\n    We run both the server and requests in separate processes.\\n\\n    We test that the server can handle incoming requests, including\\n    multiple requests at the same time, and that it can handle requests\\n    being cancelled without crashing.\\n    \"\"\"\\n    with Pool(32) as pool:\\n        # Wait until the server is ready\\n        prompts = [\"Hello world\"] * 1\\n        result = None\\n        while not result:\\n            try:\\n                for result in pool.map(_query_server, prompts):\\n                    break\\n            except:\\n                time.sleep(1)\\n\\n        # Actual tests start here\\n        # Try with 1 prompt\\n        for result in pool.map(_query_server, prompts):\\n            assert result\\n\\n        num_aborted_requests = requests.get(\\n            \"http://localhost:8000/stats\").json()[\"num_aborted_requests\"]\\n        assert num_aborted_requests == 0\\n\\n        # Try with 100 prompts\\n        prompts = [\"Hello world\"] * 100\\n        for result in pool.map(_query_server, prompts):\\n            assert result\\n\\n        # Cancel requests\\n        pool.map_async(_query_server, prompts)\\n        time.sleep(0.01)\\n        pool.terminate()\\n        pool.join()\\n\\n        # check cancellation stats\\n        num_aborted_requests = requests.get(\\n            \"http://localhost:8000/stats\").json()[\"num_aborted_requests\"]\\n        assert num_aborted_requests > 0\\n\\n    # check that server still runs after cancellations\\n    with Pool(32) as pool:\\n        # Try with 100 prompts\\n        prompts = [\"Hello world\"] * 100\\n        for result in pool.map(_query_server, prompts):\\n            assert result\\n<<End File>>\\n<<Begin file>>\\nPath:\\ntests/async_engine/api_server_async_engine.py\\n---------\\nContent:\\n\"\"\"vllm.entrypoints.api_server with some extra logging for testing.\"\"\"\\nimport argparse\\nfrom typing import Any, Dict\\n\\nimport uvicorn\\nfrom fastapi.responses import JSONResponse, Response\\n\\nimport vllm.entrypoints.api_server\\nfrom vllm.engine.arg_utils import AsyncEngineArgs\\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine\\n\\napp = vllm.entrypoints.api_server.app\\n\\n\\nclass AsyncLLMEngineWithStats(AsyncLLMEngine):\\n\\n    def __init__(self, *args, **kwargs):\\n        super().__init__(*args, **kwargs)\\n        self._num_aborts = 0\\n\\n    async def abort(self, request_id: str) -> None:\\n        await super().abort(request_id)\\n        self._num_aborts += 1\\n\\n    def testing_stats(self) -> Dict[str, Any]:\\n        return {\"num_aborted_requests\": self._num_aborts}\\n\\n\\n@app.get(\"/stats\")\\ndef stats() -> Response:\\n    \"\"\"Get the statistics of the engine.\"\"\"\\n    return JSONResponse(engine.testing_stats())\\n\\n\\nif __name__ == \"__main__\":\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\\n    parser.add_argument(\"--port\", type=int, default=8000)\\n    parser = AsyncEngineArgs.add_cli_args(parser)\\n    args = parser.parse_args()\\n\\n    engine_args = AsyncEngineArgs.from_cli_args(args)\\n    engine = AsyncLLMEngineWithStats.from_engine_args(engine_args)\\n    vllm.entrypoints.api_server.engine = engine\\n    uvicorn.run(\\n        app,\\n        host=args.host,\\n        port=args.port,\\n        log_level=\"debug\",\\n        timeout_keep_alive=vllm.entrypoints.api_server.TIMEOUT_KEEP_ALIVE)\\n<<End File>>\\n<<Begin file>>\\nPath:\\ntests/async_engine/test_request_tracker.py\\n---------\\nContent:\\nimport pytest\\n\\nfrom vllm.engine.async_llm_engine import RequestTracker\\nfrom vllm.outputs import RequestOutput\\n\\n\\nclass DummyEvent:\\n\\n    def __init__(self):\\n        self._flag = False\\n\\n    def set(self):\\n        self._flag = True\\n\\n    def clear(self):\\n        self._flag = False\\n\\n\\ndef test_request_tracker():\\n    tracker = RequestTracker()\\n    tracker.new_requests_event = DummyEvent()\\n    stream_1 = tracker.add_request(\"1\")\\n    assert tracker.new_requests_event._flag\\n    new, finished = tracker.get_new_and_finished_requests()\\n    assert not tracker.new_requests_event._flag\\n    assert len(new) == 1\\n    assert new[0][\"request_id\"] == \"1\"\\n    assert not finished\\n    assert not stream_1.finished\\n\\n    stream_2 = tracker.add_request(\"2\")\\n    stream_3 = tracker.add_request(\"3\")\\n    assert tracker.new_requests_event._flag\\n    new, finished = tracker.get_new_and_finished_requests()\\n    assert not tracker.new_requests_event._flag\\n    assert len(new) == 2\\n    assert new[0][\"request_id\"] == \"2\"\\n    assert new[1][\"request_id\"] == \"3\"\\n    assert not finished\\n    assert not stream_2.finished\\n    assert not stream_3.finished\\n\\n    # request_ids must be unique\\n    with pytest.raises(KeyError):\\n        tracker.add_request(\"1\")\\n    assert not tracker.new_requests_event._flag\\n\\n    tracker.abort_request(\"1\")\\n    new, finished = tracker.get_new_and_finished_requests()\\n    assert len(finished) == 1\\n    assert \"1\" in finished\\n    assert not new\\n    assert stream_1.finished\\n\\n    stream_4 = tracker.add_request(\"4\")\\n    tracker.abort_request(\"4\")\\n    assert tracker.new_requests_event._flag\\n    new, finished = tracker.get_new_and_finished_requests()\\n    assert len(finished) == 1\\n    assert \"4\" in finished\\n    assert not new\\n    assert stream_4.finished\\n\\n    stream_5 = tracker.add_request(\"5\")\\n    assert tracker.new_requests_event._flag\\n    tracker.process_request_output(\\n        RequestOutput(\"2\", \"output\", [], [], finished=True))\\n    new, finished = tracker.get_new_and_finished_requests()\\n    assert not tracker.new_requests_event._flag\\n    assert len(finished) == 1\\n    assert \"2\" in finished\\n    assert len(new) == 1\\n    assert new[0][\"request_id\"] == \"5\"\\n    assert stream_2.finished\\n    assert not stream_5.finished\\n<<End File>>\\n<<Begin file>>\\nPath:\\ntests/samplers/test_beam_search.py\\n---------\\nContent:\\n\"\"\"Compare the outputs of HF and vLLM when using beam search.\\n\\nRun `pytest tests/samplers/test_beam_search.py --forked`.\\n\"\"\"\\nimport pytest\\n\\n# FIXME(zhuohan): The test can not pass if we:\\n#   1. Increase max_tokens to 256.\\n#   2. Increase beam_width to 8.\\n#   3. Use the model \"huggyllama/llama-7b\".\\nMAX_TOKENS = [128]\\nBEAM_WIDTHS = [4]\\nMODELS = [\"facebook/opt-125m\"]\\n\\n\\n@pytest.mark.parametrize(\"model\", MODELS)\\n@pytest.mark.parametrize(\"dtype\", [\"half\"])\\n@pytest.mark.parametrize(\"max_tokens\", MAX_TOKENS)\\n@pytest.mark.parametrize(\"beam_width\", BEAM_WIDTHS)\\ndef test_beam_search_single_input(\\n    hf_runner,\\n    vllm_runner,\\n    example_prompts,\\n    model: str,\\n    dtype: str,\\n    max_tokens: int,\\n    beam_width: int,\\n) -> None:\\n    hf_model = hf_runner(model, dtype=dtype)\\n    hf_outputs = hf_model.generate_beam_search(example_prompts, beam_width,\\n                                               max_tokens)\\n    del hf_model\\n\\n    vllm_model = vllm_runner(model, dtype=dtype)\\n    vllm_outputs = vllm_model.generate_beam_search(example_prompts, beam_width,\\n                                                   max_tokens)\\n    del vllm_model\\n\\n    for i in range(len(example_prompts)):\\n        hf_output_ids, _ = hf_outputs[i]\\n        vllm_output_ids, _ = vllm_outputs[i]\\n        assert len(hf_output_ids) == len(vllm_output_ids)\\n        for j in range(len(hf_output_ids)):\\n            assert hf_output_ids[j] == vllm_output_ids[j], (\\n                f\"Test{i} output{j}:\\\\nHF: {hf_output_ids}\\\\n\"\\n                f\"vLLM: {vllm_output_ids}\")\\n<<End File>>\\n<<Begin file>>\\nPath:\\ntests/engine/test_detokenize.py\\n---------\\nContent:\\nimport pytest\\n\\nfrom transformers import AutoTokenizer\\n\\nfrom vllm.transformers_utils.tokenizer import detokenize_incrementally\\n\\nTRUTH = [\\n    \"Hello here, this is a simple test\",\\n    \"vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving\",\\n    \"\"\\n]\\nTOKENIZERS = [\\n    \"facebook/opt-125m\",\\n    \"gpt2\",\\n    \"bigcode/tiny_starcoder_py\",\\n    \"EleutherAI/gpt-j-6b\",\\n    \"EleutherAI/pythia-70m\",\\n    \"bigscience/bloom-560m\",\\n    \"mosaicml/mpt-7b\",\\n    \"tiiuae/falcon-7b\",\\n    \"meta-llama/Llama-2-7b-hf\",\\n    \"codellama/CodeLlama-7b-hf\",\\n]\\n\\n\\ndef _run_incremental_decode(tokenizer, all_input_ids,\\n                            skip_special_tokens: bool):\\n    decoded_text = \"\"\\n    offset = 0\\n    token_offset = 0\\n    prev_tokens = None\\n    for i in range(len(all_input_ids)):\\n        new_tokens, text, offset, token_offset = detokenize_incrementally(\\n            tokenizer,\\n            all_input_ids[:i + 1],\\n            prev_tokens,\\n            offset,\\n            token_offset,\\n            skip_special_tokens=skip_special_tokens)\\n        decoded_text += text\\n        if prev_tokens is None:\\n            prev_tokens = new_tokens\\n        else:\\n            prev_tokens += new_tokens\\n    return decoded_text\\n\\n\\n@pytest.mark.parametrize(\"truth\", TRUTH)\\n@pytest.mark.parametrize(\"tokenizer_id\", TOKENIZERS)\\n@pytest.mark.parametrize(\"skip_special_tokens\", (True, False))\\ndef test_decode_streaming(tokenizer_id, truth, skip_special_tokens):\\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\\n    all_input_ids = tokenizer(truth, add_special_tokens=False)[\"input_ids\"]\\n    if skip_special_tokens:\\n        all_input_ids = ([tokenizer.bos_token_id]\\n                         if tokenizer.bos_token_id is not None else\\n                         []) + all_input_ids + [tokenizer.eos_token_id]\\n\\n    decoded_text = _run_incremental_decode(\\n        tokenizer, all_input_ids, skip_special_tokens=skip_special_tokens)\\n\\n    assert decoded_text == truth\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/config.py\\n---------\\nContent:\\nfrom typing import Optional\\n\\nimport torch\\nfrom transformers import PretrainedConfig\\n\\nfrom vllm.logger import init_logger\\nfrom vllm.transformers_utils.config import get_config\\nfrom vllm.utils import get_cpu_memory\\n\\nlogger = init_logger(__name__)\\n\\n_GB = 1 << 30\\n\\n\\nclass ModelConfig:\\n    \"\"\"Configuration for the model.\\n\\n    Args:\\n        model: Name or path of the huggingface model to use.\\n        tokenizer: Name or path of the huggingface tokenizer to use.\\n        tokenizer_mode: Tokenizer mode. \"auto\" will use the fast tokenizer if\\n            available, and \"slow\" will always use the slow tokenizer.\\n        trust_remote_code: Trust remote code (e.g., from HuggingFace) when\\n            downloading the model and tokenizer.\\n        download_dir: Directory to download and load the weights, default to the\\n            default cache directory of huggingface.\\n        load_format: The format of the model weights to load:\\n            \"auto\" will try to load the weights in the safetensors format and\\n                fall back to the pytorch bin format if safetensors format is\\n                not available.\\n            \"pt\" will load the weights in the pytorch bin format.\\n            \"safetensors\" will load the weights in the safetensors format.\\n            \"npcache\" will load the weights in pytorch format and store\\n                a numpy cache to speed up the loading.\\n            \"dummy\" will initialize the weights with random values, which is\\n                mainly for profiling.\\n        dtype: Data type for model weights and activations. The \"auto\" option\\n            will use FP16 precision for FP32 and FP16 models, and BF16 precision\\n            for BF16 models.\\n        seed: Random seed for reproducibility.\\n        revision: The specific model version to use. It can be a branch name,\\n            a tag name, or a commit id. If unspecified, will use the default\\n            version.\\n        max_model_len: Maximum length of a sequence (including prompt and\\n            output). If None, will be derived from the model.\\n        quantization: Quantization method that was used to quantize the model\\n            weights. If None, we assume the model weights are not quantized.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        model: str,\\n        tokenizer: str,\\n        tokenizer_mode: str,\\n        trust_remote_code: bool,\\n        download_dir: Optional[str],\\n        load_format: str,\\n        dtype: str,\\n        seed: int,\\n        revision: Optional[str],\\n        max_model_len: Optional[int] = None,\\n        quantization: Optional[str] = None,\\n    ) -> None:\\n        self.model = model\\n        self.tokenizer = tokenizer\\n        self.tokenizer_mode = tokenizer_mode\\n        self.trust_remote_code = trust_remote_code\\n        self.download_dir = download_dir\\n        self.load_format = load_format\\n        self.seed = seed\\n        self.revision = revision\\n        self.quantization = quantization\\n\\n        self.hf_config = get_config(model, trust_remote_code, revision)\\n        self.dtype = _get_and_verify_dtype(self.hf_config, dtype)\\n        self._verify_load_format()\\n        self._verify_tokenizer_mode()\\n        self._verify_quantization()\\n        self.max_model_len = None\\n        if max_model_len is not None:\\n            derived_max_model_len = self.get_max_model_len()\\n            if max_model_len > derived_max_model_len:\\n                logger.warning(\\n                    f\"User-specified max_model_len ({max_model_len}) is \"\\n                    f\"greater than the derived max_model_len \"\\n                    f\"({derived_max_model_len}). Make sure the value is \"\\n                    \"correct and within the model context size.\")\\n        self.max_model_len = max_model_len\\n\\n    def _verify_load_format(self) -> None:\\n        load_format = self.load_format.lower()\\n        if load_format not in [\\n                \"auto\", \"pt\", \"safetensors\", \"npcache\", \"dummy\"\\n        ]:\\n            raise ValueError(\\n                f\"Unknown load format: {self.load_format}. Must be one of \"\\n                \"\\'auto\\', \\'pt\\', \\'safetensors\\', \\'npcache\\', or \\'dummy\\'.\")\\n        self.load_format = load_format\\n\\n    def _verify_tokenizer_mode(self) -> None:\\n        tokenizer_mode = self.tokenizer_mode.lower()\\n        if tokenizer_mode not in [\"auto\", \"slow\"]:\\n            raise ValueError(\\n                f\"Unknown tokenizer mode: {self.tokenizer_mode}. Must be \"\\n                \"either \\'auto\\' or \\'slow\\'.\")\\n        self.tokenizer_mode = tokenizer_mode\\n\\n    def _verify_quantization(self) -> None:\\n        supported_quantization = [\"awq\"]\\n        if self.quantization is None:\\n            return\\n        quantization = self.quantization.lower()\\n        if quantization not in supported_quantization:\\n            raise ValueError(\\n                f\"Unknown quantization: {self.quantization}. Must be one of \"\\n                f\"{supported_quantization}.\")\\n        self.quantization = quantization\\n\\n    def verify_with_parallel_config(\\n        self,\\n        parallel_config: \"ParallelConfig\",\\n    ) -> None:\\n        total_num_attention_heads = self.hf_config.num_attention_heads\\n        tensor_parallel_size = parallel_config.tensor_parallel_size\\n        if total_num_attention_heads % tensor_parallel_size != 0:\\n            raise ValueError(\\n                f\"Total number of attention heads ({total_num_attention_heads})\"\\n                \" must be divisible by tensor parallel size \"\\n                f\"({tensor_parallel_size}).\")\\n\\n        total_num_hidden_layers = self.hf_config.num_hidden_layers\\n        pipeline_parallel_size = parallel_config.pipeline_parallel_size\\n        if total_num_hidden_layers % pipeline_parallel_size != 0:\\n            raise ValueError(\\n                f\"Total number of hidden layers ({total_num_hidden_layers}) \"\\n                \"must be divisible by pipeline parallel size \"\\n                f\"({pipeline_parallel_size}).\")\\n\\n    def get_hidden_size(self) -> int:\\n        return self.hf_config.hidden_size\\n\\n    def get_head_size(self) -> int:\\n        # FIXME(woosuk): This may not be true for all models.\\n        return self.hf_config.hidden_size // self.hf_config.num_attention_heads\\n\\n    def get_num_heads(self, parallel_config: \"ParallelConfig\") -> int:\\n        # For GPTBigCode & Falcon:\\n        # Note: for falcon, when new_decoder_architecture is True, the\\n        # multi_query flag is ignored and we use n_head_kv for the number of\\n        # KV heads.\\n        falcon_model_types = [\"falcon\", \"RefinedWeb\", \"RefinedWebModel\"]\\n        new_decoder_arch_falcon = (\\n            self.hf_config.model_type in falcon_model_types\\n            and getattr(self.hf_config, \"new_decoder_architecture\", False))\\n        if not new_decoder_arch_falcon and getattr(self.hf_config,\\n                                                   \"multi_query\", False):\\n            # Multi-query attention, only one KV head.\\n            return 1\\n        # For Falcon:\\n        if getattr(self.hf_config, \"n_head_kv\", None) is not None:\\n            return (self.hf_config.n_head_kv //\\n                    parallel_config.tensor_parallel_size)\\n        # For LLaMA-2:\\n        if getattr(self.hf_config, \"num_key_value_heads\", None) is not None:\\n            return (self.hf_config.num_key_value_heads //\\n                    parallel_config.tensor_parallel_size)\\n        total_num_attention_heads = self.hf_config.num_attention_heads\\n        return total_num_attention_heads // parallel_config.tensor_parallel_size\\n\\n    def get_max_model_len(self) -> int:\\n        if self.max_model_len is not None:\\n            return self.max_model_len\\n        max_model_len = float(\"inf\")\\n        possible_keys = [\\n            # OPT\\n            \"max_position_embeddings\",\\n            # GPT-2\\n            \"n_positions\",\\n            # MPT\\n            \"max_seq_len\",\\n            # Others\\n            \"max_sequence_length\",\\n            \"max_seq_length\",\\n            \"seq_len\",\\n        ]\\n        for key in possible_keys:\\n            max_len_key = getattr(self.hf_config, key, None)\\n            if max_len_key is not None:\\n                max_model_len = min(max_model_len, max_len_key)\\n        return max_model_len\\n\\n    def get_num_layers(self, parallel_config: \"ParallelConfig\") -> int:\\n        total_num_hidden_layers = self.hf_config.num_hidden_layers\\n        return total_num_hidden_layers // parallel_config.pipeline_parallel_size\\n\\n\\nclass CacheConfig:\\n    \"\"\"Configuration for the KV cache.\\n\\n    Args:\\n        block_size: Size of a cache block in number of tokens.\\n        gpu_memory_utilization: Fraction of GPU memory to use for the\\n            vLLM execution.\\n        swap_space: Size of the CPU swap space per GPU (in GiB).\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        block_size: int,\\n        gpu_memory_utilization: float,\\n        swap_space: int,\\n    ) -> None:\\n        self.block_size = block_size\\n        self.gpu_memory_utilization = gpu_memory_utilization\\n        self.swap_space_bytes = swap_space * _GB\\n        self._verify_args()\\n\\n        # Will be set after profiling.\\n        self.num_gpu_blocks = None\\n        self.num_cpu_blocks = None\\n\\n    def _verify_args(self) -> None:\\n        if self.gpu_memory_utilization > 1.0:\\n            raise ValueError(\\n                \"GPU memory utilization must be less than 1.0. Got \"\\n                f\"{self.gpu_memory_utilization}.\")\\n\\n    def verify_with_parallel_config(\\n        self,\\n        parallel_config: \"ParallelConfig\",\\n    ) -> None:\\n        total_cpu_memory = get_cpu_memory()\\n        # FIXME(woosuk): Here, it is assumed that the GPUs in a tensor parallel\\n        # group are in the same node. However, the GPUs may span multiple nodes.\\n        num_gpus_per_node = parallel_config.tensor_parallel_size\\n        cpu_memory_usage = self.swap_space_bytes * num_gpus_per_node\\n\\n        msg = (f\"{cpu_memory_usage / _GB:.2f} GiB out of \"\\n               f\"the {total_cpu_memory / _GB:.2f} GiB total CPU memory is \"\\n               \"allocated for the swap space.\")\\n        if cpu_memory_usage > 0.7 * total_cpu_memory:\\n            raise ValueError(\"Too large swap space. \" + msg)\\n        elif cpu_memory_usage > 0.4 * total_cpu_memory:\\n            logger.warning(\"Possibly too large swap space. \" + msg)\\n\\n\\nclass ParallelConfig:\\n    \"\"\"Configuration for the distributed execution.\\n\\n    Args:\\n        pipeline_parallel_size: Number of pipeline parallel groups.\\n        tensor_parallel_size: Number of tensor parallel groups.\\n        worker_use_ray: Whether to use Ray for model workers. Will be set to\\n            True if either pipeline_parallel_size or tensor_parallel_size is\\n            greater than 1.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        pipeline_parallel_size: int,\\n        tensor_parallel_size: int,\\n        worker_use_ray: bool,\\n    ) -> None:\\n        self.pipeline_parallel_size = pipeline_parallel_size\\n        self.tensor_parallel_size = tensor_parallel_size\\n        self.worker_use_ray = worker_use_ray\\n\\n        self.world_size = pipeline_parallel_size * tensor_parallel_size\\n        if self.world_size > 1:\\n            self.worker_use_ray = True\\n        self._verify_args()\\n\\n    def _verify_args(self) -> None:\\n        if self.pipeline_parallel_size > 1:\\n            raise NotImplementedError(\\n                \"Pipeline parallelism is not supported yet.\")\\n\\n\\nclass SchedulerConfig:\\n    \"\"\"Scheduler configuration.\\n\\n    Args:\\n        max_num_batched_tokens: Maximum number of tokens to be processed in\\n            a single iteration.\\n        max_num_seqs: Maximum number of sequences to be processed in a single\\n            iteration.\\n        max_model_len: Maximum length of a sequence (including prompt\\n            and generated text).\\n    \"\"\"\\n\\n    def __init__(self, max_num_batched_tokens: int, max_num_seqs: int,\\n                 max_model_len: int) -> None:\\n        self.max_num_batched_tokens = max_num_batched_tokens\\n        self.max_num_seqs = max_num_seqs\\n        self.max_model_len = max_model_len\\n\\n\\n_STR_DTYPE_TO_TORCH_DTYPE = {\\n    \"half\": torch.float16,\\n    \"float16\": torch.float16,\\n    \"float\": torch.float32,\\n    \"float32\": torch.float32,\\n    \"bfloat16\": torch.bfloat16,\\n}\\n\\n\\ndef _get_and_verify_dtype(\\n    config: PretrainedConfig,\\n    dtype: str,\\n) -> torch.dtype:\\n    # NOTE: getattr(config, \"torch_dtype\", torch.float32) is not correct\\n    # because config.torch_dtype can be None.\\n    config_dtype = getattr(config, \"torch_dtype\", None)\\n    if config_dtype is None:\\n        config_dtype = torch.float32\\n\\n    dtype = dtype.lower()\\n    if dtype == \"auto\":\\n        if config_dtype == torch.float32:\\n            # Following the common practice, we use float16 for float32 models.\\n            torch_dtype = torch.float16\\n        else:\\n            torch_dtype = config_dtype\\n    else:\\n        if dtype not in _STR_DTYPE_TO_TORCH_DTYPE:\\n            raise ValueError(f\"Unknown dtype: {dtype}\")\\n        torch_dtype = _STR_DTYPE_TO_TORCH_DTYPE[dtype]\\n\\n    # Verify the dtype.\\n    if torch_dtype != config_dtype:\\n        if torch_dtype == torch.float32:\\n            # Upcasting to float32 is allowed.\\n            pass\\n        elif config_dtype == torch.float32:\\n            # Downcasting from float32 to float16 or bfloat16 is allowed.\\n            pass\\n        else:\\n            # Casting between float16 and bfloat16 is allowed with a warning.\\n            logger.warning(f\"Casting {config_dtype} to {torch_dtype}.\")\\n\\n    # Check if the GPU supports the dtype.\\n    if torch_dtype == torch.bfloat16:\\n        compute_capability = torch.cuda.get_device_capability()\\n        if compute_capability[0] < 8:\\n            gpu_name = torch.cuda.get_device_name()\\n            raise ValueError(\\n                \"Bfloat16 is only supported on GPUs with compute capability \"\\n                f\"of at least 8.0. Your {gpu_name} GPU has compute capability \"\\n                f\"{compute_capability[0]}.{compute_capability[1]}.\")\\n    return torch_dtype\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/sampling_params.py\\n---------\\nContent:\\n\"\"\"Sampling parameters for text generation.\"\"\"\\nfrom typing import List, Optional, Union\\n\\n_SAMPLING_EPS = 1e-5\\n\\n\\nclass SamplingParams:\\n    \"\"\"Sampling parameters for text generation.\\n\\n    Overall, we follow the sampling parameters from the OpenAI text completion\\n    API (https://platform.openai.com/docs/api-reference/completions/create).\\n    In addition, we support beam search, which is not supported by OpenAI.\\n\\n    Args:\\n        n: Number of output sequences to return for the given prompt.\\n        best_of: Number of output sequences that are generated from the prompt.\\n            From these `best_of` sequences, the top `n` sequences are returned.\\n            `best_of` must be greater than or equal to `n`. This is treated as\\n            the beam width when `use_beam_search` is True. By default, `best_of`\\n            is set to `n`.\\n        presence_penalty: Float that penalizes new tokens based on whether they\\n            appear in the generated text so far. Values > 0 encourage the model\\n            to use new tokens, while values < 0 encourage the model to repeat\\n            tokens.\\n        frequency_penalty: Float that penalizes new tokens based on their\\n            frequency in the generated text so far. Values > 0 encourage the\\n            model to use new tokens, while values < 0 encourage the model to\\n            repeat tokens.\\n        temperature: Float that controls the randomness of the sampling. Lower\\n            values make the model more deterministic, while higher values make\\n            the model more random. Zero means greedy sampling.\\n        top_p: Float that controls the cumulative probability of the top tokens\\n            to consider. Must be in (0, 1]. Set to 1 to consider all tokens.\\n        top_k: Integer that controls the number of top tokens to consider. Set\\n            to -1 to consider all tokens.\\n        use_beam_search: Whether to use beam search instead of sampling.\\n        length_penalty: Float that penalizes sequences based on their length.\\n            Used in beam search.\\n        early_stopping: Controls the stopping condition for beam search. It\\n            accepts the following values: `True`, where the generation stops as\\n            soon as there are `best_of` complete candidates; `False`, where an\\n            heuristic is applied and the generation stops when is it very\\n            unlikely to find better candidates; `\"never\"`, where the beam search\\n            procedure only stops when there cannot be better candidates\\n            (canonical beam search algorithm).\\n        stop: List of strings that stop the generation when they are generated.\\n            The returned output will not contain the stop strings.\\n        ignore_eos: Whether to ignore the EOS token and continue generating\\n            tokens after the EOS token is generated.\\n        max_tokens: Maximum number of tokens to generate per output sequence.\\n        logprobs: Number of log probabilities to return per output token.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        n: int = 1,\\n        best_of: Optional[int] = None,\\n        presence_penalty: float = 0.0,\\n        frequency_penalty: float = 0.0,\\n        temperature: float = 1.0,\\n        top_p: float = 1.0,\\n        top_k: int = -1,\\n        use_beam_search: bool = False,\\n        length_penalty: float = 1.0,\\n        early_stopping: Union[bool, str] = False,\\n        stop: Union[None, str, List[str]] = None,\\n        ignore_eos: bool = False,\\n        max_tokens: int = 16,\\n        logprobs: Optional[int] = None,\\n    ) -> None:\\n        self.n = n\\n        self.best_of = best_of if best_of is not None else n\\n        self.presence_penalty = presence_penalty\\n        self.frequency_penalty = frequency_penalty\\n        self.temperature = temperature\\n        self.top_p = top_p\\n        self.top_k = top_k\\n        self.use_beam_search = use_beam_search\\n        self.length_penalty = length_penalty\\n        self.early_stopping = early_stopping\\n        if stop is None:\\n            self.stop = []\\n        elif isinstance(stop, str):\\n            self.stop = [stop]\\n        else:\\n            self.stop = list(stop)\\n        self.ignore_eos = ignore_eos\\n        self.max_tokens = max_tokens\\n        self.logprobs = logprobs\\n\\n        self._verify_args()\\n        if self.use_beam_search:\\n            self._verify_beam_search()\\n        else:\\n            self._verify_non_beam_search()\\n            if self.temperature < _SAMPLING_EPS:\\n                # Zero temperature means greedy sampling.\\n                self._verify_greedy_sampling()\\n\\n    def _verify_args(self) -> None:\\n        if self.n < 1:\\n            raise ValueError(f\"n must be at least 1, got {self.n}.\")\\n        if self.best_of < self.n:\\n            raise ValueError(f\"best_of must be greater than or equal to n, \"\\n                             f\"got n={self.n} and best_of={self.best_of}.\")\\n        if not -2.0 <= self.presence_penalty <= 2.0:\\n            raise ValueError(\"presence_penalty must be in [-2, 2], got \"\\n                             f\"{self.presence_penalty}.\")\\n        if not -2.0 <= self.frequency_penalty <= 2.0:\\n            raise ValueError(\"frequency_penalty must be in [-2, 2], got \"\\n                             f\"{self.frequency_penalty}.\")\\n        if self.temperature < 0.0:\\n            raise ValueError(\\n                f\"temperature must be non-negative, got {self.temperature}.\")\\n        if not 0.0 < self.top_p <= 1.0:\\n            raise ValueError(f\"top_p must be in (0, 1], got {self.top_p}.\")\\n        if self.top_k < -1 or self.top_k == 0:\\n            raise ValueError(f\"top_k must be -1 (disable), or at least 1, \"\\n                             f\"got {self.top_k}.\")\\n        if self.max_tokens < 1:\\n            raise ValueError(\\n                f\"max_tokens must be at least 1, got {self.max_tokens}.\")\\n        if self.logprobs is not None and self.logprobs < 0:\\n            raise ValueError(\\n                f\"logprobs must be non-negative, got {self.logprobs}.\")\\n\\n    def _verify_beam_search(self) -> None:\\n        if self.best_of == 1:\\n            raise ValueError(\"best_of must be greater than 1 when using beam \"\\n                             f\"search. Got {self.best_of}.\")\\n        if self.temperature > _SAMPLING_EPS:\\n            raise ValueError(\"temperature must be 0 when using beam search.\")\\n        if self.top_p < 1.0 - _SAMPLING_EPS:\\n            raise ValueError(\"top_p must be 1 when using beam search.\")\\n        if self.top_k != -1:\\n            raise ValueError(\"top_k must be -1 when using beam search.\")\\n        if self.early_stopping not in [True, False, \"never\"]:\\n            raise ValueError(\\n                f\"early_stopping must be True, False, or \\'never\\', \"\\n                f\"got {self.early_stopping}.\")\\n\\n    def _verify_non_beam_search(self) -> None:\\n        if self.early_stopping is not False:\\n            raise ValueError(\"early_stopping is not effective and must be \"\\n                             \"False when not using beam search.\")\\n        if (self.length_penalty < 1.0 - _SAMPLING_EPS\\n                or self.length_penalty > 1.0 + _SAMPLING_EPS):\\n            raise ValueError(\\n                \"length_penalty is not effective and must be the \"\\n                \"default value of 1.0 when not using beam search.\")\\n\\n    def _verify_greedy_sampling(self) -> None:\\n        if self.best_of > 1:\\n            raise ValueError(\"best_of must be 1 when using greedy sampling.\"\\n                             f\"Got {self.best_of}.\")\\n        if self.top_p < 1.0 - _SAMPLING_EPS:\\n            raise ValueError(\"top_p must be 1 when using greedy sampling.\")\\n        if self.top_k != -1:\\n            raise ValueError(\"top_k must be -1 when using greedy sampling.\")\\n\\n    def __repr__(self) -> str:\\n        return (f\"SamplingParams(n={self.n}, \"\\n                f\"best_of={self.best_of}, \"\\n                f\"presence_penalty={self.presence_penalty}, \"\\n                f\"frequency_penalty={self.frequency_penalty}, \"\\n                f\"temperature={self.temperature}, \"\\n                f\"top_p={self.top_p}, \"\\n                f\"top_k={self.top_k}, \"\\n                f\"use_beam_search={self.use_beam_search}, \"\\n                f\"length_penalty={self.length_penalty}, \"\\n                f\"early_stopping={self.early_stopping}, \"\\n                f\"stop={self.stop}, \"\\n                f\"ignore_eos={self.ignore_eos}, \"\\n                f\"max_tokens={self.max_tokens}, \"\\n                f\"logprobs={self.logprobs})\")\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/sequence.py\\n---------\\nContent:\\n\"\"\"Sequence and its related classes.\"\"\"\\nimport copy\\nimport enum\\nfrom typing import Dict, List, Optional, Union\\n\\nfrom vllm.block import LogicalTokenBlock\\nfrom vllm.sampling_params import SamplingParams\\n\\n\\nclass SequenceStatus(enum.Enum):\\n    \"\"\"Status of a sequence.\"\"\"\\n    WAITING = enum.auto()\\n    RUNNING = enum.auto()\\n    SWAPPED = enum.auto()\\n    FINISHED_STOPPED = enum.auto()\\n    FINISHED_LENGTH_CAPPED = enum.auto()\\n    FINISHED_ABORTED = enum.auto()\\n    FINISHED_IGNORED = enum.auto()\\n\\n    @staticmethod\\n    def is_finished(status: \"SequenceStatus\") -> bool:\\n        return status in [\\n            SequenceStatus.FINISHED_STOPPED,\\n            SequenceStatus.FINISHED_LENGTH_CAPPED,\\n            SequenceStatus.FINISHED_ABORTED,\\n            SequenceStatus.FINISHED_IGNORED,\\n        ]\\n\\n    @staticmethod\\n    def get_finished_reason(status: \"SequenceStatus\") -> Union[str, None]:\\n        if status == SequenceStatus.FINISHED_STOPPED:\\n            finish_reason = \"stop\"\\n        elif status == SequenceStatus.FINISHED_LENGTH_CAPPED:\\n            finish_reason = \"length\"\\n        elif status == SequenceStatus.FINISHED_ABORTED:\\n            finish_reason = \"abort\"\\n        elif status == SequenceStatus.FINISHED_IGNORED:\\n            finish_reason = \"length\"\\n        else:\\n            finish_reason = None\\n        return finish_reason\\n\\n\\nclass SequenceData:\\n    \"\"\"Data associated with a sequence.\\n\\n\\n    Args:\\n        prompt_token_ids: The token IDs of the prompt.\\n\\n    Attributes:\\n        prompt_token_ids: The token IDs of the prompt.\\n        output_token_ids: The token IDs of the output.\\n        cumulative_logprob: The cumulative log probability of the output.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        prompt_token_ids: List[int],\\n    ) -> None:\\n        self.prompt_token_ids = prompt_token_ids\\n        self.output_token_ids: List[int] = []\\n        self.cumulative_logprob = 0.0\\n\\n    def append_token_id(self, token_id: int, logprob: float) -> None:\\n        self.output_token_ids.append(token_id)\\n        self.cumulative_logprob += logprob\\n\\n    def get_len(self) -> int:\\n        return len(self.output_token_ids) + len(self.prompt_token_ids)\\n\\n    def get_prompt_len(self) -> int:\\n        return len(self.prompt_token_ids)\\n\\n    def get_output_len(self) -> int:\\n        return len(self.output_token_ids)\\n\\n    def get_token_ids(self) -> List[int]:\\n        return self.prompt_token_ids + self.output_token_ids\\n\\n    def get_last_token_id(self) -> int:\\n        if not self.output_token_ids:\\n            return self.prompt_token_ids[-1]\\n        return self.output_token_ids[-1]\\n\\n    def __repr__(self) -> str:\\n        return (f\"SequenceData(\"\\n                f\"prompt_token_ids={self.prompt_token_ids}, \"\\n                f\"output_token_ids={self.output_token_ids}, \"\\n                f\"cumulative_logprob={self.cumulative_logprob})\")\\n\\n\\nclass Sequence:\\n    \"\"\"Stores the data, status, and block information of a sequence.\\n\\n    Args:\\n        seq_id: The ID of the sequence.\\n        prompt: The prompt of the sequence.\\n        prompt_token_ids: The token IDs of the prompt.\\n        block_size: The block size of the sequence. Should be the same as the\\n            block size used by the block manager and cache engine.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        seq_id: int,\\n        prompt: str,\\n        prompt_token_ids: List[int],\\n        block_size: int,\\n    ) -> None:\\n        self.seq_id = seq_id\\n        self.prompt = prompt\\n        self.block_size = block_size\\n\\n        self.data = SequenceData(prompt_token_ids)\\n        self.output_logprobs: List[Dict[int, float]] = []\\n        self.output_text = \"\"\\n\\n        self.logical_token_blocks: List[LogicalTokenBlock] = []\\n        # Initialize the logical token blocks with the prompt token ids.\\n        self._append_tokens_to_blocks(prompt_token_ids)\\n        self.status = SequenceStatus.WAITING\\n\\n        # Used for incremental detokenization\\n        self.prefix_offset = 0\\n        self.read_offset = 0\\n        # Input + output tokens\\n        self.tokens: Optional[List[str]] = None\\n\\n    def _append_logical_block(self) -> None:\\n        block = LogicalTokenBlock(\\n            block_number=len(self.logical_token_blocks),\\n            block_size=self.block_size,\\n        )\\n        self.logical_token_blocks.append(block)\\n\\n    def _append_tokens_to_blocks(self, token_ids: List[int]) -> None:\\n        cursor = 0\\n        while cursor < len(token_ids):\\n            if not self.logical_token_blocks:\\n                self._append_logical_block()\\n\\n            last_block = self.logical_token_blocks[-1]\\n            if last_block.is_full():\\n                self._append_logical_block()\\n                last_block = self.logical_token_blocks[-1]\\n\\n            num_empty_slots = last_block.get_num_empty_slots()\\n            last_block.append_tokens(token_ids[cursor:cursor +\\n                                               num_empty_slots])\\n            cursor += num_empty_slots\\n\\n    def append_token_id(\\n        self,\\n        token_id: int,\\n        logprobs: Dict[int, float],\\n    ) -> None:\\n        assert token_id in logprobs\\n        self._append_tokens_to_blocks([token_id])\\n        self.output_logprobs.append(logprobs)\\n        self.data.append_token_id(token_id, logprobs[token_id])\\n\\n    def get_len(self) -> int:\\n        return self.data.get_len()\\n\\n    def get_prompt_len(self) -> int:\\n        return self.data.get_prompt_len()\\n\\n    def get_output_len(self) -> int:\\n        return self.data.get_output_len()\\n\\n    def get_token_ids(self) -> List[int]:\\n        return self.data.get_token_ids()\\n\\n    def get_last_token_id(self) -> int:\\n        return self.data.get_last_token_id()\\n\\n    def get_output_token_ids(self) -> List[int]:\\n        return self.data.output_token_ids\\n\\n    def get_cumulative_logprob(self) -> float:\\n        return self.data.cumulative_logprob\\n\\n    def get_beam_search_score(self,\\n                              length_penalty: float = 0.0,\\n                              seq_len: Optional[int] = None,\\n                              eos_token_id: Optional[int] = None) -> float:\\n        \"\"\"Calculate the beam search score with length penalty.\\n\\n        Adapted from\\n\\n        https://github.com/huggingface/transformers/blob/ccb92be23def445f2afdea94c31286f84b89eb5b/src/transformers/generation/beam_search.py#L938\\n        \"\"\"\\n        if seq_len is None:\\n            seq_len = self.get_len()\\n            # Note: HF implementation does not count the EOS token\\n            # towards the length, we align with that here for testing.\\n            if (eos_token_id is not None\\n                    and self.get_last_token_id() == eos_token_id):\\n                seq_len -= 1\\n        return self.get_cumulative_logprob() / (seq_len**length_penalty)\\n\\n    def is_finished(self) -> bool:\\n        return SequenceStatus.is_finished(self.status)\\n\\n    def fork(self, new_seq_id: int) -> \"Sequence\":\\n        new_seq = copy.deepcopy(self)\\n        new_seq.seq_id = new_seq_id\\n        return new_seq\\n\\n    def __repr__(self) -> str:\\n        return (f\"Sequence(seq_id={self.seq_id}, \"\\n                f\"status={self.status.name}, \"\\n                f\"num_blocks={len(self.logical_token_blocks)})\")\\n\\n\\nclass SequenceGroup:\\n    \"\"\"A group of sequences that are generated from the same prompt.\\n\\n    Args:\\n        request_id: The ID of the request.\\n        seqs: The list of sequences.\\n        sampling_params: The sampling parameters used to generate the outputs.\\n        arrival_time: The arrival time of the request.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        request_id: str,\\n        seqs: List[Sequence],\\n        sampling_params: SamplingParams,\\n        arrival_time: float,\\n    ) -> None:\\n        self.request_id = request_id\\n        self.seqs_dict = {seq.seq_id: seq for seq in seqs}\\n        self.sampling_params = sampling_params\\n        self.arrival_time = arrival_time\\n\\n    def get_max_num_running_seqs(self) -> int:\\n        \"\"\"The maximum number of sequences running in parallel in the remaining\\n        lifetime of the request.\"\"\"\\n        if self.sampling_params.use_beam_search:\\n            # For beam search, maximally there will always be `best_of` beam\\n            # candidates running in the future.\\n            return self.sampling_params.best_of\\n        else:\\n            if self.sampling_params.best_of > self.num_seqs():\\n                # At prompt stage, the sequence group is not yet filled up\\n                # and only have one sequence running. However, in the\\n                # generation stage, we will have `best_of` sequences running.\\n                return self.sampling_params.best_of\\n            # At sampling stages, return the number of actual sequences\\n            # running.\\n            return self.num_seqs(status=SequenceStatus.RUNNING)\\n\\n    def get_seqs(\\n        self,\\n        status: Optional[SequenceStatus] = None,\\n    ) -> List[Sequence]:\\n        if status is None:\\n            return list(self.seqs_dict.values())\\n        else:\\n            return [\\n                seq for seq in self.seqs_dict.values() if seq.status == status\\n            ]\\n\\n    def get_finished_seqs(self) -> List[Sequence]:\\n        return [seq for seq in self.seqs_dict.values() if seq.is_finished()]\\n\\n    def num_seqs(self, status: Optional[SequenceStatus] = None) -> int:\\n        return len(self.get_seqs(status))\\n\\n    def find(self, seq_id: int) -> Sequence:\\n        if seq_id not in self.seqs_dict:\\n            raise ValueError(f\"Sequence {seq_id} not found.\")\\n        return self.seqs_dict[seq_id]\\n\\n    def add(self, seq: Sequence) -> None:\\n        if seq.seq_id in self.seqs_dict:\\n            raise ValueError(f\"Sequence {seq.seq_id} already exists.\")\\n        self.seqs_dict[seq.seq_id] = seq\\n\\n    def remove(self, seq_id: int) -> None:\\n        if seq_id not in self.seqs_dict:\\n            raise ValueError(f\"Sequence {seq_id} not found.\")\\n        del self.seqs_dict[seq_id]\\n\\n    def is_finished(self) -> bool:\\n        return all(seq.is_finished() for seq in self.get_seqs())\\n\\n    def __repr__(self) -> str:\\n        return (f\"SequenceGroup(request_id={self.request_id}, \"\\n                f\"sampling_params={self.sampling_params}, \"\\n                f\"num_seqs={len(self.seqs_dict)})\")\\n\\n\\nclass SequenceGroupMetadata:\\n    \"\"\"Metadata for a sequence group. Used to create `InputMetadata`.\\n\\n\\n    Args:\\n        request_id: The ID of the request.\\n        is_prompt: Whether the request is at prompt stage.\\n        seq_data: The sequence data. (Seq id -> sequence data)\\n        sampling_params: The sampling parameters used to generate the outputs.\\n        block_tables: The block tables. (Seq id -> list of physical block\\n            numbers)\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        request_id: str,\\n        is_prompt: bool,\\n        seq_data: Dict[int, SequenceData],\\n        sampling_params: SamplingParams,\\n        block_tables: Dict[int, List[int]],\\n    ) -> None:\\n        self.request_id = request_id\\n        self.is_prompt = is_prompt\\n        self.seq_data = seq_data\\n        self.sampling_params = sampling_params\\n        self.block_tables = block_tables\\n\\n\\nclass SequenceOutputs:\\n    \"\"\"The model output associated with a sequence.\\n\\n    Args:\\n        parent_seq_id: The ID of the parent sequence (for forking in beam\\n            search).\\n        output_token: The output token ID.\\n        logprobs: The logprobs of the output token.\\n            (Token id -> logP(x_i+1 | x_0, ..., x_i))\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        parent_seq_id: int,\\n        output_token: int,\\n        logprobs: Dict[int, float],\\n    ) -> None:\\n        self.parent_seq_id = parent_seq_id\\n        self.output_token = output_token\\n        self.logprobs = logprobs\\n\\n    def __repr__(self) -> str:\\n        return (f\"SequenceOutputs(parent_seq_id={self.parent_seq_id}, \"\\n                f\"output_token={self.output_token}), \"\\n                f\"logprobs={self.logprobs}\")\\n\\n    def __eq__(self, other: object) -> bool:\\n        if not isinstance(other, SequenceOutputs):\\n            raise NotImplementedError()\\n        return (self.parent_seq_id == other.parent_seq_id\\n                and self.output_token == other.output_token\\n                and self.logprobs == other.logprobs)\\n\\n\\n# For each sequence group, we generate a list of SequenceOutputs object,\\n# each of which contains one possible candidate for the next token.\\nSamplerOutput = List[List[SequenceOutputs]]\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/__init__.py\\n---------\\nContent:\\n\"\"\"vLLM: a high-throughput and memory-efficient inference engine for LLMs\"\"\"\\n\\nfrom vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine\\nfrom vllm.engine.llm_engine import LLMEngine\\nfrom vllm.engine.ray_utils import initialize_cluster\\nfrom vllm.entrypoints.llm import LLM\\nfrom vllm.outputs import CompletionOutput, RequestOutput\\nfrom vllm.sampling_params import SamplingParams\\n\\n__version__ = \"0.1.7\"\\n\\n__all__ = [\\n    \"LLM\",\\n    \"SamplingParams\",\\n    \"RequestOutput\",\\n    \"CompletionOutput\",\\n    \"LLMEngine\",\\n    \"EngineArgs\",\\n    \"AsyncLLMEngine\",\\n    \"AsyncEngineArgs\",\\n    \"initialize_cluster\",\\n]\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/logger.py\\n---------\\nContent:\\n# Adapted from\\n# https://github.com/skypilot-org/skypilot/blob/86dc0f6283a335e4aa37b3c10716f90999f48ab6/sky/sky_logging.py\\n\"\"\"Logging configuration for vLLM.\"\"\"\\nimport logging\\nimport sys\\n\\n_FORMAT = \"%(levelname)s %(asctime)s %(filename)s:%(lineno)d] %(message)s\"\\n_DATE_FORMAT = \"%m-%d %H:%M:%S\"\\n\\n\\nclass NewLineFormatter(logging.Formatter):\\n    \"\"\"Adds logging prefix to newlines to align multi-line messages.\"\"\"\\n\\n    def __init__(self, fmt, datefmt=None):\\n        logging.Formatter.__init__(self, fmt, datefmt)\\n\\n    def format(self, record):\\n        msg = logging.Formatter.format(self, record)\\n        if record.message != \"\":\\n            parts = msg.split(record.message)\\n            msg = msg.replace(\"\\\\n\", \"\\\\r\\\\n\" + parts[0])\\n        return msg\\n\\n\\n_root_logger = logging.getLogger(\"vllm\")\\n_default_handler = None\\n\\n\\ndef _setup_logger():\\n    _root_logger.setLevel(logging.DEBUG)\\n    global _default_handler\\n    if _default_handler is None:\\n        _default_handler = logging.StreamHandler(sys.stdout)\\n        _default_handler.flush = sys.stdout.flush  # type: ignore\\n        _default_handler.setLevel(logging.INFO)\\n        _root_logger.addHandler(_default_handler)\\n    fmt = NewLineFormatter(_FORMAT, datefmt=_DATE_FORMAT)\\n    _default_handler.setFormatter(fmt)\\n    # Setting this will avoid the message\\n    # being propagated to the parent logger.\\n    _root_logger.propagate = False\\n\\n\\n# The logger is initialized when the module is imported.\\n# This is thread-safe as the module is only imported once,\\n# guaranteed by the Python GIL.\\n_setup_logger()\\n\\n\\ndef init_logger(name: str):\\n    return logging.getLogger(name)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/utils.py\\n---------\\nContent:\\nimport enum\\nfrom platform import uname\\nimport uuid\\n\\nimport psutil\\nimport torch\\n\\n\\nclass Device(enum.Enum):\\n    GPU = enum.auto()\\n    CPU = enum.auto()\\n\\n\\nclass Counter:\\n\\n    def __init__(self, start: int = 0) -> None:\\n        self.counter = start\\n\\n    def __next__(self) -> int:\\n        i = self.counter\\n        self.counter += 1\\n        return i\\n\\n    def reset(self) -> None:\\n        self.counter = 0\\n\\n\\ndef get_gpu_memory(gpu: int = 0) -> int:\\n    \"\"\"Returns the total memory of the GPU in bytes.\"\"\"\\n    return torch.cuda.get_device_properties(gpu).total_memory\\n\\n\\ndef get_cpu_memory() -> int:\\n    \"\"\"Returns the total CPU memory of the node in bytes.\"\"\"\\n    return psutil.virtual_memory().total\\n\\n\\ndef random_uuid() -> str:\\n    return str(uuid.uuid4().hex)\\n\\n\\ndef in_wsl() -> bool:\\n    # Reference: https://github.com/microsoft/WSL/issues/4071\\n    return \"microsoft\" in \" \".join(uname()).lower()\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/block.py\\n---------\\nContent:\\n\"\"\"Token blocks.\"\"\"\\nfrom typing import List\\n\\nfrom vllm.utils import Device\\n\\n_BLANK_TOKEN_ID = -1\\n\\n\\nclass LogicalTokenBlock:\\n    \"\"\"A block that stores a contiguous chunk of tokens from left to right.\\n\\n    Logical blocks are used to represent the states of the corresponding\\n    physical blocks in the KV cache.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        block_number: int,\\n        block_size: int,\\n    ) -> None:\\n        self.block_number = block_number\\n        self.block_size = block_size\\n\\n        self.token_ids = [_BLANK_TOKEN_ID] * block_size\\n        self.num_tokens = 0\\n\\n    def is_empty(self) -> bool:\\n        return self.num_tokens == 0\\n\\n    def get_num_empty_slots(self) -> int:\\n        return self.block_size - self.num_tokens\\n\\n    def is_full(self) -> bool:\\n        return self.num_tokens == self.block_size\\n\\n    def append_tokens(self, token_ids: List[int]) -> None:\\n        assert len(token_ids) <= self.get_num_empty_slots()\\n        curr_idx = self.num_tokens\\n        self.token_ids[curr_idx:curr_idx + len(token_ids)] = token_ids\\n        self.num_tokens += len(token_ids)\\n\\n    def get_token_ids(self) -> List[int]:\\n        return self.token_ids[:self.num_tokens]\\n\\n    def get_last_token_id(self) -> int:\\n        assert self.num_tokens > 0\\n        return self.token_ids[self.num_tokens - 1]\\n\\n\\nclass PhysicalTokenBlock:\\n    \"\"\"Represents the state of a block in the KV cache.\"\"\"\\n\\n    def __init__(\\n        self,\\n        device: Device,\\n        block_number: int,\\n        block_size: int,\\n    ) -> None:\\n        self.device = device\\n        self.block_number = block_number\\n        self.block_size = block_size\\n\\n        self.ref_count = 0\\n\\n    def __repr__(self) -> str:\\n        return (f\\'PhysicalTokenBlock(device={self.device}, \\'\\n                f\\'block_number={self.block_number}, \\'\\n                f\\'ref_count={self.ref_count})\\')\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/outputs.py\\n---------\\nContent:\\nfrom typing import Dict, List, Optional\\n\\nfrom vllm.sequence import SequenceGroup, SequenceStatus\\n\\n\\nclass CompletionOutput:\\n    \"\"\"The output data of one completion output of a request.\\n\\n    Args:\\n        index: The index of the output in the request.\\n        text: The generated output text.\\n        token_ids: The token IDs of the generated output text.\\n        cumulative_logprob: The cumulative log probability of the generated\\n            output text.\\n        logprobs: The log probabilities of the top probability words at each\\n            position if the logprobs are requested.\\n        finish_reason: The reason why the sequence is finished.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        index: int,\\n        text: str,\\n        token_ids: List[int],\\n        cumulative_logprob: float,\\n        logprobs: Optional[List[Dict[int, float]]],\\n        finish_reason: Optional[str] = None,\\n    ) -> None:\\n        self.index = index\\n        self.text = text\\n        self.token_ids = token_ids\\n        self.cumulative_logprob = cumulative_logprob\\n        self.logprobs = logprobs\\n        self.finish_reason = finish_reason\\n\\n    def finished(self) -> bool:\\n        return self.finish_reason is not None\\n\\n    def __repr__(self) -> str:\\n        return (f\"CompletionOutput(index={self.index}, \"\\n                f\"text={self.text!r}, \"\\n                f\"token_ids={self.token_ids}, \"\\n                f\"cumulative_logprob={self.cumulative_logprob}, \"\\n                f\"logprobs={self.logprobs}, \"\\n                f\"finish_reason={self.finish_reason})\")\\n\\n\\nclass RequestOutput:\\n    \"\"\"The output data of a request to the LLM.\\n\\n    Args:\\n        request_id: The unique ID of the request.\\n        prompt: The prompt string of the request.\\n        prompt_token_ids: The token IDs of the prompt.\\n        outputs: The output sequences of the request.\\n        finished: Whether the whole request is finished.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        request_id: str,\\n        prompt: str,\\n        prompt_token_ids: List[int],\\n        outputs: List[CompletionOutput],\\n        finished: bool,\\n    ) -> None:\\n        self.request_id = request_id\\n        self.prompt = prompt\\n        self.prompt_token_ids = prompt_token_ids\\n        self.outputs = outputs\\n        self.finished = finished\\n\\n    @classmethod\\n    def from_seq_group(cls, seq_group: SequenceGroup) -> \"RequestOutput\":\\n        # Get the top-n sequences.\\n        n = seq_group.sampling_params.n\\n        seqs = seq_group.get_seqs()\\n        if seq_group.sampling_params.use_beam_search:\\n            sorting_key = lambda seq: seq.get_beam_search_score(\\n                seq_group.sampling_params.length_penalty)\\n        else:\\n            sorting_key = lambda seq: seq.get_cumulative_logprob()\\n        sorted_seqs = sorted(seqs, key=sorting_key, reverse=True)\\n        top_n_seqs = sorted_seqs[:n]\\n\\n        # Create the outputs.\\n        outputs: List[CompletionOutput] = []\\n        for seq in top_n_seqs:\\n            logprobs = seq.output_logprobs\\n            if seq_group.sampling_params.logprobs is None:\\n                # NOTE: We need to take care of this case because the sequence\\n                # always has the logprobs of the sampled tokens even if the\\n                # logprobs are not requested.\\n                logprobs = {}\\n            finshed_reason = SequenceStatus.get_finished_reason(seq.status)\\n            output = CompletionOutput(seqs.index(seq), seq.output_text,\\n                                      seq.get_output_token_ids(),\\n                                      seq.get_cumulative_logprob(), logprobs,\\n                                      finshed_reason)\\n            outputs.append(output)\\n\\n        # Every sequence in the sequence group should have the same prompt.\\n        prompt = top_n_seqs[0].prompt\\n        prompt_token_ids = top_n_seqs[0].data.prompt_token_ids\\n        finished = seq_group.is_finished()\\n        return cls(seq_group.request_id, prompt, prompt_token_ids, outputs,\\n                   finished)\\n\\n    def __repr__(self) -> str:\\n        return (f\"RequestOutput(request_id={self.request_id}, \"\\n                f\"prompt={self.prompt!r}, \"\\n                f\"prompt_token_ids={self.prompt_token_ids}, \"\\n                f\"outputs={self.outputs}, \"\\n                f\"finished={self.finished})\")\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/core/block_manager.py\\n---------\\nContent:\\n\"\"\"A block manager that manages token blocks.\"\"\"\\nfrom typing import Dict, List, Optional, Set, Tuple\\n\\nfrom vllm.block import PhysicalTokenBlock\\nfrom vllm.sequence import Sequence, SequenceGroup, SequenceStatus\\nfrom vllm.utils import Device\\n\\n\\nclass BlockAllocator:\\n    \"\"\"Manages free physical token blocks for a device.\\n\\n    The allocator maintains a list of free blocks and allocates a block when\\n    requested. When a block is freed, its reference count is decremented. If\\n    the reference count becomes zero, the block is added back to the free list.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        device: Device,\\n        block_size: int,\\n        num_blocks: int,\\n    ) -> None:\\n        self.device = device\\n        self.block_size = block_size\\n        self.num_blocks = num_blocks\\n\\n        # Initialize the free blocks.\\n        self.free_blocks: List[PhysicalTokenBlock] = []\\n        for i in range(num_blocks):\\n            block = PhysicalTokenBlock(device=device,\\n                                       block_number=i,\\n                                       block_size=block_size)\\n            self.free_blocks.append(block)\\n\\n    def allocate(self) -> PhysicalTokenBlock:\\n        if not self.free_blocks:\\n            raise ValueError(\"Out of memory! No free blocks are available.\")\\n        block = self.free_blocks.pop()\\n        block.ref_count = 1\\n        return block\\n\\n    def free(self, block: PhysicalTokenBlock) -> None:\\n        if block.ref_count == 0:\\n            raise ValueError(f\"Double free! {block} is already freed.\")\\n        block.ref_count -= 1\\n        if block.ref_count == 0:\\n            self.free_blocks.append(block)\\n\\n    def get_num_free_blocks(self) -> int:\\n        return len(self.free_blocks)\\n\\n\\n# Mapping: logical block number -> physical block.\\nBlockTable = List[PhysicalTokenBlock]\\n\\n\\nclass BlockSpaceManager:\\n    \"\"\"Manages the mapping between logical and physical token blocks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        block_size: int,\\n        num_gpu_blocks: int,\\n        num_cpu_blocks: int,\\n        watermark: float = 0.01,\\n    ) -> None:\\n        self.block_size = block_size\\n        self.num_total_gpu_blocks = num_gpu_blocks\\n        self.num_total_cpu_blocks = num_cpu_blocks\\n        self.watermark = watermark\\n        assert watermark >= 0.0\\n\\n        self.watermark_blocks = int(watermark * num_gpu_blocks)\\n        self.gpu_allocator = BlockAllocator(Device.GPU, block_size,\\n                                            num_gpu_blocks)\\n        self.cpu_allocator = BlockAllocator(Device.CPU, block_size,\\n                                            num_cpu_blocks)\\n        # Mapping: seq_id -> BlockTable.\\n        self.block_tables: Dict[int, BlockTable] = {}\\n\\n    def can_allocate(self, seq_group: SequenceGroup) -> bool:\\n        # FIXME(woosuk): Here we assume that all sequences in the group share\\n        # the same prompt. This may not be true for preempted sequences.\\n        seq = seq_group.get_seqs()[0]\\n        num_required_blocks = len(seq.logical_token_blocks)\\n        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()\\n        # Use watermark to avoid frequent cache eviction.\\n        return (num_free_gpu_blocks - num_required_blocks >=\\n                self.watermark_blocks)\\n\\n    def allocate(self, seq_group: SequenceGroup) -> None:\\n        # NOTE: Here we assume that all sequences in the group have the same\\n        # prompt.\\n        seq = seq_group.get_seqs()[0]\\n\\n        # Allocate new physical token blocks that will store the prompt tokens.\\n        block_table: BlockTable = []\\n        for _ in range(len(seq.logical_token_blocks)):\\n            block = self.gpu_allocator.allocate()\\n            # Set the reference counts of the token blocks.\\n            block.ref_count = seq_group.num_seqs()\\n            block_table.append(block)\\n\\n        # Assign the block table for each sequence.\\n        for seq in seq_group.get_seqs():\\n            self.block_tables[seq.seq_id] = block_table.copy()\\n\\n    def can_append_slot(self, seq_group: SequenceGroup) -> bool:\\n        # Simple heuristic: If there is at least one free block\\n        # for each sequence, we can append.\\n        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()\\n        num_seqs = seq_group.num_seqs(status=SequenceStatus.RUNNING)\\n        return num_seqs <= num_free_gpu_blocks\\n\\n    def append_slot(self, seq: Sequence) -> Optional[Tuple[int, int]]:\\n        \"\"\"Allocate a physical slot for a new token.\"\"\"\\n        logical_blocks = seq.logical_token_blocks\\n        block_table = self.block_tables[seq.seq_id]\\n\\n        if len(block_table) < len(logical_blocks):\\n            # The sequence has a new logical block.\\n            # Allocate a new physical block.\\n            block = self.gpu_allocator.allocate()\\n            block_table.append(block)\\n            return None\\n\\n        # We want to append the token to the last physical block.\\n        last_block = block_table[-1]\\n        assert last_block.device == Device.GPU\\n        if last_block.ref_count == 1:\\n            # Not shared with other sequences. Appendable.\\n            return None\\n        else:\\n            # The last block is shared with other sequences.\\n            # Copy on Write: Allocate a new block and copy the tokens.\\n            new_block = self.gpu_allocator.allocate()\\n            block_table[-1] = new_block\\n            self.gpu_allocator.free(last_block)\\n            return last_block.block_number, new_block.block_number\\n\\n    def fork(self, parent_seq: Sequence, child_seq: Sequence) -> None:\\n        # NOTE: fork does not allocate a new physical block.\\n        # Thus, it is always safe from OOM.\\n        src_block_table = self.block_tables[parent_seq.seq_id]\\n        self.block_tables[child_seq.seq_id] = src_block_table.copy()\\n        for block in src_block_table:\\n            block.ref_count += 1\\n\\n    def _get_physical_blocks(\\n            self, seq_group: SequenceGroup) -> List[PhysicalTokenBlock]:\\n        # NOTE: Here, we assume that the physical blocks are only shared by\\n        # the sequences in the same group.\\n        blocks: Set[PhysicalTokenBlock] = set()\\n        for seq in seq_group.get_seqs():\\n            if seq.is_finished():\\n                continue\\n            block_table = self.block_tables[seq.seq_id]\\n            for block in block_table:\\n                blocks.add(block)\\n        return list(blocks)\\n\\n    def can_swap_in(self, seq_group: SequenceGroup) -> bool:\\n        blocks = self._get_physical_blocks(seq_group)\\n        num_swapped_seqs = seq_group.num_seqs(status=SequenceStatus.SWAPPED)\\n        num_free_blocks = self.gpu_allocator.get_num_free_blocks()\\n        # NOTE: Conservatively, we assume that every sequence will allocate\\n        # at least one free block right after the swap-in.\\n        # NOTE: This should match the logic in can_append_slot().\\n        num_required_blocks = len(blocks) + num_swapped_seqs\\n        return num_free_blocks - num_required_blocks >= self.watermark_blocks\\n\\n    def swap_in(self, seq_group: SequenceGroup) -> Dict[int, int]:\\n        # CPU block -> GPU block.\\n        mapping: Dict[PhysicalTokenBlock, PhysicalTokenBlock] = {}\\n        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):\\n            new_block_table: BlockTable = []\\n            block_table = self.block_tables[seq.seq_id]\\n\\n            for cpu_block in block_table:\\n                if cpu_block in mapping:\\n                    gpu_block = mapping[cpu_block]\\n                    gpu_block.ref_count += 1\\n                else:\\n                    gpu_block = self.gpu_allocator.allocate()\\n                    mapping[cpu_block] = gpu_block\\n                new_block_table.append(gpu_block)\\n                # Free the CPU block swapped in to GPU.\\n                self.cpu_allocator.free(cpu_block)\\n            self.block_tables[seq.seq_id] = new_block_table\\n\\n        block_number_mapping = {\\n            cpu_block.block_number: gpu_block.block_number\\n            for cpu_block, gpu_block in mapping.items()\\n        }\\n        return block_number_mapping\\n\\n    def can_swap_out(self, seq_group: SequenceGroup) -> bool:\\n        blocks = self._get_physical_blocks(seq_group)\\n        return len(blocks) <= self.cpu_allocator.get_num_free_blocks()\\n\\n    def swap_out(self, seq_group: SequenceGroup) -> Dict[int, int]:\\n        # GPU block -> CPU block.\\n        mapping: Dict[PhysicalTokenBlock, PhysicalTokenBlock] = {}\\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\\n            new_block_table: BlockTable = []\\n            block_table = self.block_tables[seq.seq_id]\\n\\n            for gpu_block in block_table:\\n                if gpu_block in mapping:\\n                    cpu_block = mapping[gpu_block]\\n                    cpu_block.ref_count += 1\\n                else:\\n                    cpu_block = self.cpu_allocator.allocate()\\n                    mapping[gpu_block] = cpu_block\\n                new_block_table.append(cpu_block)\\n                # Free the GPU block swapped out to CPU.\\n                self.gpu_allocator.free(gpu_block)\\n            self.block_tables[seq.seq_id] = new_block_table\\n\\n        block_number_mapping = {\\n            gpu_block.block_number: cpu_block.block_number\\n            for gpu_block, cpu_block in mapping.items()\\n        }\\n        return block_number_mapping\\n\\n    def _free_block_table(self, block_table: BlockTable) -> None:\\n        for block in block_table:\\n            if block.device == Device.GPU:\\n                self.gpu_allocator.free(block)\\n            else:\\n                self.cpu_allocator.free(block)\\n\\n    def free(self, seq: Sequence) -> None:\\n        if seq.seq_id not in self.block_tables:\\n            # Already freed or haven\\'t been scheduled yet.\\n            return\\n        block_table = self.block_tables[seq.seq_id]\\n        self._free_block_table(block_table)\\n        del self.block_tables[seq.seq_id]\\n\\n    def reset(self) -> None:\\n        for block_table in self.block_tables.values():\\n            self._free_block_table(block_table)\\n        self.block_tables.clear()\\n\\n    def get_block_table(self, seq: Sequence) -> List[int]:\\n        block_table = self.block_tables[seq.seq_id]\\n        return [block.block_number for block in block_table]\\n\\n    def get_num_free_gpu_blocks(self) -> int:\\n        return self.gpu_allocator.get_num_free_blocks()\\n\\n    def get_num_free_cpu_blocks(self) -> int:\\n        return self.cpu_allocator.get_num_free_blocks()\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/core/policy.py\\n---------\\nContent:\\nfrom typing import List\\n\\nfrom vllm.sequence import SequenceGroup\\n\\n\\nclass Policy:\\n\\n    def get_priority(\\n        self,\\n        now: float,\\n        seq_group: SequenceGroup,\\n    ) -> float:\\n        raise NotImplementedError\\n\\n    def sort_by_priority(\\n        self,\\n        now: float,\\n        seq_groups: List[SequenceGroup],\\n    ) -> List[SequenceGroup]:\\n        return sorted(\\n            seq_groups,\\n            key=lambda seq_group: self.get_priority(now, seq_group),\\n            reverse=True,\\n        )\\n\\n\\nclass FCFS(Policy):\\n\\n    def get_priority(\\n        self,\\n        now: float,\\n        seq_group: SequenceGroup,\\n    ) -> float:\\n        return now - seq_group.arrival_time\\n\\n\\nclass PolicyFactory:\\n\\n    _POLICY_REGISTRY = {\\n        \\'fcfs\\': FCFS,\\n    }\\n\\n    @classmethod\\n    def get_policy(cls, policy_name: str, **kwargs) -> Policy:\\n        return cls._POLICY_REGISTRY[policy_name](**kwargs)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/core/__init__.py\\n---------\\nContent:\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/core/scheduler.py\\n---------\\nContent:\\nimport enum\\nimport time\\nfrom typing import Dict, Iterable, List, Optional, Tuple, Union\\n\\nfrom vllm.config import CacheConfig, SchedulerConfig\\nfrom vllm.core.block_manager import BlockSpaceManager\\nfrom vllm.core.policy import PolicyFactory\\nfrom vllm.logger import init_logger\\nfrom vllm.sequence import (Sequence, SequenceData, SequenceGroup,\\n                           SequenceGroupMetadata, SequenceStatus)\\n\\nlogger = init_logger(__name__)\\n\\n\\nclass PreemptionMode(enum.Enum):\\n    \"\"\"Preemption modes.\\n\\n    1. Swapping: Swap out the blocks of the preempted sequences to CPU memory\\n    and swap them back in when the sequences are resumed.\\n    2. Recomputation: Discard the blocks of the preempted sequences and\\n    recompute them when the sequences are resumed, treating the sequences as\\n    new prompts.\\n    \"\"\"\\n    SWAP = enum.auto()\\n    RECOMPUTE = enum.auto()\\n\\n\\nclass SchedulerOutputs:\\n\\n    def __init__(\\n        self,\\n        scheduled_seq_groups: List[SequenceGroup],\\n        prompt_run: bool,\\n        num_batched_tokens: int,\\n        blocks_to_swap_in: Dict[int, int],\\n        blocks_to_swap_out: Dict[int, int],\\n        blocks_to_copy: Dict[int, List[int]],\\n        ignored_seq_groups: List[SequenceGroup],\\n    ) -> None:\\n        self.scheduled_seq_groups = scheduled_seq_groups\\n        self.prompt_run = prompt_run\\n        self.num_batched_tokens = num_batched_tokens\\n        self.blocks_to_swap_in = blocks_to_swap_in\\n        self.blocks_to_swap_out = blocks_to_swap_out\\n        self.blocks_to_copy = blocks_to_copy\\n        # Swap in and swap out should never happen at the same time.\\n        assert not (blocks_to_swap_in and blocks_to_swap_out)\\n        self.ignored_seq_groups = ignored_seq_groups\\n\\n    def is_empty(self) -> bool:\\n        # NOTE: We do not consider the ignored sequence groups.\\n        return (not self.scheduled_seq_groups and not self.blocks_to_swap_in\\n                and not self.blocks_to_swap_out and not self.blocks_to_copy)\\n\\n\\nclass Scheduler:\\n\\n    def __init__(\\n        self,\\n        scheduler_config: SchedulerConfig,\\n        cache_config: CacheConfig,\\n    ) -> None:\\n        self.scheduler_config = scheduler_config\\n        self.cache_config = cache_config\\n\\n        self.prompt_limit = min(self.scheduler_config.max_model_len,\\n                                self.scheduler_config.max_num_batched_tokens)\\n\\n        # Instantiate the scheduling policy.\\n        self.policy = PolicyFactory.get_policy(policy_name=\"fcfs\")\\n        # Create the block space manager.\\n        self.block_manager = BlockSpaceManager(\\n            block_size=self.cache_config.block_size,\\n            num_gpu_blocks=self.cache_config.num_gpu_blocks,\\n            num_cpu_blocks=self.cache_config.num_cpu_blocks,\\n        )\\n\\n        # TODO(zhuohan): Use deque instead of list for better performance.\\n        # Sequence groups in the WAITING state.\\n        self.waiting: List[SequenceGroup] = []\\n        # Sequence groups in the RUNNING state.\\n        self.running: List[SequenceGroup] = []\\n        # Sequence groups in the SWAPPED state.\\n        self.swapped: List[SequenceGroup] = []\\n\\n    def add_seq_group(self, seq_group: SequenceGroup) -> None:\\n        # Add sequence groups to the waiting queue.\\n        self.waiting.append(seq_group)\\n\\n    def abort_seq_group(self, request_id: Union[str, Iterable[str]]) -> None:\\n        if isinstance(request_id, str):\\n            request_id = (request_id, )\\n        request_ids = set(request_id)\\n        for state_queue in [self.waiting, self.running, self.swapped]:\\n            # We need to reverse the list as we are removing elements\\n            # from it as we iterate over it. If we don\\'t do it,\\n            # indices will get messed up and we will skip over elements.\\n            for seq_group in reversed(state_queue):\\n                if seq_group.request_id in request_ids:\\n                    # Remove the sequence group from the state queue.\\n                    state_queue.remove(seq_group)\\n                    for seq in seq_group.get_seqs():\\n                        if seq.is_finished():\\n                            continue\\n                        seq.status = SequenceStatus.FINISHED_ABORTED\\n                        self.free_seq(seq)\\n                    request_ids.remove(seq_group.request_id)\\n                    if not request_ids:\\n                        return\\n\\n    def has_unfinished_seqs(self) -> bool:\\n        return self.waiting or self.running or self.swapped\\n\\n    def get_num_unfinished_seq_groups(self) -> int:\\n        return len(self.waiting) + len(self.running) + len(self.swapped)\\n\\n    def _schedule(self) -> SchedulerOutputs:\\n        # Blocks that need to be swaped or copied before model execution.\\n        blocks_to_swap_in: Dict[int, int] = {}\\n        blocks_to_swap_out: Dict[int, int] = {}\\n        blocks_to_copy: Dict[int, List[int]] = {}\\n\\n        # Fix the current time.\\n        now = time.time()\\n\\n        # Join waiting sequences if possible.\\n        if not self.swapped:\\n            ignored_seq_groups: List[SequenceGroup] = []\\n            scheduled: List[SequenceGroup] = []\\n            # The total number of sequences on the fly, including the\\n            # requests in the generation phase.\\n            num_curr_seqs = sum(seq_group.get_max_num_running_seqs()\\n                                for seq_group in self.running)\\n            num_batched_tokens = 0\\n            # Optimization: We do not sort the waiting queue since the preempted\\n            # sequence groups are added to the front and the new sequence groups\\n            # are added to the back.\\n            while self.waiting:\\n                seq_group = self.waiting[0]\\n\\n                assert seq_group.num_seqs() == 1, (\\n                    \"Waiting sequence group should have only one prompt \"\\n                    \"sequence.\")\\n                num_prompt_tokens = seq_group.get_seqs()[0].get_len()\\n                if num_prompt_tokens > self.prompt_limit:\\n                    logger.warning(\\n                        f\"Input prompt ({num_prompt_tokens} tokens) is too long\"\\n                        f\" and exceeds limit of {self.prompt_limit}\")\\n                    for seq in seq_group.get_seqs():\\n                        seq.status = SequenceStatus.FINISHED_IGNORED\\n                    ignored_seq_groups.append(seq_group)\\n                    self.waiting.pop(0)\\n                    continue\\n\\n                # If the sequence group cannot be allocated, stop.\\n                if not self.block_manager.can_allocate(seq_group):\\n                    break\\n\\n                # If the number of batched tokens exceeds the limit, stop.\\n                if (num_batched_tokens + num_prompt_tokens >\\n                        self.scheduler_config.max_num_batched_tokens):\\n                    break\\n\\n                # The total number of sequences in the RUNNING state should not\\n                # exceed the maximum number of sequences.\\n                num_new_seqs = seq_group.get_max_num_running_seqs()\\n                if (num_curr_seqs + num_new_seqs >\\n                        self.scheduler_config.max_num_seqs):\\n                    break\\n\\n                seq_group = self.waiting.pop(0)\\n                self._allocate(seq_group)\\n                self.running.append(seq_group)\\n                num_batched_tokens += num_prompt_tokens\\n                num_curr_seqs += num_new_seqs\\n                scheduled.append(seq_group)\\n\\n            if scheduled or ignored_seq_groups:\\n                scheduler_outputs = SchedulerOutputs(\\n                    scheduled_seq_groups=scheduled,\\n                    prompt_run=True,\\n                    num_batched_tokens=num_batched_tokens,\\n                    blocks_to_swap_in=blocks_to_swap_in,\\n                    blocks_to_swap_out=blocks_to_swap_out,\\n                    blocks_to_copy=blocks_to_copy,\\n                    ignored_seq_groups=ignored_seq_groups,\\n                )\\n                return scheduler_outputs\\n\\n        # NOTE(woosuk): Preemption happens only when there is no available slot\\n        # to keep all the sequence groups in the RUNNING state.\\n        # In this case, the policy is responsible for deciding which sequence\\n        # groups to preempt.\\n        self.running = self.policy.sort_by_priority(now, self.running)\\n\\n        # Reserve new token slots for the running sequence groups.\\n        running: List[SequenceGroup] = []\\n        preempted: List[SequenceGroup] = []\\n        while self.running:\\n            seq_group = self.running.pop(0)\\n            while not self.block_manager.can_append_slot(seq_group):\\n                if self.running:\\n                    # Preempt the lowest-priority sequence groups.\\n                    victim_seq_group = self.running.pop(-1)\\n                    self._preempt(victim_seq_group, blocks_to_swap_out)\\n                    preempted.append(victim_seq_group)\\n                else:\\n                    # No other sequence groups can be preempted.\\n                    # Preempt the current sequence group.\\n                    self._preempt(seq_group, blocks_to_swap_out)\\n                    preempted.append(seq_group)\\n                    break\\n            else:\\n                # Append new slots to the sequence group.\\n                self._append_slot(seq_group, blocks_to_copy)\\n                running.append(seq_group)\\n        self.running = running\\n\\n        # Swap in the sequence groups in the SWAPPED state if possible.\\n        self.swapped = self.policy.sort_by_priority(now, self.swapped)\\n        if not preempted:\\n            num_curr_seqs = sum(seq_group.get_max_num_running_seqs()\\n                                for seq_group in self.running)\\n\\n            while self.swapped:\\n                seq_group = self.swapped[0]\\n                # If the sequence group cannot be swapped in, stop.\\n                if not self.block_manager.can_swap_in(seq_group):\\n                    break\\n\\n                # The total number of sequences in the RUNNING state should not\\n                # exceed the maximum number of sequences.\\n                num_new_seqs = seq_group.get_max_num_running_seqs()\\n                if (num_curr_seqs + num_new_seqs >\\n                        self.scheduler_config.max_num_seqs):\\n                    break\\n\\n                seq_group = self.swapped.pop(0)\\n                self._swap_in(seq_group, blocks_to_swap_in)\\n                self._append_slot(seq_group, blocks_to_copy)\\n                num_curr_seqs += num_new_seqs\\n                self.running.append(seq_group)\\n\\n        # Each sequence in the generation phase only takes one token slot.\\n        # Therefore, the number of batched tokens is equal to the number of\\n        # sequences in the RUNNING state.\\n        num_batched_tokens = sum(\\n            seq_group.num_seqs(status=SequenceStatus.RUNNING)\\n            for seq_group in self.running)\\n\\n        scheduler_outputs = SchedulerOutputs(\\n            scheduled_seq_groups=self.running,\\n            prompt_run=False,\\n            num_batched_tokens=num_batched_tokens,\\n            blocks_to_swap_in=blocks_to_swap_in,\\n            blocks_to_swap_out=blocks_to_swap_out,\\n            blocks_to_copy=blocks_to_copy,\\n            ignored_seq_groups=[],\\n        )\\n        return scheduler_outputs\\n\\n    def schedule(self) -> Tuple[List[SequenceGroupMetadata], SchedulerOutputs]:\\n        # Schedule sequence groups.\\n        # This function call changes the internal states of the scheduler\\n        # such as self.running, self.swapped, and self.waiting.\\n        scheduler_outputs = self._schedule()\\n\\n        # Create input data structures.\\n        seq_group_metadata_list: List[SequenceGroupMetadata] = []\\n        for seq_group in scheduler_outputs.scheduled_seq_groups:\\n            seq_data: Dict[int, List[SequenceData]] = {}\\n            block_tables: Dict[int, List[int]] = {}\\n            for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\\n                seq_id = seq.seq_id\\n                seq_data[seq_id] = seq.data\\n                block_tables[seq_id] = self.block_manager.get_block_table(seq)\\n\\n            seq_group_metadata = SequenceGroupMetadata(\\n                request_id=seq_group.request_id,\\n                is_prompt=scheduler_outputs.prompt_run,\\n                seq_data=seq_data,\\n                sampling_params=seq_group.sampling_params,\\n                block_tables=block_tables,\\n            )\\n            seq_group_metadata_list.append(seq_group_metadata)\\n        return seq_group_metadata_list, scheduler_outputs\\n\\n    def fork_seq(self, parent_seq: Sequence, child_seq: Sequence) -> None:\\n        self.block_manager.fork(parent_seq, child_seq)\\n\\n    def free_seq(self, seq: Sequence) -> None:\\n        self.block_manager.free(seq)\\n\\n    def free_finished_seq_groups(self) -> None:\\n        self.running = [\\n            seq_group for seq_group in self.running\\n            if not seq_group.is_finished()\\n        ]\\n\\n    def _allocate(self, seq_group: SequenceGroup) -> None:\\n        self.block_manager.allocate(seq_group)\\n        for seq in seq_group.get_seqs():\\n            seq.status = SequenceStatus.RUNNING\\n\\n    def _append_slot(\\n        self,\\n        seq_group: SequenceGroup,\\n        blocks_to_copy: Dict[int, List[int]],\\n    ) -> None:\\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\\n            ret = self.block_manager.append_slot(seq)\\n            if ret is not None:\\n                src_block, dst_block = ret\\n                if src_block in blocks_to_copy:\\n                    blocks_to_copy[src_block].append(dst_block)\\n                else:\\n                    blocks_to_copy[src_block] = [dst_block]\\n\\n    def _preempt(\\n        self,\\n        seq_group: SequenceGroup,\\n        blocks_to_swap_out: Dict[int, int],\\n        preemption_mode: Optional[PreemptionMode] = None,\\n    ) -> None:\\n        # If preemption mode is not specified, we determine the mode as follows:\\n        # We use recomputation by default since it incurs lower overhead than\\n        # swapping. However, when the sequence group has multiple sequences\\n        # (e.g., beam search), recomputation is not currently supported. In\\n        # such a case, we use swapping instead.\\n        # FIXME(woosuk): This makes our scheduling policy a bit bizarre.\\n        # As swapped sequences are prioritized over waiting sequences,\\n        # sequence groups with multiple sequences are implicitly prioritized\\n        # over sequence groups with a single sequence.\\n        # TODO(woosuk): Support recomputation for sequence groups with multiple\\n        # sequences. This may require a more sophisticated CUDA kernel.\\n        if preemption_mode is None:\\n            if seq_group.get_max_num_running_seqs() == 1:\\n                preemption_mode = PreemptionMode.RECOMPUTE\\n            else:\\n                preemption_mode = PreemptionMode.SWAP\\n        if preemption_mode == PreemptionMode.RECOMPUTE:\\n            self._preempt_by_recompute(seq_group)\\n        elif preemption_mode == PreemptionMode.SWAP:\\n            self._preempt_by_swap(seq_group, blocks_to_swap_out)\\n        else:\\n            assert False, \"Invalid preemption mode.\"\\n\\n    def _preempt_by_recompute(\\n        self,\\n        seq_group: SequenceGroup,\\n    ) -> None:\\n        seqs = seq_group.get_seqs(status=SequenceStatus.RUNNING)\\n        assert len(seqs) == 1\\n        for seq in seqs:\\n            seq.status = SequenceStatus.WAITING\\n            self.block_manager.free(seq)\\n        # NOTE: For FCFS, we insert the preempted sequence group to the front\\n        # of the waiting queue.\\n        self.waiting.insert(0, seq_group)\\n\\n    def _preempt_by_swap(\\n        self,\\n        seq_group: SequenceGroup,\\n        blocks_to_swap_out: Dict[int, int],\\n    ) -> None:\\n        self._swap_out(seq_group, blocks_to_swap_out)\\n        self.swapped.append(seq_group)\\n\\n    def _swap_in(\\n        self,\\n        seq_group: SequenceGroup,\\n        blocks_to_swap_in: Dict[int, int],\\n    ) -> None:\\n        mapping = self.block_manager.swap_in(seq_group)\\n        blocks_to_swap_in.update(mapping)\\n        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):\\n            seq.status = SequenceStatus.RUNNING\\n\\n    def _swap_out(\\n        self,\\n        seq_group: SequenceGroup,\\n        blocks_to_swap_out: Dict[int, int],\\n    ) -> None:\\n        if not self.block_manager.can_swap_out(seq_group):\\n            # FIXME(woosuk): Abort the sequence group instead of aborting the\\n            # entire engine.\\n            raise RuntimeError(\\n                \"Aborted due to the lack of CPU swap space. Please increase \"\\n                \"the swap space to avoid this error.\")\\n        mapping = self.block_manager.swap_out(seq_group)\\n        blocks_to_swap_out.update(mapping)\\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\\n            seq.status = SequenceStatus.SWAPPED\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/weight_utils.py\\n---------\\nContent:\\n\"\"\"Utilities for downloading and initializing model weights.\"\"\"\\nimport filelock\\nimport glob\\nimport json\\nimport os\\nfrom collections import defaultdict\\nfrom typing import Any, Iterator, List, Optional, Tuple\\n\\nfrom huggingface_hub import snapshot_download\\nfrom safetensors.torch import load_file, save_file, safe_open\\nimport numpy as np\\nimport torch\\nfrom tqdm.auto import tqdm\\n\\nfrom vllm.logger import init_logger\\nfrom vllm.model_executor.quantization_utils import get_quant_class\\nfrom vllm.model_executor.quantization_utils.base import QuantizationConfig\\n\\nlogger = init_logger(__name__)\\n\\n\\nclass Disabledtqdm(tqdm):\\n\\n    def __init__(self, *args, **kwargs):\\n        super().__init__(*args, **kwargs, disable=True)\\n\\n\\ndef get_lock(model_name_or_path: str, cache_dir: Optional[str] = None):\\n    lock_dir = cache_dir if cache_dir is not None else \"/tmp\"\\n    lock_file_name = model_name_or_path.replace(\"/\", \"-\") + \".lock\"\\n    lock = filelock.FileLock(os.path.join(lock_dir, lock_file_name))\\n    return lock\\n\\n\\ndef _shared_pointers(tensors):\\n    ptrs = defaultdict(list)\\n    for k, v in tensors.items():\\n        ptrs[v.data_ptr()].append(k)\\n    failing = []\\n    for _, names in ptrs.items():\\n        if len(names) > 1:\\n            failing.append(names)\\n    return failing\\n\\n\\ndef convert_bin_to_safetensor_file(\\n    pt_filename: str,\\n    sf_filename: str,\\n) -> None:\\n    loaded = torch.load(pt_filename, map_location=\"cpu\")\\n    if \"state_dict\" in loaded:\\n        loaded = loaded[\"state_dict\"]\\n    shared = _shared_pointers(loaded)\\n    for shared_weights in shared:\\n        for name in shared_weights[1:]:\\n            loaded.pop(name)\\n\\n    # For tensors to be contiguous\\n    loaded = {k: v.contiguous() for k, v in loaded.items()}\\n\\n    dirname = os.path.dirname(sf_filename)\\n    os.makedirs(dirname, exist_ok=True)\\n    save_file(loaded, sf_filename, metadata={\"format\": \"pt\"})\\n\\n    # check file size\\n    sf_size = os.stat(sf_filename).st_size\\n    pt_size = os.stat(pt_filename).st_size\\n    if (sf_size - pt_size) / pt_size > 0.01:\\n        raise RuntimeError(f\"\"\"The file size different is more than 1%:\\n         - {sf_filename}: {sf_size}\\n         - {pt_filename}: {pt_size}\\n         \"\"\")\\n\\n    # check if the tensors are the same\\n    reloaded = load_file(sf_filename)\\n    for k in loaded:\\n        pt_tensor = loaded[k]\\n        sf_tensor = reloaded[k]\\n        if not torch.equal(pt_tensor, sf_tensor):\\n            raise RuntimeError(f\"The output tensors do not match for key {k}\")\\n\\n\\n# TODO(woosuk): Move this to other place.\\ndef get_quant_config(\\n    quantization: str,\\n    model_name_or_path: str,\\n    cache_dir: Optional[str] = None,\\n) -> QuantizationConfig:\\n    is_local = os.path.isdir(model_name_or_path)\\n    if not is_local:\\n        # Download the config files.\\n        with get_lock(model_name_or_path, cache_dir):\\n            hf_folder = snapshot_download(model_name_or_path,\\n                                          allow_patterns=\"*.json\",\\n                                          cache_dir=cache_dir,\\n                                          tqdm_class=Disabledtqdm)\\n    else:\\n        hf_folder = model_name_or_path\\n    config_files = glob.glob(os.path.join(hf_folder, \"*.json\"))\\n\\n    quant_cls = get_quant_class(quantization)\\n    quant_config_files = [\\n        f for f in config_files if any(\\n            f.endswith(x) for x in quant_cls.get_config_filenames())\\n    ]\\n    if len(quant_config_files) == 0:\\n        raise ValueError(f\"Cannot find the config file for {quantization}\")\\n    if len(quant_config_files) > 1:\\n        raise ValueError(f\"Found multiple config files for {quantization}: \"\\n                         f\"{quant_config_files}\")\\n\\n    quant_config_file = quant_config_files[0]\\n    with open(quant_config_file, \"r\") as f:\\n        config = json.load(f)\\n    return quant_cls.from_config(config)\\n\\n\\ndef prepare_hf_model_weights(\\n    model_name_or_path: str,\\n    cache_dir: Optional[str] = None,\\n    use_safetensors: bool = False,\\n    fall_back_to_pt: bool = True,\\n    revision: Optional[str] = None,\\n) -> Tuple[str, List[str], bool]:\\n    # Download model weights from huggingface.\\n    is_local = os.path.isdir(model_name_or_path)\\n    if use_safetensors:\\n        allow_patterns = [\"*.safetensors\"]\\n    else:\\n        # Some quantized models use .pt files for storing the weights.\\n        allow_patterns = [\"*.bin\", \"*.pt\"]\\n    if not is_local:\\n        # Use file lock to prevent multiple processes from\\n        # downloading the same model weights at the same time.\\n        with get_lock(model_name_or_path, cache_dir):\\n            hf_folder = snapshot_download(model_name_or_path,\\n                                          allow_patterns=allow_patterns,\\n                                          cache_dir=cache_dir,\\n                                          tqdm_class=Disabledtqdm,\\n                                          revision=revision)\\n    else:\\n        hf_folder = model_name_or_path\\n    hf_weights_files: List[str] = []\\n    for pattern in allow_patterns:\\n        hf_weights_files += glob.glob(os.path.join(hf_folder, pattern))\\n    if not use_safetensors:\\n        hf_weights_files = [\\n            x for x in hf_weights_files if not x.endswith(\"training_args.bin\")\\n        ]\\n\\n    if len(hf_weights_files) == 0 and use_safetensors and fall_back_to_pt:\\n        return prepare_hf_model_weights(model_name_or_path,\\n                                        cache_dir=cache_dir,\\n                                        use_safetensors=False,\\n                                        fall_back_to_pt=False,\\n                                        revision=revision)\\n\\n    if len(hf_weights_files) == 0:\\n        raise RuntimeError(\\n            f\"Cannot find any model weights with `{model_name_or_path}`\")\\n\\n    return hf_folder, hf_weights_files, use_safetensors\\n\\n\\ndef hf_model_weights_iterator(\\n    model_name_or_path: str,\\n    cache_dir: Optional[str] = None,\\n    load_format: str = \"auto\",\\n    revision: Optional[str] = None,\\n) -> Iterator[Tuple[str, torch.Tensor]]:\\n    use_safetensors = False\\n    use_np_cache = False\\n    fall_back_to_pt = False\\n    if load_format == \"auto\":\\n        use_safetensors = True\\n        fall_back_to_pt = True\\n    elif load_format == \"safetensors\":\\n        use_safetensors = True\\n    elif load_format == \"pt\":\\n        pass\\n    elif load_format == \"npcache\":\\n        use_np_cache = True\\n    else:\\n        raise ValueError(f\"Unknown load_format: {load_format}\")\\n\\n    hf_folder, hf_weights_files, use_safetensors = prepare_hf_model_weights(\\n        model_name_or_path,\\n        cache_dir=cache_dir,\\n        use_safetensors=use_safetensors,\\n        fall_back_to_pt=fall_back_to_pt,\\n        revision=revision)\\n\\n    if use_np_cache:\\n        # Currently np_cache only support *.bin checkpoints\\n        assert use_safetensors is False\\n\\n        # Convert the model weights from torch tensors to numpy arrays for\\n        # faster loading.\\n        np_folder = os.path.join(hf_folder, \"np\")\\n        os.makedirs(np_folder, exist_ok=True)\\n        weight_names_file = os.path.join(np_folder, \"weight_names.json\")\\n        # Use file lock to prevent multiple processes from\\n        # dumping the same model weights to numpy at the same time.\\n        with get_lock(model_name_or_path, cache_dir):\\n            if not os.path.exists(weight_names_file):\\n                weight_names = []\\n                for bin_file in hf_weights_files:\\n                    state = torch.load(bin_file, map_location=\"cpu\")\\n                    for name, param in state.items():\\n                        param_path = os.path.join(np_folder, name)\\n                        with open(param_path, \"wb\") as f:\\n                            np.save(f, param.cpu().detach().numpy())\\n                        weight_names.append(name)\\n                with open(weight_names_file, \"w\") as f:\\n                    json.dump(weight_names, f)\\n\\n        with open(weight_names_file, \"r\") as f:\\n            weight_names = json.load(f)\\n\\n        for name in weight_names:\\n            param_path = os.path.join(np_folder, name)\\n            with open(param_path, \"rb\") as f:\\n                param = np.load(f)\\n            yield name, torch.from_numpy(param)\\n    elif use_safetensors:\\n        for st_file in hf_weights_files:\\n            with safe_open(st_file, framework=\"pt\") as f:\\n                for name in f.keys():\\n                    param = f.get_slice(name)\\n                    yield name, param\\n    else:\\n        for bin_file in hf_weights_files:\\n            state = torch.load(bin_file, map_location=\"cpu\")\\n            for name, param in state.items():\\n                yield name, param\\n            del state\\n            torch.cuda.empty_cache()\\n\\n\\ndef convert_pyslice_to_tensor(x: Any) -> torch.Tensor:\\n    \"\"\"convert PySafeSlice object from safetensors to torch.Tensor\\n\\n    PySafeSlice object supports indexing, which is done before loading the\\n    actual tensor and can reduce the amount of memory being read into the\\n    memory. However, it does not support more advanced functionalities\\n    like `.view()` or `.t()`. Therefore, if we need to modify the loaded\\n    tensor with these more complicated operators, we need to convert to\\n    tensor first.\\n    \"\"\"\\n    if not isinstance(x, torch.Tensor):\\n        x = x[:]\\n    return x\\n\\n\\ndef load_padded_tensor_parallel_vocab(\\n    param: torch.Tensor,\\n    loaded_weight: Any,  # `torch.Tensor` or `PySafeSlice`\\n    tensor_model_parallel_rank: int,\\n) -> None:\\n    shard_size = param.shape[0]\\n    start_idx = tensor_model_parallel_rank * shard_size\\n    end_idx = (tensor_model_parallel_rank + 1) * shard_size\\n    loaded_weight = loaded_weight[start_idx:end_idx]\\n    loaded_weight = convert_pyslice_to_tensor(loaded_weight)\\n    param[:loaded_weight.shape[0]].copy_(loaded_weight)\\n\\n\\ndef load_tensor_parallel_weights(\\n    param: torch.Tensor,\\n    loaded_weight: Any,  # `torch.Tensor` or `PySafeSlice`\\n    param_name: str,\\n    column_parallel_weight_names: List[str],\\n    row_parallel_weight_names: List[str],\\n    tensor_model_parallel_rank: int,\\n) -> None:\\n    for p in column_parallel_weight_names:\\n        if p in param_name:\\n            shard_size = param.shape[0]\\n            start_idx = tensor_model_parallel_rank * shard_size\\n            end_idx = (tensor_model_parallel_rank + 1) * shard_size\\n            loaded_weight = loaded_weight[start_idx:end_idx]\\n            break\\n    for p in row_parallel_weight_names:\\n        if p in param_name:\\n            shard_size = param.shape[1]\\n            start_idx = tensor_model_parallel_rank * shard_size\\n            end_idx = (tensor_model_parallel_rank + 1) * shard_size\\n            loaded_weight = loaded_weight[:, start_idx:end_idx]\\n            break\\n\\n    loaded_weight = convert_pyslice_to_tensor(loaded_weight)\\n    assert param.shape == loaded_weight.shape, (\\n        f\"{param_name} shape mismatch between model and checkpoint: \"\\n        f\"{param.shape} != {loaded_weight.shape}\")\\n    param.data.copy_(loaded_weight)\\n\\n\\ndef initialize_dummy_weights(\\n    model: torch.nn.Module,\\n    low: float = -1e-3,\\n    high: float = 1e-3,\\n) -> None:\\n    \"\"\"Initialize model weights with random values.\\n\\n    The model weights must be randomly initialized for accurate performance\\n    measurements. Additionally, the model weights should not cause NaNs in the\\n    forward pass. We empirically found that initializing the weights with\\n    values between -1e-3 and 1e-3 works well for most models.\\n    \"\"\"\\n    for param in model.state_dict().values():\\n        param.data.uniform_(low, high)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/__init__.py\\n---------\\nContent:\\nfrom vllm.model_executor.input_metadata import InputMetadata\\nfrom vllm.model_executor.model_loader import get_model\\nfrom vllm.model_executor.utils import set_random_seed\\n\\n__all__ = [\\n    \"InputMetadata\",\\n    \"get_model\",\\n    \"set_random_seed\",\\n]\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/utils.py\\n---------\\nContent:\\n\"\"\"Utils for model executor.\"\"\"\\nimport random\\n\\nimport numpy as np\\nimport torch\\n\\nfrom vllm.model_executor.parallel_utils.parallel_state import model_parallel_is_initialized\\nfrom vllm.model_executor.parallel_utils.tensor_parallel import model_parallel_cuda_manual_seed\\n\\n\\ndef set_random_seed(seed: int) -> None:\\n    random.seed(seed)\\n    np.random.seed(seed)\\n    torch.manual_seed(seed)\\n    if torch.cuda.is_available():\\n        torch.cuda.manual_seed_all(seed)\\n\\n    if model_parallel_is_initialized():\\n        model_parallel_cuda_manual_seed(seed)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/input_metadata.py\\n---------\\nContent:\\nfrom typing import Dict, List, Tuple\\n\\nimport torch\\nfrom xformers.ops import AttentionBias\\n\\nfrom vllm.sampling_params import SamplingParams\\nfrom vllm.sequence import SequenceData\\n\\n\\nclass InputMetadata:\\n    \"\"\"Metadata for input sequences. Used for PagedAttention.\\n\\n    Args:\\n        seq_groups: List of (seq_ids, sampling_params).\\n        seq_data: Seq_id -> SequenceData.\\n        prompt_lens: Lengths of prompts.\\n        slot_mapping: The address to write the new KV to of each token.\\n        context_lens: the length of attention context for each generation token.\\n        max_context_len: The maximum context length.\\n        block_tables: The block tables. (Seq id -> list of physical block)\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        seq_groups: List[Tuple[List[int], SamplingParams]],\\n        seq_data: Dict[int, SequenceData],\\n        prompt_lens: List[int],\\n        slot_mapping: torch.Tensor,\\n        context_lens: torch.Tensor,\\n        max_context_len: int,\\n        block_tables: torch.Tensor,\\n    ) -> None:\\n        self.seq_groups = seq_groups\\n        self.seq_data = seq_data\\n        self.prompt_lens = prompt_lens\\n        self.slot_mapping = slot_mapping\\n        self.context_lens = context_lens\\n        self.max_context_len = max_context_len\\n        self.block_tables = block_tables\\n\\n        self.num_prompts = len(prompt_lens)\\n        self.num_prompt_tokens = sum(prompt_lens)\\n        self.num_generation_tokens = context_lens.shape[0]\\n        self.num_valid_tokens = slot_mapping.shape[0]\\n        if block_tables.numel() > 0:\\n            self.max_num_blocks_per_seq = block_tables.shape[1]\\n        else:\\n            self.max_num_blocks_per_seq = 0\\n        assert block_tables.shape[0] == self.num_generation_tokens\\n        assert context_lens.shape[0] == self.num_generation_tokens\\n\\n        # Set during the execution of the first attention op.\\n        self.attn_bias: List[AttentionBias] = []\\n\\n    def __repr__(self) -> str:\\n        # Print only useful metadata.\\n        return (f\\'InputMetadata(\\'\\n                f\\'num_valid_tokens={self.num_valid_tokens}, \\'\\n                f\\'num_prompt_tokens={self.num_prompt_tokens}, \\'\\n                f\\'num_prompts={self.num_prompts}, \\'\\n                f\\'prompt_lens={self.prompt_lens}, \\'\\n                f\\'num_generation_tokens={self.num_generation_tokens}, \\'\\n                f\\'context_lens={self.context_lens}, \\'\\n                f\\'max_context_len={self.max_context_len}), \\'\\n                f\\'max_num_blocks_per_seq={self.max_num_blocks_per_seq}, \\'\\n                f\\'block_tables={self.block_tables}), \\'\\n                f\\'slot_mapping={self.slot_mapping}\\')\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/model_loader.py\\n---------\\nContent:\\n\"\"\"Utilities for selecting and loading models.\"\"\"\\nimport contextlib\\nfrom typing import Type\\n\\nimport torch\\nimport torch.nn as nn\\nfrom transformers import PretrainedConfig\\n\\nfrom vllm.config import ModelConfig\\nfrom vllm.model_executor.models import *  # pylint: disable=wildcard-import\\nfrom vllm.model_executor.weight_utils import (get_quant_config,\\n                                              initialize_dummy_weights)\\n\\n# TODO(woosuk): Lazy-load the model classes.\\n_MODEL_REGISTRY = {\\n    \"AquilaModel\": AquilaForCausalLM,\\n    \"BaiChuanForCausalLM\": BaiChuanForCausalLM,  # baichuan-7b\\n    \"BaichuanForCausalLM\": BaichuanForCausalLM,  # baichuan-13b\\n    \"BloomForCausalLM\": BloomForCausalLM,\\n    \"FalconForCausalLM\": FalconForCausalLM,\\n    \"GPT2LMHeadModel\": GPT2LMHeadModel,\\n    \"GPTBigCodeForCausalLM\": GPTBigCodeForCausalLM,\\n    \"GPTJForCausalLM\": GPTJForCausalLM,\\n    \"GPTNeoXForCausalLM\": GPTNeoXForCausalLM,\\n    \"InternLMForCausalLM\": InternLMForCausalLM,\\n    \"LlamaForCausalLM\": LlamaForCausalLM,\\n    \"LLaMAForCausalLM\": LlamaForCausalLM,  # For decapoda-research/llama-*\\n    \"MPTForCausalLM\": MPTForCausalLM,\\n    \"OPTForCausalLM\": OPTForCausalLM,\\n    \"QWenLMHeadModel\": QWenLMHeadModel,\\n    \"RWForCausalLM\": FalconForCausalLM,\\n}\\n\\n# FIXME(woosuk): Remove this once all models support quantization.\\n_MODEL_CLASSES_SUPPORT_QUANTIZATION = [\\n    LlamaForCausalLM,\\n]\\n\\n\\n@contextlib.contextmanager\\ndef _set_default_torch_dtype(dtype: torch.dtype):\\n    \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\\n    old_dtype = torch.get_default_dtype()\\n    torch.set_default_dtype(dtype)\\n    yield\\n    torch.set_default_dtype(old_dtype)\\n\\n\\ndef _get_model_architecture(config: PretrainedConfig) -> Type[nn.Module]:\\n    architectures = getattr(config, \"architectures\", [])\\n    for arch in architectures:\\n        if arch in _MODEL_REGISTRY:\\n            return _MODEL_REGISTRY[arch]\\n    raise ValueError(\\n        f\"Model architectures {architectures} are not supported for now. \"\\n        f\"Supported architectures: {list(_MODEL_REGISTRY.keys())}\")\\n\\n\\ndef get_model(model_config: ModelConfig) -> nn.Module:\\n    model_class = _get_model_architecture(model_config.hf_config)\\n\\n    # Get the quantization config.\\n    quant_config = None\\n    if model_config.quantization is not None:\\n        if model_class not in _MODEL_CLASSES_SUPPORT_QUANTIZATION:\\n            raise ValueError(\\n                f\"Quantization is not supported for {model_class}.\")\\n        quant_config = get_quant_config(model_config.quantization,\\n                                        model_config.model,\\n                                        model_config.download_dir)\\n        supported_dtypes = quant_config.get_supported_act_dtypes()\\n        if model_config.dtype not in supported_dtypes:\\n            raise ValueError(\\n                f\"{model_config.dtype} is not supported for quantization \"\\n                f\"method {model_config.quantization}. Supported dtypes: \"\\n                f\"{supported_dtypes}\")\\n\\n    with _set_default_torch_dtype(model_config.dtype):\\n        # Create a model instance.\\n        # The weights will be initialized as empty tensors.\\n        if model_class in _MODEL_CLASSES_SUPPORT_QUANTIZATION:\\n            model = model_class(model_config.hf_config, quant_config)\\n        else:\\n            model = model_class(model_config.hf_config)\\n        if model_config.load_format == \"dummy\":\\n            model = model.cuda()\\n            # NOTE(woosuk): For accurate performance evaluation, we assign\\n            # random values to the weights.\\n            initialize_dummy_weights(model)\\n        else:\\n            # Load the weights from the cached or downloaded files.\\n            model.load_weights(model_config.model, model_config.download_dir,\\n                               model_config.load_format, model_config.revision)\\n            model = model.cuda()\\n    return model.eval()\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/quantization_utils/__init__.py\\n---------\\nContent:\\nfrom typing import Type\\n\\nfrom vllm.model_executor.quantization_utils.awq import AWQConfig\\nfrom vllm.model_executor.quantization_utils.base import QuantizationConfig\\n\\n_QUANTIZATION_REGISTRY = {\\n    \"awq\": AWQConfig,\\n}\\n\\n\\ndef get_quant_class(quantization: str) -> Type[QuantizationConfig]:\\n    if quantization not in _QUANTIZATION_REGISTRY:\\n        raise ValueError(f\"Invalid quantization method: {quantization}\")\\n    return _QUANTIZATION_REGISTRY[quantization]\\n\\n\\n__all__ = [\\n    \"QuantizationConfig\",\\n    \"get_quant_class\",\\n]\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/quantization_utils/awq.py\\n---------\\nContent:\\nfrom typing import Any, Dict, List\\n\\nimport torch\\n\\nfrom vllm.model_executor.quantization_utils.base import QuantizationConfig\\n\\n\\nclass AWQConfig(QuantizationConfig):\\n    \"\"\"Config class for AWQ.\\n\\n    Reference: https://arxiv.org/abs/2306.00978\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        weight_bits: int,\\n        group_size: int,\\n        zero_point: bool,\\n    ) -> None:\\n        self.weight_bits = weight_bits\\n        self.group_size = group_size\\n        self.zero_point = zero_point\\n\\n        if self.weight_bits != 4:\\n            raise ValueError(\\n                \"Currently, only 4-bit weight quantization is supported for \"\\n                f\"AWQ, but got {self.weight_bits} bits.\")\\n        self.pack_factor = 32 // self.weight_bits\\n\\n    def __repr__(self) -> str:\\n        return (f\"AWQConfig(weight_bits={self.weight_bits}, \"\\n                f\"group_size={self.group_size}, \"\\n                f\"zero_point={self.zero_point})\")\\n\\n    @classmethod\\n    def get_name(cls) -> str:\\n        return \"awq\"\\n\\n    @classmethod\\n    def get_supported_act_dtypes(cls) -> List[torch.dtype]:\\n        return [torch.half]\\n\\n    @classmethod\\n    def get_config_filenames(cls) -> List[str]:\\n        return [\\n            \"quant_config.json\",  # E.g., casperhansen/vicuna-7b-v1.5-awq\\n            \"quantize_config.json\",  # E.g., abhinavkulkarni/mosaicml-mpt-7b-instruct-w4-g128-awq  # pylint: disable=line-too-long\\n        ]\\n\\n    @classmethod\\n    def from_config(cls, config: Dict[str, Any]) -> \"AWQConfig\":\\n        weight_bits = cls.get_from_keys(config, [\"w_bit\", \"bits\"])\\n        group_size = cls.get_from_keys(config, [\"q_group_size\", \"group_size\"])\\n        zero_point = cls.get_from_keys(config, [\"zero_point\"])\\n        return cls(weight_bits, group_size, zero_point)\\n\\n    @classmethod\\n    def get_packed_tensor_names(cls) -> List[str]:\\n        return [\"qweight\", \"qzeros\"]\\n\\n    @classmethod\\n    def get_transposed_tensor_names(cls) -> List[str]:\\n        return [\"qweight\", \"qzeros\", \"scales\"]\\n\\n    @classmethod\\n    def get_tp_tensor_names(cls) -> List[str]:\\n        return [\"qweight\", \"qzeros\", \"scales\"]\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/quantization_utils/base.py\\n---------\\nContent:\\nfrom typing import Any, Dict, List\\n\\nimport torch\\n\\n\\nclass QuantizationConfig:\\n\\n    @classmethod\\n    def get_name(cls) -> str:\\n        \"\"\"Name of the quantization method.\"\"\"\\n        raise NotImplementedError\\n\\n    @classmethod\\n    def get_supported_act_dtypes(cls) -> List[torch.dtype]:\\n        \"\"\"List of supported activation dtypes.\"\"\"\\n        raise NotImplementedError\\n\\n    @classmethod\\n    def get_config_filenames(cls) -> List[str]:\\n        \"\"\"List of filenames to search for in the model directory.\"\"\"\\n        raise NotImplementedError\\n\\n    @classmethod\\n    def from_config(cls, config: Dict[str, Any]) -> \"QuantizationConfig\":\\n        \"\"\"Create a config class from the model\\'s quantization config.\"\"\"\\n        raise NotImplementedError\\n\\n    @staticmethod\\n    def get_from_keys(config: Dict[str, Any], keys: List[str]) -> Any:\\n        \"\"\"Get a value from the model\\'s quantization config.\"\"\"\\n        for key in keys:\\n            if key in config:\\n                return config[key]\\n        raise ValueError(f\"Cannot find any of {keys} in the model\\'s \"\\n                         \"quantization config.\")\\n\\n    @classmethod\\n    def get_packed_tensor_names(cls) -> List[str]:\\n        raise NotImplementedError\\n\\n    @classmethod\\n    def is_packed(cls, tensor_name: str) -> bool:\\n        \"\"\"Returns True if a tensor is packed.\\n\\n        A tensor is considered packed if each element in the tensor is a\\n        packed representation of multiple elements in the original tensor.\\n        For example, an INT32 element in the tensor may represent 8 INT4\\n        elements in the original tensor.\\n        \"\"\"\\n        return any(tag in tensor_name for tag in cls.get_packed_tensor_names())\\n\\n    @classmethod\\n    def get_transposed_tensor_names(cls) -> List[str]:\\n        raise NotImplementedError\\n\\n    @classmethod\\n    def is_transposed(cls, tensor_name: str) -> bool:\\n        \"\"\"Returns True if a tensor is transposed relative to nn.Linear.weight.\\n        \"\"\"\\n        return any(tag in tensor_name\\n                   for tag in cls.get_transposed_tensor_names())\\n\\n    @classmethod\\n    def get_tp_tensor_names(cls) -> List[str]:\\n        raise NotImplementedError\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/layers/attention.py\\n---------\\nContent:\\n\"\"\"Multi-head attention.\"\"\"\\nfrom typing import List, Optional\\n\\nimport torch\\nimport torch.nn as nn\\nfrom xformers import ops as xops\\nfrom xformers.ops.fmha.attn_bias import (BlockDiagonalCausalMask,\\n                                         LowerTriangularMaskWithTensorBias)\\n\\nfrom vllm import attention_ops\\nfrom vllm import cache_ops\\nfrom vllm import pos_encoding_ops\\nfrom vllm.model_executor.input_metadata import InputMetadata\\n\\n_SUPPORTED_HEAD_SIZES = [64, 80, 96, 112, 128, 256]\\n\\n\\nclass PagedAttention(nn.Module):\\n    # pylint: disable=line-too-long\\n    \"\"\"GPT-style multi-head PagedAttention.\\n\\n    This class takes flattened 1D query, key, and value tensors as input. The\\n    input 1D tensors can either contain prompt tokens or generation tokens, in\\n    addition to paddings.\\n\\n    If the input tensors contain prompt tokens, the layout is as follows:\\n\\n    |<---------------------- num_valid_tokens ---------------------->|\\n    |<--------------- num_prompt_tokens -------------->|\\n    |<--prompt_0-->|<--prompt_1-->|...|<--prompt_N-1-->|<--padding-->|\\n\\n    Otherwise, the layout is as follows:\\n\\n    |<------------------ num_valid_tokens ------------------->|\\n    |<------- num_generation_tokens (M) ------->|\\n    |<--generation_0-->|...|<--generation_M-1-->|<--padding-->|\\n\\n    The prompts might have different lengths, while the generation tokens always\\n    have length 1. The paddings are appended to make the input length a multiple\\n    of 8, which is desirable for Tensor Cores.\\n\\n    The class does the following:\\n    1. Perform multi_query_kv_attention for the prompts. This operation does\\n        not use the KV cache.\\n    2. Wait for the cache operations (e.g., swap, copy) to finish. The cache\\n        operations are issued by the cache engine before executing the forward\\n        pass of the model, and they are executed asynchronously.\\n    3. Reshape and store the input key and value tensors in the KV cache.\\n    4. Perform single_query_cached_kv_attention for the generation tokens.\\n        This operation reads the previous key and value tensors from the KV\\n        cache.\\n    5. Output a flattened 1D tensor.\\n    \"\"\"\\n\\n    def __init__(self,\\n                 num_heads: int,\\n                 head_size: int,\\n                 scale: float,\\n                 num_kv_heads: Optional[int] = None) -> None:\\n        super().__init__()\\n        self.num_heads = num_heads\\n        self.head_size = head_size\\n        self.scale = float(scale)\\n        self.num_kv_heads = num_heads if num_kv_heads is None else num_kv_heads\\n\\n        assert self.num_heads % self.num_kv_heads == 0\\n        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\\n        self.head_mapping = torch.repeat_interleave(\\n            torch.arange(self.num_kv_heads, dtype=torch.int32, device=\"cuda\"),\\n            self.num_queries_per_kv)\\n\\n        if self.head_size not in _SUPPORTED_HEAD_SIZES:\\n            raise ValueError(f\"head_size ({self.head_size}) is not supported. \"\\n                             f\"Supported head sizes: {_SUPPORTED_HEAD_SIZES}.\")\\n\\n    def set_attn_bias(\\n        self,\\n        input_metadata: InputMetadata,\\n        dtype: torch.dtype,\\n    ) -> None:\\n        del dtype  # Unused.\\n        if input_metadata.attn_bias:\\n            # Already set by a previous layer.\\n            return\\n        prompt_lens = input_metadata.prompt_lens\\n        attn_bias = BlockDiagonalCausalMask.from_seqlens(prompt_lens)\\n        input_metadata.attn_bias.append(attn_bias)\\n\\n    def multi_query_kv_attention(\\n        self,\\n        output: torch.Tensor,\\n        query: torch.Tensor,\\n        key: torch.Tensor,\\n        value: torch.Tensor,\\n        input_metadata: InputMetadata,\\n    ) -> torch.Tensor:\\n        \"\"\"Normal attention for the prompt tokens.\\n\\n        Args:\\n            output: shape = [num_prompt_tokens, num_heads, head_size]\\n            query: shape = [num_prompt_tokens, num_heads, head_size]\\n            key: shape = [num_prompt_tokens, num_kv_heads, head_size]\\n            value: shape = [num_prompt_tokens, num_kv_heads, head_size]\\n            input_metadata: metadata for paged attention.\\n        \"\"\"\\n\\n        if self.num_kv_heads != self.num_heads:\\n            # Project the key and value tensors to the desired number of heads.\\n            key = torch.repeat_interleave(key, self.num_queries_per_kv, dim=1)\\n            value = torch.repeat_interleave(value,\\n                                            self.num_queries_per_kv,\\n                                            dim=1)\\n\\n        # TODO(woosuk): The unsqueeze op may incur some CPU overhead. Optimize.\\n        out = xops.memory_efficient_attention_forward(\\n            query.unsqueeze(0),\\n            key.unsqueeze(0),\\n            value.unsqueeze(0),\\n            attn_bias=input_metadata.attn_bias[0],\\n            p=0.0,\\n            scale=self.scale,\\n        )\\n        # TODO(woosuk): Unnecessary copy. Optimize.\\n        output.copy_(out.squeeze(0))\\n        return output\\n\\n    def single_query_cached_kv_attention(\\n        self,\\n        output: torch.Tensor,\\n        query: torch.Tensor,\\n        key_cache: torch.Tensor,\\n        value_cache: torch.Tensor,\\n        input_metadata: InputMetadata,\\n    ) -> None:\\n        \"\"\"PagedAttention for the generation tokens.\\n\\n        Args:\\n            output: shape = [num_generation_tokens, num_heads, head_size]\\n            query: shape = [num_generation_tokens, num_heads, head_size]\\n            key_cache: shape = [num_blocks, num_kv_heads, head_size/x,\\n                block_size, x]\\n            value_cache: shape = [num_blocks, num_kv_heads, head_size,\\n                block_size]\\n            input_metadata: metadata for paged attention.\\n        \"\"\"\\n        block_size = value_cache.shape[3]\\n        attention_ops.single_query_cached_kv_attention(\\n            output,\\n            query,\\n            key_cache,\\n            value_cache,\\n            self.head_mapping,\\n            self.scale,\\n            input_metadata.block_tables,\\n            input_metadata.context_lens,\\n            block_size,\\n            input_metadata.max_context_len,\\n            None,  # alibi_slopes\\n        )\\n\\n    def forward(\\n        self,\\n        query: torch.Tensor,\\n        key: torch.Tensor,\\n        value: torch.Tensor,\\n        key_cache: Optional[torch.Tensor],\\n        value_cache: Optional[torch.Tensor],\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        \"\"\"PagedAttention forward pass.\\n\\n        NOTE: The query, key, and value tensors must be sliced from a qkv\\n        tensor of shape [num_tokens, 3 * num_heads * head_size].\\n\\n        Args:\\n            query: shape = [num_tokens, num_heads * head_size]\\n            key: shape = [num_tokens, num_kv_heads * head_size]\\n            value: shape = [num_tokens, num_kv_heads * head_size]\\n            key_cache: shape = [num_blocks, num_kv_heads, head_size/x,\\n                block_size, x]\\n            value_cache: shape = [num_blocks, num_kv_heads, head_size,\\n                block_size]\\n            input_metadata: metadata for paged attention.\\n            cache_event: event to wait for the cache operations to finish.\\n\\n        Returns:\\n            shape = [num_tokens, num_heads * head_size]\\n        \"\"\"\\n\\n        # Reshape the query, key, and value tensors.\\n        query = query.view(-1, self.num_heads, self.head_size)\\n        key = key.view(-1, self.num_kv_heads, self.head_size)\\n        value = value.view(-1, self.num_kv_heads, self.head_size)\\n\\n        # Pre-allocate the output tensor.\\n        output = torch.empty_like(query)\\n\\n        # Compute the attention op for prompts.\\n        num_prompt_tokens = input_metadata.num_prompt_tokens\\n        if num_prompt_tokens > 0:\\n            # Prompt run.\\n            assert input_metadata.num_generation_tokens == 0\\n            self.set_attn_bias(input_metadata, dtype=query.dtype)\\n            self.multi_query_kv_attention(\\n                output[:num_prompt_tokens],\\n                query[:num_prompt_tokens],\\n                key[:num_prompt_tokens],\\n                value[:num_prompt_tokens],\\n                input_metadata,\\n            )\\n\\n        # Wait until the cache op is done.\\n        if cache_event is not None:\\n            cache_event.wait()\\n\\n        # Reshape the keys and values and store them in the cache.\\n        # When key_cache and value_cache are not provided, the new key\\n        # and value vectors will not be cached.\\n        num_valid_tokens = input_metadata.num_valid_tokens\\n        if (num_valid_tokens > 0 and key_cache is not None\\n                and value_cache is not None):\\n            # The stride is 3 because the key and value are sliced from qkv.\\n            cache_ops.reshape_and_cache(\\n                key[:num_valid_tokens],\\n                value[:num_valid_tokens],\\n                key_cache,\\n                value_cache,\\n                input_metadata.slot_mapping,\\n            )\\n\\n        if input_metadata.num_generation_tokens > 0:\\n            # Decoding run.\\n            assert input_metadata.num_prompt_tokens == 0\\n            assert key_cache is not None and value_cache is not None, (\\n                \"key_cache and value_cache must be provided when \"\\n                \"generating tokens.\")\\n            # Compute the attention op for generation tokens.\\n            self.single_query_cached_kv_attention(\\n                output[num_prompt_tokens:num_valid_tokens],\\n                query[num_prompt_tokens:num_valid_tokens], key_cache,\\n                value_cache, input_metadata)\\n\\n        # Reshape the output tensor.\\n        # NOTE(woosuk): The output tensor may include paddings.\\n        return output.view(-1, self.num_heads * self.head_size)\\n\\n\\nclass PagedAttentionWithRoPE(PagedAttention):\\n    \"\"\"PagedAttention with rotary embedding.\"\"\"\\n\\n    def __init__(\\n        self,\\n        num_heads: int,\\n        head_size: int,\\n        scale: float,\\n        rotary_dim: int,\\n        max_position: int = 8192,\\n        base: int = 10000,\\n        num_kv_heads: Optional[int] = None,\\n        is_neox_style: bool = True,\\n    ) -> None:\\n        super().__init__(num_heads, head_size, scale, num_kv_heads)\\n        self.is_neox_style = is_neox_style\\n\\n        # Create the cos and sin cache.\\n        inv_freq = 1.0 / (base**(torch.arange(\\n            0, rotary_dim, 2, dtype=torch.float, device=\"cuda\") / rotary_dim))\\n        t = torch.arange(max_position, dtype=torch.float, device=\"cuda\")\\n        freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\\n        cos = freqs.cos()\\n        sin = freqs.sin()\\n        cache = torch.cat((cos, sin), dim=-1)\\n\\n        # FIXME(woosuk): This assumes that we configure the default dtype when\\n        # initializing the model.\\n        # TODO(woosuk): Make it more robust.\\n        torch_dtype = torch.get_default_dtype()\\n        cache = cache.to(torch_dtype)\\n        # Embedding size: [max_position, rotary_dim]\\n        self.register_buffer(\"cos_sin_cache\", cache, persistent=False)\\n\\n    def forward(\\n        self,\\n        positions: torch.Tensor,\\n        query: torch.Tensor,\\n        key: torch.Tensor,\\n        value: torch.Tensor,\\n        key_cache: torch.Tensor,\\n        value_cache: torch.Tensor,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        \"\"\" PagedAttention forward pass with rotary embedding.\\n\\n        Args:\\n            positions: shape = [num_tokens]\\n                        query: shape = [num_tokens, num_heads * head_size]\\n            key: shape = [num_tokens, num_kv_heads * head_size]\\n            value: shape = [num_tokens, num_kv_heads * head_size]\\n            key_cache: shape = [num_blocks, num_kv_heads, head_size/x,\\n                block_size, x]\\n            value_cache: shape = [num_blocks, num_kv_heads, head_size,\\n                block_size]\\n            input_metadata: metadata for paged attention.\\n            cache_event: event to wait for the cache operations to finish.\\n\\n        Returns:\\n            shape = [num_tokens, num_heads * head_size]\\n        \"\"\"\\n\\n        # Apply rotary embedding to the query and key before passing them\\n        # to the attention op.\\n        pos_encoding_ops.rotary_embedding(\\n            positions,\\n            query,\\n            key,\\n            self.head_size,\\n            self.cos_sin_cache,\\n            self.is_neox_style,\\n        )\\n        return super().forward(\\n            query,\\n            key,\\n            value,\\n            key_cache,\\n            value_cache,\\n            input_metadata,\\n            cache_event,\\n        )\\n\\n\\nclass PagedAttentionWithALiBi(PagedAttention):\\n    \"\"\"PagedAttention with ALiBi attention bias.\"\"\"\\n\\n    def __init__(self,\\n                 num_heads: int,\\n                 head_size: int,\\n                 scale: float,\\n                 slopes: List[float],\\n                 num_kv_heads: Optional[int] = None) -> None:\\n        super().__init__(num_heads, head_size, scale, num_kv_heads)\\n        assert len(slopes) == num_heads\\n\\n        slopes = torch.tensor(slopes, dtype=torch.float32)\\n        self.register_buffer(\"alibi_slopes\", slopes, persistent=False)\\n\\n    def set_attn_bias(self, input_metadata: InputMetadata,\\n                      dtype: torch.dtype) -> None:\\n        if input_metadata.attn_bias:\\n            # Already set by a previous layer.\\n            return\\n        # Generates ALiBi mask for each prompt.\\n        for prompt_len in input_metadata.prompt_lens:\\n            bias = torch.arange(prompt_len, dtype=dtype)\\n            # Note(zhuohan): HF uses\\n            #     `bias = bias[None, :].repeat(prompt_len, 1)`\\n            # here. We find that both biases give the same results, but\\n            # the bias below more accurately follows the original ALiBi\\n            # paper.\\n            bias = bias[None, :] - bias[:, None]\\n            bias = bias.to(self.alibi_slopes.device)\\n\\n            # When using custom attention bias, xformers requires the bias to\\n            # be sliced from a tensor whose length is a multiple of 8.\\n            padded_len = (prompt_len + 7) // 8 * 8\\n            bias = torch.empty(\\n                1,  # batch_size\\n                self.num_heads,\\n                prompt_len,\\n                padded_len,\\n                device=self.alibi_slopes.device,\\n                dtype=dtype,\\n            )[:, :, :, :prompt_len].copy_(bias)\\n            bias.mul_(self.alibi_slopes[:, None, None])\\n            attn_bias = LowerTriangularMaskWithTensorBias(bias)\\n            input_metadata.attn_bias.append(attn_bias)\\n\\n    def multi_query_kv_attention(\\n        self,\\n        output: torch.Tensor,\\n        query: torch.Tensor,\\n        key: torch.Tensor,\\n        value: torch.Tensor,\\n        input_metadata: InputMetadata,\\n    ) -> torch.Tensor:\\n        \"\"\"Attention with ALiBi bias for the prompt tokens.\\n\\n        Args:\\n            output: shape = [num_prompt_tokens, num_heads, head_size]\\n            query: shape = [num_prompt_tokens, num_heads, head_size]\\n            key: shape = [num_prompt_tokens, num_kv_heads, head_size]\\n            value: shape = [num_prompt_tokens, num_kv_heads, head_size]\\n            input_metadata: metadata for paged attention.\\n        \"\"\"\\n        if self.num_kv_heads != self.num_heads:\\n            # Project the key and value tensors to the desired number of heads.\\n            key = torch.repeat_interleave(key, self.num_queries_per_kv, dim=1)\\n            value = torch.repeat_interleave(value,\\n                                            self.num_queries_per_kv,\\n                                            dim=1)\\n\\n        # FIXME(woosuk): Because xformers does not support dynamic sequence\\n        # lengths with custom attention bias, we process each prompt one by\\n        # one. This is inefficient, especially when we have many short prompts.\\n        start = 0\\n        for i, prompt_len in enumerate(input_metadata.prompt_lens):\\n            end = start + prompt_len\\n            out = xops.memory_efficient_attention_forward(\\n                query[None, start:end],\\n                key[None, start:end],\\n                value[None, start:end],\\n                attn_bias=input_metadata.attn_bias[i],\\n                p=0.0,\\n                scale=self.scale,\\n            )\\n            # TODO(woosuk): Unnecessary copy. Optimize.\\n            output[start:end].copy_(out.squeeze(0))\\n            start += prompt_len\\n        return output\\n\\n    def single_query_cached_kv_attention(\\n        self,\\n        output: torch.Tensor,\\n        query: torch.Tensor,\\n        key_cache: torch.Tensor,\\n        value_cache: torch.Tensor,\\n        input_metadata: InputMetadata,\\n    ) -> None:\\n        \"\"\"PagedAttention with ALiBi bias for the generation tokens.\\n\\n        Args:\\n            output: shape = [num_generation_tokens, num_heads, head_size]\\n            query: shape = [num_generation_tokens, num_heads, head_size]\\n            key_cache: shape = [num_blocks, num_kv_heads, head_size/x,\\n                block_size, x]\\n            value_cache: shape = [num_blocks, num_kv_heads, head_size,\\n                block_size]\\n            input_metadata: metadata for paged attention.\\n        \"\"\"\\n        block_size = value_cache.shape[3]\\n        attention_ops.single_query_cached_kv_attention(\\n            output,\\n            query,\\n            key_cache,\\n            value_cache,\\n            self.head_mapping,\\n            self.scale,\\n            input_metadata.block_tables,\\n            input_metadata.context_lens,\\n            block_size,\\n            input_metadata.max_context_len,\\n            self.alibi_slopes,\\n        )\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/layers/layernorm.py\\n---------\\nContent:\\n\"\"\"Custom normalization layers.\"\"\"\\nimport torch\\nimport torch.nn as nn\\n\\nfrom vllm import layernorm_ops\\n\\n\\nclass RMSNorm(nn.Module):\\n    \"\"\"Root mean square normalization.\\n\\n    Computes x -> w * x / sqrt(E[x^2] + eps) where w is the learned weight.\\n    Refer to https://arxiv.org/abs/1910.07467\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        hidden_size: int,\\n        eps: float = 1e-6,\\n    ) -> None:\\n        super().__init__()\\n        self.weight = nn.Parameter(torch.ones(hidden_size))\\n        self.variance_epsilon = eps\\n\\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\\n        out = torch.empty_like(x)\\n        layernorm_ops.rms_norm(\\n            out,\\n            x,\\n            self.weight.data,\\n            self.variance_epsilon,\\n        )\\n        return out\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/layers/__init__.py\\n---------\\nContent:\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/layers/activation.py\\n---------\\nContent:\\n\"\"\"Custom activation functions.\"\"\"\\nimport torch\\nimport torch.nn as nn\\n\\nfrom vllm import activation_ops\\n\\n\\nclass SiluAndMul(nn.Module):\\n    \"\"\"An activation function for SwiGLU.\\n\\n    The function computes x -> silu(x[:d]) * x[d:] where d = x.shape[1] // 2.\\n\\n    Shapes:\\n        x: (num_tokens, 2 * d)\\n        return: (num_tokens, d)\\n    \"\"\"\\n\\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\\n        num_tokens = x.shape[0]\\n        d = x.shape[1] // 2\\n        out = torch.empty(num_tokens, d, dtype=x.dtype, device=x.device)\\n        activation_ops.silu_and_mul(out, x)\\n        return out\\n\\n\\nclass NewGELU(nn.Module):\\n\\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\\n        num_tokens = x.shape[0]\\n        d = x.shape[1]\\n        out = torch.empty(num_tokens, d, dtype=x.dtype, device=x.device)\\n        activation_ops.gelu_new(out, x)\\n        return out\\n\\n\\nclass FastGELU(nn.Module):\\n\\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\\n        num_tokens = x.shape[0]\\n        d = x.shape[1]\\n        out = torch.empty(num_tokens, d, dtype=x.dtype, device=x.device)\\n        activation_ops.gelu_fast(out, x)\\n        return out\\n\\n\\n_ACTIVATION_REGISTRY = {\\n    \"gelu\": nn.GELU(),\\n    \"gelu_fast\": FastGELU(),\\n    \"gelu_new\": NewGELU(),\\n    \"gelu_pytorch_tanh\": nn.GELU(approximate=\"tanh\"),\\n    \"relu\": nn.ReLU(),\\n}\\n\\n\\ndef get_act_fn(act_fn: str) -> nn.Module:\\n    \"\"\"Get an activation function by name.\"\"\"\\n    act_fn = act_fn.lower()\\n    if act_fn in _ACTIVATION_REGISTRY:\\n        return _ACTIVATION_REGISTRY[act_fn]\\n    raise ValueError(f\"Activation function {act_fn!r} is not supported.\")\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/layers/sampler.py\\n---------\\nContent:\\n\"\"\"A layer that samples the next tokens from the model\\'s outputs.\"\"\"\\nfrom typing import Dict, List, Tuple, Optional\\n\\nimport numpy as np\\nimport torch\\nimport torch.nn as nn\\n\\nfrom vllm.model_executor.input_metadata import InputMetadata\\nfrom vllm.model_executor.parallel_utils.tensor_parallel import (\\n    gather_from_tensor_model_parallel_region)\\nfrom vllm.sampling_params import SamplingParams\\nfrom vllm.sequence import SamplerOutput, SequenceOutputs\\n\\n_SAMPLING_EPS = 1e-5\\n\\n\\nclass Sampler(nn.Module):\\n    \"\"\"Samples the next tokens from the model\\'s outputs.\\n\\n    This layer does the following:\\n    1. Discard the hidden states that are not used for sampling (i.e., all\\n        tokens except the final one in each prompt).\\n    2. Compute the logits for the next tokens.\\n    3. Apply presence and frequency penalties.\\n    4. Apply temperature scaling.\\n    5. Apply top-p and top-k truncation.\\n    6. Sample the next tokens.\\n    Here, each sequence group within the batch can have different sampling\\n    parameters (e.g., sampling method, temperature, top-p, top-k, etc.).\\n    \"\"\"\\n\\n    def __init__(self, vocab_size: int) -> None:\\n        super().__init__()\\n        self.vocab_size = vocab_size\\n\\n    def forward(\\n        self,\\n        embedding: torch.Tensor,\\n        hidden_states: torch.Tensor,\\n        input_metadata: InputMetadata,\\n        embedding_bias: Optional[torch.Tensor] = None,\\n    ) -> SamplerOutput:\\n        # Get the hidden states that we use for sampling.\\n        hidden_states = _prune_hidden_states(hidden_states, input_metadata)\\n\\n        # Get the logits for the next tokens.\\n        logits = torch.matmul(hidden_states, embedding.t())\\n        if embedding_bias is not None:\\n            logits += embedding_bias\\n        logits = gather_from_tensor_model_parallel_region(logits)\\n        # Remove paddings in vocab (if any).\\n        logits = logits[:, :self.vocab_size]\\n\\n        # Apply presence and frequency penalties.\\n        output_tokens = _get_output_tokens(input_metadata)\\n        assert len(output_tokens) == logits.shape[0]\\n        presence_penalties, frequency_penalties = _get_penalties(\\n            input_metadata)\\n        assert len(presence_penalties) == logits.shape[0]\\n        assert len(frequency_penalties) == logits.shape[0]\\n        logits = _apply_penalties(logits, output_tokens, presence_penalties,\\n                                  frequency_penalties, self.vocab_size)\\n\\n        # Apply temperature scaling.\\n        temperatures = _get_temperatures(input_metadata)\\n        assert len(temperatures) == logits.shape[0]\\n        if any(t != 1.0 for t in temperatures):\\n            t = torch.tensor(temperatures,\\n                             dtype=logits.dtype,\\n                             device=logits.device)\\n            # Use in-place division to avoid creating a new tensor.\\n            logits.div_(t.unsqueeze(dim=1))\\n\\n        # Apply top-p and top-k truncation.\\n        top_ps, top_ks = _get_top_p_top_k(input_metadata, self.vocab_size)\\n        assert len(top_ps) == len(top_ks) == logits.shape[0]\\n        do_top_p = any(p < 1.0 - _SAMPLING_EPS for p in top_ps)\\n        do_top_k = any(k != self.vocab_size for k in top_ks)\\n        if do_top_p or do_top_k:\\n            logits = _apply_top_p_top_k(logits, top_ps, top_ks)\\n\\n        # We use float32 for probabilities and log probabilities.\\n        # Compute the probabilities.\\n        probs = torch.softmax(logits, dim=-1, dtype=torch.float)\\n        # Compute the log probabilities.\\n        # Use log_softmax to ensure numerical stability.\\n        logprobs = torch.log_softmax(logits, dim=-1, dtype=torch.float)\\n\\n        # Sample the next tokens.\\n        return _sample(probs, logprobs, input_metadata)\\n\\n\\ndef _prune_hidden_states(\\n    hidden_states: torch.Tensor,\\n    input_metadata: InputMetadata,\\n) -> torch.Tensor:\\n    start_idx = 0\\n    last_token_indicies: List[int] = []\\n    for prompt_len in input_metadata.prompt_lens:\\n        last_token_indicies.append(start_idx + prompt_len - 1)\\n        start_idx += prompt_len\\n    last_token_indicies.extend(\\n        range(start_idx, start_idx + input_metadata.num_generation_tokens))\\n    return hidden_states.index_select(\\n        0, torch.tensor(last_token_indicies, device=hidden_states.device))\\n\\n\\ndef _get_penalties(\\n        input_metadata: InputMetadata) -> Tuple[List[float], List[float]]:\\n    # Collect the presence and frequency penalties.\\n    presence_penalties: List[float] = []\\n    frequency_penalties: List[float] = []\\n    for i, seq_group in enumerate(input_metadata.seq_groups):\\n        seq_ids, sampling_params = seq_group\\n        p = sampling_params.presence_penalty\\n        f = sampling_params.frequency_penalty\\n        if i < input_metadata.num_prompts:\\n            # A prompt input.\\n            presence_penalties.append(p)\\n            frequency_penalties.append(f)\\n        else:\\n            # A generation token.\\n            presence_penalties += [p] * len(seq_ids)\\n            frequency_penalties += [f] * len(seq_ids)\\n    return presence_penalties, frequency_penalties\\n\\n\\ndef _get_output_tokens(input_metadata: InputMetadata) -> List[List[int]]:\\n    output_tokens: List[List[int]] = []\\n    for i, seq_group in enumerate(input_metadata.seq_groups):\\n        seq_ids, _ = seq_group\\n        if i < input_metadata.num_prompts:\\n            # A prompt input.\\n            # NOTE: While the prompt input usually has no output tokens,\\n            # it may have output tokens in the case of recomputation.\\n            seq_id = seq_ids[0]\\n            seq_data = input_metadata.seq_data[seq_id]\\n            output_tokens.append(seq_data.output_token_ids)\\n        else:\\n            # A generation token.\\n            for seq_id in seq_ids:\\n                seq_data = input_metadata.seq_data[seq_id]\\n                output_tokens.append(seq_data.output_token_ids)\\n    return output_tokens\\n\\n\\ndef _apply_penalties(\\n    logits: torch.Tensor,\\n    output_tokens: List[List[int]],\\n    presence_penalties: List[float],\\n    frequency_penalties: List[float],\\n    vocab_size: int,\\n) -> torch.Tensor:\\n    num_seqs = logits.shape[0]\\n    # Collect the indices of sequences that have non-zero penalties.\\n    indices = []\\n    for i in range(num_seqs):\\n        if not output_tokens[i]:\\n            continue\\n        p = presence_penalties[i]\\n        f = frequency_penalties[i]\\n        if abs(p) < _SAMPLING_EPS and abs(f) < _SAMPLING_EPS:\\n            continue\\n        indices.append(i)\\n\\n    # Return early if all sequences have zero penalties.\\n    if not indices:\\n        return logits\\n\\n    bin_counts = []\\n    for i in indices:\\n        bin_counts.append(np.bincount(output_tokens[i], minlength=vocab_size))\\n    bin_counts = np.stack(bin_counts, axis=0)\\n    bin_counts = torch.from_numpy(bin_counts).to(dtype=logits.dtype,\\n                                                 device=logits.device)\\n\\n    frequency_penalties = [frequency_penalties[i] for i in indices]\\n    frequency_penalties = torch.tensor(frequency_penalties,\\n                                       dtype=logits.dtype,\\n                                       device=logits.device)\\n    presence_penalties = [presence_penalties[i] for i in indices]\\n    presence_penalties = torch.tensor(presence_penalties,\\n                                      dtype=logits.dtype,\\n                                      device=logits.device)\\n\\n    # We follow the definition in OpenAI API.\\n    # Refer to https://platform.openai.com/docs/api-reference/parameter-details\\n    logits[indices] -= frequency_penalties.unsqueeze(dim=1) * bin_counts\\n    presence_mask = (bin_counts > 0.0).to(dtype=logits.dtype)\\n    logits[indices] -= presence_penalties.unsqueeze(dim=1) * presence_mask\\n    return logits\\n\\n\\ndef _get_temperatures(input_metadata: InputMetadata) -> List[float]:\\n    # Collect the temperatures for the logits.\\n    temperatures: List[float] = []\\n    for i, seq_group in enumerate(input_metadata.seq_groups):\\n        seq_ids, sampling_params = seq_group\\n        temperature = sampling_params.temperature\\n        if temperature < _SAMPLING_EPS:\\n            # NOTE: Zero temperature means deterministic sampling\\n            # (i.e., greedy sampling or beam search).\\n            # Set the temperature to 1 to avoid division by zero.\\n            temperature = 1.0\\n\\n        if i < input_metadata.num_prompts:\\n            # A prompt input.\\n            temperatures.append(temperature)\\n        else:\\n            # A generation token.\\n            temperatures += [temperature] * len(seq_ids)\\n    return temperatures\\n\\n\\ndef _get_top_p_top_k(\\n    input_metadata: InputMetadata,\\n    vocab_size: int,\\n) -> Tuple[List[float], List[int]]:\\n    top_ps: List[float] = []\\n    top_ks: List[int] = []\\n    for i, seq_group in enumerate(input_metadata.seq_groups):\\n        seq_ids, sampling_params = seq_group\\n        top_p = sampling_params.top_p\\n        # k should not be greater than the vocab size.\\n        top_k = min(sampling_params.top_k, vocab_size)\\n        # k=-1 means no truncation.\\n        top_k = vocab_size if top_k == -1 else top_k\\n        if i < input_metadata.num_prompts:\\n            # A prompt input.\\n            top_ps.append(top_p)\\n            top_ks.append(top_k)\\n        else:\\n            # A generation token.\\n            top_ps += [top_p] * len(seq_ids)\\n            top_ks += [top_k] * len(seq_ids)\\n    return top_ps, top_ks\\n\\n\\ndef _apply_top_p_top_k(\\n    logits: torch.Tensor,\\n    top_ps: List[float],\\n    top_ks: List[int],\\n) -> torch.Tensor:\\n    p = torch.tensor(top_ps, dtype=logits.dtype, device=logits.device)\\n    k = torch.tensor(top_ks, dtype=torch.int, device=logits.device)\\n    logits_sort, logits_idx = logits.sort(dim=-1, descending=True)\\n\\n    # Apply top-p.\\n    probs_sort = logits_sort.softmax(dim=-1)\\n    probs_sum = probs_sort.cumsum(dim=-1)\\n    top_p_mask = (probs_sum - probs_sort) > p.unsqueeze(dim=1)\\n    logits_sort[top_p_mask] = -float(\"inf\")\\n\\n    # Apply top-k.\\n    # Create a mask for the top-k elements.\\n    top_k_mask = torch.arange(logits_idx.shape[-1], device=logits_idx.device)\\n    top_k_mask = top_k_mask.expand(logits_idx.shape[0], -1)\\n    top_k_mask = top_k_mask >= k.unsqueeze(dim=1)\\n    logits_sort[top_k_mask] = -float(\"inf\")\\n\\n    # Re-sort the probabilities.\\n    logits = torch.gather(logits_sort,\\n                          dim=-1,\\n                          index=torch.argsort(logits_idx, dim=-1))\\n    return logits\\n\\n\\ndef _get_topk_logprobs(\\n    logprobs: torch.Tensor,\\n    num_logprobs: Optional[int],\\n) -> Dict[int, float]:\\n    if num_logprobs is None or num_logprobs == 0:\\n        return {}\\n\\n    topk_logprobs, topk_ids = torch.topk(logprobs, num_logprobs)\\n    if num_logprobs == 1:\\n        topk_logprobs = [topk_logprobs.item()]\\n        topk_ids = [topk_ids.item()]\\n    else:\\n        topk_logprobs = topk_logprobs.tolist()\\n        topk_ids = topk_ids.tolist()\\n\\n    token_to_logprob: Dict[int, float] = {}\\n    for token_id, logprob in zip(topk_ids, topk_logprobs):\\n        token_to_logprob[token_id] = logprob\\n    return token_to_logprob\\n\\n\\ndef _sample_from_prompt(\\n    prob: torch.Tensor,\\n    sampling_params: SamplingParams,\\n) -> List[int]:\\n    if sampling_params.use_beam_search:\\n        # Beam search.\\n        beam_width = sampling_params.best_of\\n        # Sample 2 * beam_width candidates to make sure that with high\\n        # probability we can get `beam_width` candidates in addition to\\n        # the finished sequences for the next iteration. See\\n        # https://github.com/tensorflow/tensor2tensor/blob/bafdc1b67730430d38d6ab802cbd51f9d053ba2e/tensor2tensor/utils/beam_search.py#L557-L563\\n        # for details. See also HF reference:\\n        # https://github.com/huggingface/transformers/blob/a4dd53d88e4852f023332d284ff07a01afcd5681/src/transformers/generation/utils.py#L3063-L3065\\n        _, next_token_ids = torch.topk(prob, 2 * beam_width)\\n        next_token_ids = next_token_ids.tolist()\\n    elif sampling_params.temperature < _SAMPLING_EPS:\\n        # Greedy sampling.\\n        assert sampling_params.best_of == 1\\n        next_token_id = torch.argmax(prob)\\n        next_token_ids = [next_token_id.item()]\\n    else:\\n        # Random sampling.\\n        # Sample `best_of` tokens for the prompt.\\n        num_seqs = sampling_params.best_of\\n        next_token_ids = torch.multinomial(prob,\\n                                           num_samples=num_seqs,\\n                                           replacement=True)\\n        next_token_ids = next_token_ids.tolist()\\n    return next_token_ids\\n\\n\\ndef _sample_from_generation_tokens(\\n    seq_ids: List[int],\\n    probs: torch.Tensor,\\n    logprobs: torch.Tensor,\\n    seq_logprobs: List[float],\\n    sampling_params: SamplingParams,\\n) -> Tuple[List[int], List[int]]:\\n    # NOTE(woosuk): sampling_params.best_of can be greater than\\n    # len(seq_ids) because some sequences in the group might have\\n    # been already terminated.\\n    if sampling_params.use_beam_search:\\n        # Beam search.\\n        # Add cumulative logprobs for the sequences in the group.\\n        seq_logprobs = torch.tensor(seq_logprobs,\\n                                    dtype=torch.float,\\n                                    device=logprobs.device)\\n        logprobs = logprobs + seq_logprobs.unsqueeze(dim=1)\\n\\n        vocab_size = logprobs.size(-1)\\n        beam_width = len(seq_ids)\\n        _, topk_ids = torch.topk(logprobs.flatten(), 2 * beam_width)\\n        topk_ids = topk_ids.tolist()\\n        seq_idx = [i // vocab_size for i in topk_ids]\\n        parent_seq_ids = [seq_ids[i] for i in seq_idx]\\n        next_token_ids = [i % vocab_size for i in topk_ids]\\n    elif sampling_params.temperature < _SAMPLING_EPS:\\n        # Greedy sampling.\\n        assert len(seq_ids) == 1\\n        next_token_id = torch.argmax(probs, dim=-1)\\n        next_token_ids = [int(next_token_id.item())]\\n        parent_seq_ids = seq_ids\\n    else:\\n        # Random sampling.\\n        # Sample 1 token for each sequence in the group.\\n        next_token_ids = torch.multinomial(probs,\\n                                           num_samples=1,\\n                                           replacement=True)\\n        next_token_ids = next_token_ids.squeeze(dim=-1).tolist()\\n        parent_seq_ids = seq_ids\\n    return parent_seq_ids, next_token_ids\\n\\n\\ndef _sample(\\n    probs: torch.Tensor,\\n    logprobs: torch.Tensor,\\n    input_metadata: InputMetadata,\\n) -> SamplerOutput:\\n    seq_outputs: SamplerOutput = []\\n\\n    # TODO(woosuk): Optimize.\\n    idx = 0\\n    for i, seq_group in enumerate(input_metadata.seq_groups):\\n        seq_group_outputs: List[SequenceOutputs] = []\\n        seq_ids, sampling_params = seq_group\\n        if i < input_metadata.num_prompts:\\n            # Generate the next tokens for a prompt input.\\n            assert len(seq_ids) == 1, \"Prompt input should have only one seq.\"\\n            parent_seq_id = seq_ids[0]\\n            prob = probs[idx]\\n            logprob = logprobs[idx]\\n            idx += 1\\n\\n            # Sample the next tokens.\\n            next_token_ids = _sample_from_prompt(prob, sampling_params)\\n            # Get top-k log probabilities for the next tokens.\\n            next_logprobs = _get_topk_logprobs(logprob,\\n                                               sampling_params.logprobs)\\n\\n            # Build the output.\\n            for next_token_id in next_token_ids:\\n                output_logprobs = next_logprobs.copy()\\n                output_logprobs[next_token_id] = logprob[next_token_id].item()\\n                seq_group_outputs.append(\\n                    SequenceOutputs(parent_seq_id, next_token_id,\\n                                    output_logprobs))\\n        else:\\n            # Generate the next tokens for generation tokens.\\n            num_parent_seqs = len(seq_ids)\\n            prob = probs[idx:idx + num_parent_seqs]\\n            logprob = logprobs[idx:idx + num_parent_seqs]\\n            idx += num_parent_seqs\\n\\n            # Sample the next tokens.\\n            seq_logprobs = [\\n                input_metadata.seq_data[seq_id].cumulative_logprob\\n                for seq_id in seq_ids\\n            ]\\n            parent_seq_ids, next_token_ids = _sample_from_generation_tokens(\\n                seq_ids, prob, logprob, seq_logprobs, sampling_params)\\n\\n            # Get top-k log probabilities for the next tokens.\\n            next_logprobs: Dict[int, Dict[int, float]] = {}\\n            for j, seq_id in enumerate(seq_ids):\\n                next_logprobs[seq_id] = _get_topk_logprobs(\\n                    logprob[j], sampling_params.logprobs)\\n\\n            # Build the output.\\n            for parent_seq_id, next_token_id in zip(parent_seq_ids,\\n                                                    next_token_ids):\\n                j = seq_ids.index(parent_seq_id)\\n                output_logprobs = next_logprobs[parent_seq_id].copy()\\n                output_logprobs[next_token_id] = logprob[j,\\n                                                         next_token_id].item()\\n                seq_group_outputs.append(\\n                    SequenceOutputs(parent_seq_id, next_token_id,\\n                                    output_logprobs))\\n        seq_outputs.append(seq_group_outputs)\\n\\n    return seq_outputs\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/layers/quantized_linear/__init__.py\\n---------\\nContent:\\nfrom vllm.model_executor.layers.quantized_linear.awq import (\\n    AWQColumnParallelLinear, AWQRowParallelLinear)\\nfrom vllm.model_executor.parallel_utils.tensor_parallel import (\\n    ColumnParallelLinear, RowParallelLinear)\\n\\n_QUANTIZED_LINEAR_REGISTRY = {\\n    \"awq\": (AWQColumnParallelLinear, AWQRowParallelLinear),\\n}\\n\\n\\nclass ParallelLinear:\\n\\n    @classmethod\\n    def column(cls, *args, **kwargs) -> ColumnParallelLinear:\\n        quant_config = kwargs.get(\"quant_config\", None)\\n        if quant_config is None:\\n            return ColumnParallelLinear(*args, **kwargs)\\n\\n        name = quant_config.get_name()\\n        if name not in _QUANTIZED_LINEAR_REGISTRY:\\n            raise ValueError(f\"No quantized linear is found for {name}\")\\n\\n        quant_linear_cls = _QUANTIZED_LINEAR_REGISTRY[name][0]\\n        return quant_linear_cls(*args, **kwargs)\\n\\n    @classmethod\\n    def row(cls, *args, **kwargs) -> RowParallelLinear:\\n        quant_config = kwargs.get(\"quant_config\", None)\\n        if quant_config is None:\\n            return RowParallelLinear(*args, **kwargs)\\n\\n        name = quant_config.get_name()\\n        if name not in _QUANTIZED_LINEAR_REGISTRY:\\n            raise ValueError(f\"No quantized linear is found for {name}\")\\n\\n        quant_linear_cls = _QUANTIZED_LINEAR_REGISTRY[name][1]\\n        return quant_linear_cls(*args, **kwargs)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/layers/quantized_linear/awq.py\\n---------\\nContent:\\nfrom typing import Optional\\n\\nimport torch\\nfrom torch.nn.parameter import Parameter\\n\\nfrom vllm import quantization_ops\\nfrom vllm.model_executor.parallel_utils.tensor_parallel.layers import (\\n    ColumnParallelLinear, RowParallelLinear)\\n\\n\\nclass AWQColumnParallelLinear(ColumnParallelLinear):\\n\\n    def create_weights(self, dtype: torch.dtype) -> None:\\n        assert self.input_size % self.quant_config.weight_bits == 0\\n        assert (self.output_size_per_partition %\\n                self.quant_config.pack_factor == 0)\\n        self.qweight = Parameter(\\n            torch.empty(\\n                self.input_size,\\n                self.output_size_per_partition //\\n                self.quant_config.pack_factor,\\n                device=\"cuda\",\\n                dtype=torch.int32,\\n            ),\\n            requires_grad=False,\\n        )\\n        self.qzeros = Parameter(\\n            torch.empty(\\n                self.input_size // self.quant_config.group_size,\\n                self.output_size_per_partition //\\n                self.quant_config.pack_factor,\\n                device=\"cuda\",\\n                dtype=torch.int32,\\n            ),\\n            requires_grad=False,\\n        )\\n        self.scales = Parameter(\\n            torch.empty(\\n                self.input_size // self.quant_config.group_size,\\n                self.output_size_per_partition,\\n                device=\"cuda\",\\n                dtype=dtype,\\n            ),\\n            requires_grad=False,\\n        )\\n\\n    def apply_weights(\\n        self,\\n        x: torch.Tensor,\\n        bias: Optional[torch.Tensor],\\n    ) -> torch.Tensor:\\n        pack_factor = self.quant_config.pack_factor\\n        out_shape = (x.shape[-2], self.qweight.shape[-1] * pack_factor)\\n        reshaped_x = x.reshape(-1, x.shape[-1])\\n        out = quantization_ops.awq_gemm(reshaped_x, self.qweight, self.scales,\\n                                        self.qzeros, pack_factor)\\n        if bias is not None:\\n            out = out + bias\\n        return out.reshape(out_shape)\\n\\n\\nclass AWQRowParallelLinear(RowParallelLinear):\\n\\n    def create_weights(self, dtype: torch.dtype) -> None:\\n        assert (self.input_size_per_partition %\\n                self.quant_config.weight_bits == 0)\\n        assert self.output_size % self.quant_config.pack_factor == 0\\n        self.qweight = Parameter(\\n            torch.empty(\\n                self.input_size_per_partition,\\n                self.output_size // self.quant_config.pack_factor,\\n                device=\"cuda\",\\n                dtype=torch.int32,\\n            ),\\n            requires_grad=False,\\n        )\\n        self.qzeros = Parameter(\\n            torch.empty(\\n                self.input_size_per_partition // self.quant_config.group_size,\\n                self.output_size // self.quant_config.pack_factor,\\n                device=\"cuda\",\\n                dtype=torch.int32,\\n            ),\\n            requires_grad=False,\\n        )\\n        self.scales = Parameter(\\n            torch.empty(\\n                self.input_size_per_partition // self.quant_config.group_size,\\n                self.output_size,\\n                device=\"cuda\",\\n                dtype=dtype,\\n            ),\\n            requires_grad=False,\\n        )\\n\\n    def apply_weights(self, x: torch.Tensor) -> torch.Tensor:\\n        pack_factor = self.quant_config.pack_factor\\n        out_shape = (x.shape[-2], self.qweight.shape[-1] * pack_factor)\\n        reshaped_x = x.reshape(-1, x.shape[-1])\\n        out = quantization_ops.awq_gemm(reshaped_x, self.qweight, self.scales,\\n                                        self.qzeros, pack_factor)\\n        return out.reshape(out_shape)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/models/aquila.py\\n---------\\nContent:\\n# coding=utf-8\\n# Adapted from\\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/llama/modeling_llama.py\\n# Copyright 2023 The vLLM team.\\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\\n#\\n# This code is based on EleutherAI\\'s GPT-NeoX library and the GPT-NeoX\\n# and OPT implementations in this library. It has been modified from its\\n# original forms to accommodate minor architectural differences compared\\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\"\"\"Inference-only LLaMA model compatible with HuggingFace weights.\\n\\nThe input of the model is flattened to a 1D tensor of tokens. The model uses\\nInputMetadata to extract the original 2D shape of the input.\\n\"\"\"\\nfrom typing import List, Optional, Tuple\\n\\nimport torch\\nfrom torch import nn\\n\\nfrom vllm.model_executor.input_metadata import InputMetadata\\nfrom vllm.model_executor.layers.activation import SiluAndMul\\nfrom vllm.model_executor.layers.attention import PagedAttentionWithRoPE\\nfrom vllm.model_executor.layers.sampler import Sampler\\nfrom vllm.model_executor.weight_utils import (\\n    hf_model_weights_iterator, load_padded_tensor_parallel_vocab,\\n    load_tensor_parallel_weights)\\nfrom vllm.model_executor.parallel_utils.parallel_state import (\\n    get_tensor_model_parallel_rank, get_tensor_model_parallel_world_size)\\nfrom vllm.model_executor.parallel_utils.tensor_parallel import (\\n    VocabParallelEmbedding, ColumnParallelLinear, RowParallelLinear)\\nfrom vllm.sequence import SamplerOutput\\nfrom vllm.transformers_utils.configs.aquila import AquilaConfig\\n\\nKVCache = Tuple[torch.Tensor, torch.Tensor]\\n\\n\\nclass AquilaMLP(nn.Module):\\n\\n    def __init__(\\n        self,\\n        hidden_size: int,\\n        intermediate_size: int,\\n        hidden_act: str,\\n    ):\\n        super().__init__()\\n        self.gate_up_proj = ColumnParallelLinear(hidden_size,\\n                                                 2 * intermediate_size,\\n                                                 bias=False,\\n                                                 gather_output=False,\\n                                                 perform_initialization=False)\\n        self.down_proj = RowParallelLinear(intermediate_size,\\n                                           hidden_size,\\n                                           bias=False,\\n                                           input_is_parallel=True,\\n                                           perform_initialization=False)\\n        if hidden_act != \"silu\":\\n            raise ValueError(f\"Unsupported activation: {hidden_act}. \"\\n                             \"Only silu is supported for now.\")\\n        self.act_fn = SiluAndMul()\\n\\n    def forward(self, x):\\n        gate_up, _ = self.gate_up_proj(x)\\n        x = self.act_fn(gate_up)\\n        x, _ = self.down_proj(x)\\n        return x\\n\\n\\nclass AquilaRMSNorm(nn.Module):\\n\\n    def __init__(self, hidden_size, eps=1e-6):\\n        \"\"\"\\n        AquilaRMSNorm is equivalent to T5LayerNorm\\n        \"\"\"\\n        super().__init__()\\n        self.weight = nn.Parameter(torch.ones(hidden_size))\\n        self.variance_epsilon = eps\\n\\n    def forward(self, hidden_states):\\n        input_dtype = hidden_states.dtype\\n        variance = hidden_states.to(torch.float32).pow(2).mean(-1,\\n                                                               keepdim=True)\\n        hidden_states = hidden_states * torch.rsqrt(variance +\\n                                                    self.variance_epsilon)\\n\\n        return (self.weight * hidden_states).to(input_dtype)\\n\\n\\nclass AquilaAttention(nn.Module):\\n\\n    def __init__(\\n        self,\\n        hidden_size: int,\\n        num_heads: int,\\n        num_kv_heads: int,\\n    ):\\n        super().__init__()\\n        self.hidden_size = hidden_size\\n        tp_size = get_tensor_model_parallel_world_size()\\n        self.total_num_heads = num_heads\\n        assert self.total_num_heads % tp_size == 0\\n        self.num_heads = self.total_num_heads // tp_size\\n        self.total_num_kv_heads = num_kv_heads\\n        assert self.total_num_kv_heads % tp_size == 0\\n        self.num_kv_heads = self.total_num_kv_heads // tp_size\\n        self.head_dim = hidden_size // self.total_num_heads\\n        self.q_size = self.num_heads * self.head_dim\\n        self.kv_size = self.num_kv_heads * self.head_dim\\n        self.scaling = self.head_dim**-0.5\\n\\n        self.qkv_proj = ColumnParallelLinear(\\n            hidden_size,\\n            (self.total_num_heads + 2 * self.total_num_kv_heads) *\\n            self.head_dim,\\n            bias=False,\\n            gather_output=False,\\n            perform_initialization=False,\\n        )\\n        self.o_proj = RowParallelLinear(\\n            self.total_num_heads * self.head_dim,\\n            hidden_size,\\n            bias=False,\\n            input_is_parallel=True,\\n            perform_initialization=False,\\n        )\\n        self.attn = PagedAttentionWithRoPE(\\n            self.num_heads,\\n            self.head_dim,\\n            self.scaling,\\n            rotary_dim=self.head_dim,\\n        )\\n\\n    def forward(\\n        self,\\n        positions: torch.Tensor,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        qkv, _ = self.qkv_proj(hidden_states)\\n        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\\n        k_cache, v_cache = kv_cache\\n        attn_output = self.attn(positions, q, k, v, k_cache, v_cache,\\n                                input_metadata, cache_event)\\n        output, _ = self.o_proj(attn_output)\\n        return output\\n\\n\\nclass AquilaDecoderLayer(nn.Module):\\n\\n    def __init__(self, config: AquilaConfig):\\n        super().__init__()\\n        self.hidden_size = config.hidden_size\\n        self.self_attn = AquilaAttention(\\n            hidden_size=self.hidden_size,\\n            num_heads=config.num_attention_heads,\\n            num_kv_heads=config.num_attention_heads,\\n        )\\n        self.mlp = AquilaMLP(\\n            hidden_size=self.hidden_size,\\n            intermediate_size=config.intermediate_size,\\n            hidden_act=config.hidden_act,\\n        )\\n        self.input_layernorm = AquilaRMSNorm(config.hidden_size,\\n                                             eps=config.rms_norm_eps)\\n        self.post_attention_layernorm = AquilaRMSNorm(config.hidden_size,\\n                                                      eps=config.rms_norm_eps)\\n\\n    def forward(\\n        self,\\n        positions: torch.Tensor,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        # Self Attention\\n        residual = hidden_states\\n        hidden_states = self.input_layernorm(hidden_states)\\n        hidden_states = self.self_attn(\\n            positions=positions,\\n            hidden_states=hidden_states,\\n            kv_cache=kv_cache,\\n            input_metadata=input_metadata,\\n            cache_event=cache_event,\\n        )\\n        hidden_states = residual + hidden_states\\n\\n        # Fully Connected\\n        residual = hidden_states\\n        hidden_states = self.post_attention_layernorm(hidden_states)\\n        hidden_states = self.mlp(hidden_states)\\n        hidden_states = residual + hidden_states\\n        return hidden_states\\n\\n\\nclass AquilaModel(nn.Module):\\n\\n    def __init__(self, config: AquilaConfig):\\n        super().__init__()\\n        self.config = config\\n        self.padding_idx = config.pad_token_id\\n        self.vocab_size = config.vocab_size\\n\\n        #vocab_size = ((config.vocab_size + 63) // 64) * 64\\n        self.embed_tokens = VocabParallelEmbedding(\\n            config.vocab_size,\\n            config.hidden_size,\\n            perform_initialization=False)\\n        self.layers = nn.ModuleList([\\n            AquilaDecoderLayer(config) for _ in range(config.num_hidden_layers)\\n        ])\\n        self.norm = AquilaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        positions: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> torch.Tensor:\\n        hidden_states = self.embed_tokens(input_ids)\\n        for i in range(len(self.layers)):\\n            if cache_events is None:\\n                cache_event = None\\n            else:\\n                cache_event = cache_events[i]\\n            layer = self.layers[i]\\n            hidden_states = layer(\\n                positions,\\n                hidden_states,\\n                kv_caches[i],\\n                input_metadata,\\n                cache_event,\\n            )\\n        hidden_states = self.norm(hidden_states)\\n\\n        return hidden_states\\n\\n\\nclass AquilaForCausalLM(nn.Module):\\n\\n    def __init__(self, config):\\n        super().__init__()\\n        self.config = config\\n        self.model = AquilaModel(config)\\n        vocab_size = ((config.vocab_size + 63) // 64) * 64\\n        self.lm_head = ColumnParallelLinear(config.hidden_size,\\n                                            vocab_size,\\n                                            bias=False,\\n                                            gather_output=False,\\n                                            perform_initialization=False)\\n        self.sampler = Sampler(config.vocab_size)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        positions: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> SamplerOutput:\\n        hidden_states = self.model(input_ids, positions, kv_caches,\\n                                   input_metadata, cache_events)\\n        next_tokens = self.sampler(self.lm_head.weight, hidden_states,\\n                                   input_metadata)\\n        return next_tokens\\n\\n    _column_parallel_weights = [\\n        \"qkv_proj.weight\", \"gate_proj.weight\", \"up_proj.weight\"\\n    ]\\n    _row_parallel_weights = [\"o_proj.weight\", \"down_proj.weight\"]\\n\\n    def load_weights(self,\\n                     model_name_or_path: str,\\n                     cache_dir: Optional[str] = None,\\n                     load_format: str = \"auto\",\\n                     revision: Optional[str] = None):\\n        tp_size = get_tensor_model_parallel_world_size()\\n        tensor_model_parallel_rank = get_tensor_model_parallel_rank()\\n        q_proj_shard_size = (self.config.hidden_size // tp_size)\\n        kv_proj_shard_size = (self.config.hidden_size //\\n                              self.config.num_attention_heads *\\n                              self.config.num_attention_heads // tp_size)\\n        attention_weight_specs = [\\n            # (weight_name, shard_size, offset)\\n            (\"q_proj\", q_proj_shard_size, 0),\\n            (\"k_proj\", kv_proj_shard_size, q_proj_shard_size),\\n            (\"v_proj\", kv_proj_shard_size,\\n             q_proj_shard_size + kv_proj_shard_size),\\n        ]\\n        state_dict = self.state_dict()\\n\\n        for name, loaded_weight in hf_model_weights_iterator(\\n                model_name_or_path, cache_dir, load_format, revision):\\n            if \"rotary_emb.inv_freq\" in name:\\n                continue\\n\\n            is_attention_weight = False\\n            for weight_name, shard_size, offset in attention_weight_specs:\\n                if weight_name not in name:\\n                    continue\\n                param = state_dict[name.replace(weight_name, \"qkv_proj\")]\\n\\n                loaded_weight = loaded_weight[\\n                    shard_size * tensor_model_parallel_rank:shard_size *\\n                    (tensor_model_parallel_rank + 1)]\\n                param_slice = param.data[offset:offset + shard_size]\\n                assert param_slice.shape == loaded_weight.shape\\n\\n                param_slice.copy_(loaded_weight)\\n                is_attention_weight = True\\n                break\\n            if is_attention_weight:\\n                continue\\n\\n            is_gate_up_weight = False\\n            for stride_id, weight_name in enumerate([\"gate_proj\", \"up_proj\"]):\\n                if weight_name not in name:\\n                    continue\\n                param = state_dict[name.replace(weight_name, \"gate_up_proj\")]\\n                shard_size = param.shape[0] // 2\\n                loaded_weight = loaded_weight[\\n                    shard_size * tensor_model_parallel_rank:shard_size *\\n                    (tensor_model_parallel_rank + 1)]\\n                param_slice = param.data[shard_size * stride_id:shard_size *\\n                                         (stride_id + 1)]\\n                assert param_slice.shape == loaded_weight.shape\\n                param_slice.copy_(loaded_weight)\\n                is_gate_up_weight = True\\n                break\\n            if is_gate_up_weight:\\n                continue\\n\\n            param = state_dict[name]\\n            if \"embed_tokens\" in name or \"lm_head\" in name:\\n                load_padded_tensor_parallel_vocab(param, loaded_weight,\\n                                                  tensor_model_parallel_rank)\\n                continue\\n\\n            load_tensor_parallel_weights(param, loaded_weight, name,\\n                                         self._column_parallel_weights,\\n                                         self._row_parallel_weights,\\n                                         tensor_model_parallel_rank)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/models/gpt_j.py\\n---------\\nContent:\\n# coding=utf-8\\n# Adapted from\\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/gptj/modeling_gptj.py\\n# Copyright 2023 The vLLM team.\\n# Copyright 2021 The EleutherAI and HuggingFace Teams. All rights reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\"\"\"Inference-only GPT-J model compatible with HuggingFace weights.\\n\\nThe input of the model is flattened to a 1D tensor of tokens. The model uses\\nInputMetadata to extract the original 2D shape of the input.\\n\"\"\"\\nfrom typing import List, Optional, Tuple\\n\\nimport torch\\nfrom torch import nn\\nfrom transformers import GPTJConfig\\n\\nfrom vllm.model_executor.input_metadata import InputMetadata\\nfrom vllm.model_executor.layers.activation import get_act_fn\\nfrom vllm.model_executor.layers.attention import PagedAttentionWithRoPE\\nfrom vllm.model_executor.layers.sampler import Sampler\\nfrom vllm.model_executor.weight_utils import (hf_model_weights_iterator,\\n                                              load_tensor_parallel_weights)\\nfrom vllm.model_executor.parallel_utils.parallel_state import (\\n    get_tensor_model_parallel_rank, get_tensor_model_parallel_world_size)\\nfrom vllm.model_executor.parallel_utils.tensor_parallel import (\\n    VocabParallelEmbedding, ColumnParallelLinear, RowParallelLinear)\\nfrom vllm.sequence import SamplerOutput\\n\\nKVCache = Tuple[torch.Tensor, torch.Tensor]\\n\\n\\nclass GPTJAttention(nn.Module):\\n\\n    def __init__(self, config: GPTJConfig):\\n        super().__init__()\\n        self.total_num_heads = config.num_attention_heads\\n        self.hidden_size = config.hidden_size\\n        self.head_size = self.hidden_size // self.total_num_heads\\n\\n        self.qkv_proj = ColumnParallelLinear(config.hidden_size,\\n                                             3 * config.hidden_size,\\n                                             bias=False,\\n                                             gather_output=False,\\n                                             perform_initialization=False)\\n        self.out_proj = RowParallelLinear(config.hidden_size,\\n                                          config.hidden_size,\\n                                          bias=False,\\n                                          input_is_parallel=True,\\n                                          perform_initialization=False)\\n\\n        tp_world_size = get_tensor_model_parallel_world_size()\\n        assert self.total_num_heads % tp_world_size == 0\\n        self.num_heads = self.total_num_heads // tp_world_size\\n\\n        scaling = self.head_size**-0.5\\n        assert getattr(config, \"rotary\", True)\\n        assert config.rotary_dim % 2 == 0\\n        self.attn = PagedAttentionWithRoPE(self.num_heads,\\n                                           self.head_size,\\n                                           scaling,\\n                                           config.rotary_dim,\\n                                           is_neox_style=False)\\n        self.warmup = False\\n\\n    def forward(\\n        self,\\n        position_ids: torch.Tensor,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        qkv, _ = self.qkv_proj(hidden_states)\\n        q, k, v = qkv.chunk(chunks=3, dim=-1)\\n        k_cache, v_cache = kv_cache\\n        attn_output = self.attn(position_ids, q, k, v, k_cache, v_cache,\\n                                input_metadata, cache_event)\\n        attn_output, _ = self.out_proj(attn_output)\\n        return attn_output\\n\\n\\nclass GPTJMLP(nn.Module):\\n\\n    def __init__(self, intermediate_size: int, config: GPTJConfig):\\n        super().__init__()\\n        hidden_size = config.n_embd\\n        self.fc_in = ColumnParallelLinear(hidden_size,\\n                                          intermediate_size,\\n                                          gather_output=False,\\n                                          perform_initialization=False)\\n        self.fc_out = RowParallelLinear(intermediate_size,\\n                                        hidden_size,\\n                                        input_is_parallel=True,\\n                                        perform_initialization=False)\\n        self.act = get_act_fn(config.activation_function)\\n\\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\\n        hidden_states, _ = self.fc_in(hidden_states)\\n        hidden_states = self.act(hidden_states)\\n        hidden_states, _ = self.fc_out(hidden_states)\\n        return hidden_states\\n\\n\\nclass GPTJBlock(nn.Module):\\n\\n    def __init__(self, config: GPTJConfig):\\n        super().__init__()\\n        if config.n_inner is None:\\n            inner_dim = 4 * config.n_embd\\n        else:\\n            inner_dim = config.n_inner\\n        self.ln_1 = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\\n        self.attn = GPTJAttention(config)\\n        self.mlp = GPTJMLP(inner_dim, config)\\n\\n    def forward(\\n        self,\\n        position_ids: torch.Tensor,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        residual = hidden_states\\n        hidden_states = self.ln_1(hidden_states)\\n        attn_output = self.attn(\\n            position_ids=position_ids,\\n            hidden_states=hidden_states,\\n            kv_cache=kv_cache,\\n            input_metadata=input_metadata,\\n            cache_event=cache_event,\\n        )\\n        mlp_output = self.mlp(hidden_states)\\n        hidden_states = attn_output + mlp_output + residual\\n        return hidden_states\\n\\n\\nclass GPTJModel(nn.Module):\\n\\n    def __init__(self, config: GPTJConfig):\\n        super().__init__()\\n        self.config = config\\n        self.embed_dim = config.n_embd\\n        self.wte = VocabParallelEmbedding(config.vocab_size,\\n                                          self.embed_dim,\\n                                          perform_initialization=False)\\n        self.h = nn.ModuleList(\\n            [GPTJBlock(config) for _ in range(config.n_layer)])\\n        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        position_ids: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> torch.Tensor:\\n        hidden_states = self.wte(input_ids)\\n        for i in range(len(self.h)):\\n            if cache_events is None:\\n                cache_event = None\\n            else:\\n                cache_event = cache_events[i]\\n            layer = self.h[i]\\n            hidden_states = layer(\\n                position_ids,\\n                hidden_states,\\n                kv_caches[i],\\n                input_metadata,\\n                cache_event,\\n            )\\n        hidden_states = self.ln_f(hidden_states)\\n        return hidden_states\\n\\n\\nclass GPTJForCausalLM(nn.Module):\\n\\n    def __init__(self, config: GPTJConfig):\\n        super().__init__()\\n        self.config = config\\n        assert not config.tie_word_embeddings\\n        self.transformer = GPTJModel(config)\\n        self.lm_head = ColumnParallelLinear(config.n_embd,\\n                                            config.vocab_size,\\n                                            gather_output=False,\\n                                            perform_initialization=False)\\n        self.sampler = Sampler(config.vocab_size)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        positions: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> SamplerOutput:\\n        hidden_states = self.transformer(input_ids, positions, kv_caches,\\n                                         input_metadata, cache_events)\\n        next_tokens = self.sampler(self.lm_head.weight, hidden_states,\\n                                   input_metadata, self.lm_head.bias)\\n        return next_tokens\\n\\n    _column_parallel_weights = [\\n        \"wte.weight\", \"fc_in.weight\", \"fc_in.bias\", \"lm_head.weight\",\\n        \"lm_head.bias\"\\n    ]\\n    _row_parallel_weights = [\"out_proj.weight\", \"fc_out.weight\"]\\n\\n    def load_weights(self,\\n                     model_name_or_path: str,\\n                     cache_dir: Optional[str] = None,\\n                     load_format: str = \"auto\",\\n                     revision: Optional[str] = None):\\n        tp_rank = get_tensor_model_parallel_rank()\\n        state_dict = self.state_dict()\\n        for name, loaded_weight in hf_model_weights_iterator(\\n                model_name_or_path, cache_dir, load_format, revision):\\n            if \"attn.bias\" in name or \"attn.masked_bias\" in name:\\n                continue\\n\\n            is_attention_weight = False\\n            for stride_id, att_weight_name in enumerate(\\n                [\"q_proj\", \"k_proj\", \"v_proj\"]):\\n                if att_weight_name not in name:\\n                    continue\\n                param = state_dict[name.replace(att_weight_name, \"qkv_proj\")]\\n                shard_size = param.shape[1]\\n                loaded_weight = loaded_weight[shard_size * tp_rank:shard_size *\\n                                              (tp_rank + 1)]\\n                param_slice = param.data[shard_size * stride_id:shard_size *\\n                                         (stride_id + 1)]\\n                assert param_slice.shape == loaded_weight.shape\\n                param_slice.copy_(loaded_weight)\\n                is_attention_weight = True\\n                break\\n            if is_attention_weight:\\n                continue\\n\\n            param = state_dict[name]\\n            load_tensor_parallel_weights(param, loaded_weight, name,\\n                                         self._column_parallel_weights,\\n                                         self._row_parallel_weights, tp_rank)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/models/gpt2.py\\n---------\\nContent:\\n# coding=utf-8\\n# Adapted from\\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/gpt2/modeling_gpt2.py\\n# Copyright 2023 The vLLM team.\\n# Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.\\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\"\"\"Inference-only GPT-2 model compatible with HuggingFace weights.\\n\\nThe input of the model is flattened to a 1D tensor of tokens. The model uses\\nInputMetadata to extract the original 2D shape of the input.\\n\"\"\"\\nfrom typing import List, Optional, Tuple\\n\\nimport torch\\nfrom torch import nn\\nfrom transformers import GPT2Config\\n\\nfrom vllm.model_executor.input_metadata import InputMetadata\\nfrom vllm.model_executor.layers.activation import get_act_fn\\nfrom vllm.model_executor.layers.attention import PagedAttention\\nfrom vllm.model_executor.layers.sampler import Sampler\\nfrom vllm.model_executor.weight_utils import (\\n    convert_pyslice_to_tensor, hf_model_weights_iterator,\\n    load_padded_tensor_parallel_vocab, load_tensor_parallel_weights)\\nfrom vllm.model_executor.parallel_utils.parallel_state import (\\n    get_tensor_model_parallel_rank, get_tensor_model_parallel_world_size)\\nfrom vllm.model_executor.parallel_utils.tensor_parallel import (\\n    VocabParallelEmbedding, ColumnParallelLinear, RowParallelLinear)\\nfrom vllm.sequence import SamplerOutput\\n\\nKVCache = Tuple[torch.Tensor, torch.Tensor]\\n\\n\\nclass GPT2Attention(nn.Module):\\n\\n    def __init__(self, config: GPT2Config):\\n        super().__init__()\\n        self.hidden_size = config.hidden_size\\n        total_num_heads = config.num_attention_heads\\n        tensor_model_parallel_world_size = (\\n            get_tensor_model_parallel_world_size())\\n        assert total_num_heads % tensor_model_parallel_world_size == 0\\n        self.num_heads = total_num_heads // tensor_model_parallel_world_size\\n        self.head_dim = self.hidden_size // total_num_heads\\n        self.scale = self.head_dim**-0.5\\n\\n        self.c_attn = ColumnParallelLinear(self.hidden_size,\\n                                           3 * self.hidden_size,\\n                                           bias=True,\\n                                           gather_output=False,\\n                                           perform_initialization=False)\\n        self.c_proj = RowParallelLinear(self.hidden_size,\\n                                        self.hidden_size,\\n                                        bias=True,\\n                                        input_is_parallel=True,\\n                                        perform_initialization=False)\\n        self.attn = PagedAttention(self.num_heads,\\n                                   self.head_dim,\\n                                   scale=self.scale)\\n\\n    def forward(\\n        self,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        qkv, _ = self.c_attn(hidden_states)\\n        q, k, v = qkv.chunk(chunks=3, dim=-1)\\n        key_cache, value_cache = kv_cache\\n        attn_output = self.attn(q, k, v, key_cache, value_cache,\\n                                input_metadata, cache_event)\\n        attn_output, _ = self.c_proj(attn_output)\\n        return attn_output\\n\\n\\nclass GPT2MLP(nn.Module):\\n\\n    def __init__(\\n        self,\\n        intermediate_size: int,\\n        config: GPT2Config,\\n    ):\\n        super().__init__()\\n        hidden_size = config.hidden_size\\n        self.c_fc = ColumnParallelLinear(hidden_size,\\n                                         intermediate_size,\\n                                         bias=True,\\n                                         gather_output=False,\\n                                         perform_initialization=False)\\n        self.c_proj = RowParallelLinear(intermediate_size,\\n                                        hidden_size,\\n                                        bias=True,\\n                                        input_is_parallel=True,\\n                                        perform_initialization=False)\\n        self.act = get_act_fn(config.activation_function)\\n\\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\\n        hidden_states, _ = self.c_fc(hidden_states)\\n        hidden_states = self.act(hidden_states)\\n        hidden_states, _ = self.c_proj(hidden_states)\\n        return hidden_states\\n\\n\\nclass GPT2Block(nn.Module):\\n\\n    def __init__(self, config: GPT2Config):\\n        super().__init__()\\n        hidden_size = config.hidden_size\\n        inner_dim = (config.n_inner if config.n_inner is not None else 4 *\\n                     hidden_size)\\n\\n        self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\\n        self.attn = GPT2Attention(config)\\n        self.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\\n        self.mlp = GPT2MLP(inner_dim, config)\\n\\n    def forward(\\n        self,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        residual = hidden_states\\n        hidden_states = self.ln_1(hidden_states)\\n        attn_output = self.attn(\\n            hidden_states=hidden_states,\\n            kv_cache=kv_cache,\\n            input_metadata=input_metadata,\\n            cache_event=cache_event,\\n        )\\n        # residual connection\\n        hidden_states = attn_output + residual\\n\\n        residual = hidden_states\\n        hidden_states = self.ln_2(hidden_states)\\n        feed_forward_hidden_states = self.mlp(hidden_states)\\n        # residual connection\\n        hidden_states = residual + feed_forward_hidden_states\\n        return hidden_states\\n\\n\\nclass GPT2Model(nn.Module):\\n\\n    def __init__(self, config: GPT2Config):\\n        super().__init__()\\n        self.config = config\\n        assert not config.add_cross_attention\\n        assert not config.scale_attn_by_inverse_layer_idx\\n        assert not config.reorder_and_upcast_attn\\n        self.embed_dim = config.hidden_size\\n\\n        # Optimization: While the vocab size of GPT-2 is 50257, we extend it\\n        # to 50304 in order to make it divisible by 64.\\n        # This improves performance since GPUs are faster if the dimension\\n        # is divisible by 64. In addition, it allows us to shard the embedding\\n        # layer across 2, 4, 8, or more GPUs.\\n        vocab_size = ((config.vocab_size + 63) // 64) * 64\\n        self.wte = VocabParallelEmbedding(vocab_size, self.embed_dim)\\n        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\\n        self.h = nn.ModuleList(\\n            [GPT2Block(config) for _ in range(config.num_hidden_layers)])\\n        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        position_ids: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> torch.Tensor:\\n        inputs_embeds = self.wte(input_ids)\\n        position_embeds = self.wpe(position_ids)\\n        hidden_states = inputs_embeds + position_embeds\\n\\n        for i in range(len(self.h)):\\n            if cache_events is None:\\n                cache_event = None\\n            else:\\n                cache_event = cache_events[i]\\n            layer = self.h[i]\\n            hidden_states = layer(hidden_states, kv_caches[i], input_metadata,\\n                                  cache_event)\\n\\n        hidden_states = self.ln_f(hidden_states)\\n        return hidden_states\\n\\n\\nclass GPT2LMHeadModel(nn.Module):\\n\\n    def __init__(self, config: GPT2Config):\\n        super().__init__()\\n        self.config = config\\n        self.transformer = GPT2Model(config)\\n        # TODO(zhuohan): create a new weight after implementing pipeline\\n        #                parallelism\\n        self.lm_head_weight = self.transformer.wte.weight\\n        self.sampler = Sampler(config.vocab_size)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        positions: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> SamplerOutput:\\n        hidden_states = self.transformer(input_ids, positions, kv_caches,\\n                                         input_metadata, cache_events)\\n        next_tokens = self.sampler(self.lm_head_weight, hidden_states,\\n                                   input_metadata)\\n        return next_tokens\\n\\n    _column_parallel_weights = [\"c_fc.weight\", \"c_fc.bias\"]\\n    _row_parallel_weights = [\"c_proj.weight\"]\\n\\n    def load_weights(self,\\n                     model_name_or_path: str,\\n                     cache_dir: Optional[str] = None,\\n                     load_format: str = \"auto\",\\n                     revision: Optional[str] = None):\\n        tensor_model_parallel_world_size = (\\n            get_tensor_model_parallel_world_size())\\n        tensor_model_parallel_rank = get_tensor_model_parallel_rank()\\n        state_dict = self.state_dict()\\n\\n        for name, loaded_weight in hf_model_weights_iterator(\\n                model_name_or_path, cache_dir, load_format, revision):\\n            if \"lm_head.weight\" in name:\\n                # GPT-2 ties the weights of the embedding layer and the final\\n                # linear layer.\\n                continue\\n            if \".attn.bias\" in name or \".attn.masked_bias\" in name:\\n                # Skip attention mask.\\n                # NOTE: \"c_attn.bias\" should not be skipped.\\n                continue\\n\\n            if not name.startswith(\"transformer.\"):\\n                name = \"transformer.\" + name\\n\\n            loaded_weight = convert_pyslice_to_tensor(loaded_weight)\\n\\n            # The HF\\'s GPT-2 implementation uses Conv1D instead of Linear.\\n            # Because of this, we need to transpose the weights.\\n            for conv1d_weight_name in [\"c_attn\", \"c_proj\", \"c_fc\"]:\\n                if conv1d_weight_name not in name:\\n                    continue\\n                if not name.endswith(\".weight\"):\\n                    continue\\n                loaded_weight = loaded_weight.t()\\n            param = state_dict[name]\\n\\n            if name == \"transformer.wte.weight\":\\n                load_padded_tensor_parallel_vocab(param, loaded_weight,\\n                                                  tensor_model_parallel_rank)\\n                continue\\n\\n            # For the fused QKV linear layer, manually shard the weights.\\n            if \"c_attn\" in name:\\n                # GPT-2\\'s fused QKV has the shape of\\n                # [3 * num_heads * head_size, hidden_size].\\n                # When tensor parallelism is used, we shard the weights along\\n                # the head dimension.\\n                total_num_heads = self.config.num_attention_heads\\n                hidden_size = self.config.hidden_size\\n                head_size = hidden_size // total_num_heads\\n                num_heads = total_num_heads // tensor_model_parallel_world_size\\n                head_start = tensor_model_parallel_rank * num_heads\\n                head_end = (tensor_model_parallel_rank + 1) * num_heads\\n\\n                if name.endswith(\".weight\"):\\n                    loaded_weight = loaded_weight.view(3, total_num_heads,\\n                                                       head_size, hidden_size)\\n                    loaded_weight = loaded_weight[:, head_start:head_end, :, :]\\n                    loaded_weight = loaded_weight.reshape(-1, hidden_size)\\n                elif name.endswith(\".bias\"):\\n                    loaded_weight = loaded_weight.view(3, total_num_heads,\\n                                                       head_size)\\n                    loaded_weight = loaded_weight[:, head_start:head_end, :]\\n                    loaded_weight = loaded_weight.reshape(-1)\\n                else:\\n                    raise ValueError(f\"Unexpected parameter name {name}\")\\n            load_tensor_parallel_weights(param, loaded_weight, name,\\n                                         self._column_parallel_weights,\\n                                         self._row_parallel_weights,\\n                                         tensor_model_parallel_rank)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/models/__init__.py\\n---------\\nContent:\\nfrom vllm.model_executor.models.aquila import AquilaForCausalLM\\nfrom vllm.model_executor.models.baichuan import (BaiChuanForCausalLM,\\n                                                 BaichuanForCausalLM)\\nfrom vllm.model_executor.models.bloom import BloomForCausalLM\\nfrom vllm.model_executor.models.falcon import FalconForCausalLM\\nfrom vllm.model_executor.models.gpt2 import GPT2LMHeadModel\\nfrom vllm.model_executor.models.gpt_bigcode import GPTBigCodeForCausalLM\\nfrom vllm.model_executor.models.gpt_j import GPTJForCausalLM\\nfrom vllm.model_executor.models.gpt_neox import GPTNeoXForCausalLM\\nfrom vllm.model_executor.models.internlm import InternLMForCausalLM\\nfrom vllm.model_executor.models.llama import LlamaForCausalLM\\nfrom vllm.model_executor.models.mpt import MPTForCausalLM\\nfrom vllm.model_executor.models.opt import OPTForCausalLM\\nfrom vllm.model_executor.models.qwen import QWenLMHeadModel\\n\\n__all__ = [\\n    \"AquilaForCausalLM\",\\n    \"BaiChuanForCausalLM\",\\n    \"BaichuanForCausalLM\",\\n    \"BloomForCausalLM\",\\n    \"FalconForCausalLM\",\\n    \"GPT2LMHeadModel\",\\n    \"GPTBigCodeForCausalLM\",\\n    \"GPTJForCausalLM\",\\n    \"GPTNeoXForCausalLM\",\\n    \"InternLMForCausalLM\",\\n    \"LlamaForCausalLM\",\\n    \"MPTForCausalLM\",\\n    \"OPTForCausalLM\",\\n    \"QWenLMHeadModel\",\\n]\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/models/opt.py\\n---------\\nContent:\\n# coding=utf-8\\n# Adapted from\\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/opt/modeling_opt.py\\n# Copyright 2023 The vLLM team.\\n# Copyright 2022 The Fairseq Authors and The HuggingFace Inc. team. All rights\\n# reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\"\"\"Inference-only OPT model compatible with HuggingFace weights.\\n\\nThe input of the model is flattened to a 1D tensor of tokens. The model uses\\nInputMetadata to extract the original 2D shape of the input.\\n\"\"\"\\nfrom typing import List, Optional, Tuple\\n\\nimport torch\\nfrom torch import nn\\nfrom transformers import OPTConfig\\n\\nfrom vllm.model_executor.input_metadata import InputMetadata\\nfrom vllm.model_executor.layers.activation import get_act_fn\\nfrom vllm.model_executor.layers.attention import PagedAttention\\nfrom vllm.model_executor.layers.sampler import Sampler\\nfrom vllm.model_executor.weight_utils import (hf_model_weights_iterator,\\n                                              load_tensor_parallel_weights)\\nfrom vllm.model_executor.parallel_utils.parallel_state import (\\n    get_tensor_model_parallel_rank, get_tensor_model_parallel_world_size)\\nfrom vllm.model_executor.parallel_utils.tensor_parallel import (\\n    VocabParallelEmbedding, ColumnParallelLinear, RowParallelLinear)\\nfrom vllm.sequence import SamplerOutput\\n\\nKVCache = Tuple[torch.Tensor, torch.Tensor]\\n\\n\\nclass OPTLearnedPositionalEmbedding(nn.Embedding):\\n\\n    def __init__(self, num_embeddings: int, embedding_dim: int):\\n        # OPT is set up so that if padding_idx is specified then offset the\\n        # embedding ids by 2 and adjust num_embeddings appropriately. Other\\n        # models don\\'t have this hack\\n        self.offset = 2\\n        super().__init__(num_embeddings + self.offset, embedding_dim)\\n\\n    def forward(self, positions: torch.Tensor):\\n        return super().forward(positions + self.offset)\\n\\n\\nclass OPTAttention(nn.Module):\\n\\n    def __init__(\\n        self,\\n        embed_dim: int,\\n        num_heads: int,\\n        bias: bool = True,\\n    ) -> None:\\n        super().__init__()\\n        self.embed_dim = embed_dim\\n        tensor_model_parallel_world_size = (\\n            get_tensor_model_parallel_world_size())\\n        total_num_heads = num_heads\\n        assert num_heads % tensor_model_parallel_world_size == 0\\n        self.num_heads = total_num_heads // tensor_model_parallel_world_size\\n        self.head_dim = embed_dim // total_num_heads\\n        self.scaling = self.head_dim**-0.5\\n\\n        self.qkv_proj = ColumnParallelLinear(embed_dim,\\n                                             3 * embed_dim,\\n                                             bias=bias,\\n                                             gather_output=False,\\n                                             perform_initialization=False)\\n        self.out_proj = RowParallelLinear(embed_dim,\\n                                          embed_dim,\\n                                          bias=bias,\\n                                          input_is_parallel=True,\\n                                          perform_initialization=False)\\n        self.attn = PagedAttention(self.num_heads,\\n                                   self.head_dim,\\n                                   scale=self.scaling)\\n\\n    def forward(\\n        self,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        qkv, _ = self.qkv_proj(hidden_states)\\n        q, k, v = qkv.chunk(chunks=3, dim=-1)\\n        key_cache, value_cache = kv_cache\\n        attn_output = self.attn(q, k, v, key_cache, value_cache,\\n                                input_metadata, cache_event)\\n        output, _ = self.out_proj(attn_output)\\n        return output\\n\\n\\nclass OPTDecoderLayer(nn.Module):\\n\\n    def __init__(self, config: OPTConfig):\\n        super().__init__()\\n        self.config = config\\n        self.embed_dim = config.hidden_size\\n        self.self_attn = OPTAttention(\\n            embed_dim=self.embed_dim,\\n            num_heads=config.num_attention_heads,\\n            bias=config.enable_bias,\\n        )\\n        self.do_layer_norm_before = config.do_layer_norm_before\\n        self.activation_fn = get_act_fn(config.activation_function)\\n\\n        self.self_attn_layer_norm = nn.LayerNorm(\\n            self.embed_dim,\\n            elementwise_affine=config.layer_norm_elementwise_affine)\\n        self.fc1 = ColumnParallelLinear(self.embed_dim,\\n                                        config.ffn_dim,\\n                                        bias=config.enable_bias,\\n                                        gather_output=False,\\n                                        perform_initialization=False)\\n        self.fc2 = RowParallelLinear(config.ffn_dim,\\n                                     self.embed_dim,\\n                                     bias=config.enable_bias,\\n                                     input_is_parallel=True,\\n                                     perform_initialization=False)\\n        self.final_layer_norm = nn.LayerNorm(\\n            self.embed_dim,\\n            elementwise_affine=config.layer_norm_elementwise_affine)\\n\\n    def forward(\\n        self,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        # Self Attention\\n        residual = hidden_states\\n        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\\n        if self.do_layer_norm_before:\\n            hidden_states = self.self_attn_layer_norm(hidden_states)\\n        hidden_states = self.self_attn(hidden_states=hidden_states,\\n                                       kv_cache=kv_cache,\\n                                       input_metadata=input_metadata,\\n                                       cache_event=cache_event)\\n        hidden_states = residual + hidden_states\\n        # 350m applies layer norm AFTER attention\\n        if not self.do_layer_norm_before:\\n            hidden_states = self.self_attn_layer_norm(hidden_states)\\n\\n        # Fully Connected\\n        residual = hidden_states\\n        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\\n        if self.do_layer_norm_before:\\n            hidden_states = self.final_layer_norm(hidden_states)\\n        hidden_states, _ = self.fc1(hidden_states)\\n        hidden_states = self.activation_fn(hidden_states)\\n        hidden_states, _ = self.fc2(hidden_states)\\n        hidden_states = residual + hidden_states\\n        # 350m applies layer norm AFTER attention\\n        if not self.do_layer_norm_before:\\n            hidden_states = self.final_layer_norm(hidden_states)\\n        return hidden_states\\n\\n\\nclass OPTDecoder(nn.Module):\\n\\n    def __init__(self, config: OPTConfig):\\n        super().__init__()\\n        self.config = config\\n        self.padding_idx = config.pad_token_id\\n        self.max_target_positions = config.max_position_embeddings\\n        self.vocab_size = config.vocab_size\\n\\n        self.embed_tokens = VocabParallelEmbedding(\\n            config.vocab_size,\\n            config.word_embed_proj_dim,\\n            perform_initialization=False)\\n        # Positional embeddings are replicated (not sharded).\\n        self.embed_positions = OPTLearnedPositionalEmbedding(\\n            config.max_position_embeddings, config.hidden_size)\\n\\n        # Project out & in will be replicated if they exist.\\n        if config.word_embed_proj_dim != config.hidden_size:\\n            self.project_out = nn.Linear(config.hidden_size,\\n                                         config.word_embed_proj_dim,\\n                                         bias=False)\\n        else:\\n            self.project_out = None\\n\\n        if config.word_embed_proj_dim != config.hidden_size:\\n            self.project_in = nn.Linear(config.word_embed_proj_dim,\\n                                        config.hidden_size,\\n                                        bias=False)\\n        else:\\n            self.project_in = None\\n\\n        # Note that the only purpose of `config._remove_final_layer_norm` is to\\n        # keep backward compatibility with checkpoints that have been fine-tuned\\n        # before transformers v4.20.1\\n        # see https://github.com/facebookresearch/metaseq/pull/164\\n        if config.do_layer_norm_before and not config._remove_final_layer_norm:\\n            self.final_layer_norm = nn.LayerNorm(\\n                config.hidden_size,\\n                elementwise_affine=config.layer_norm_elementwise_affine)\\n        else:\\n            self.final_layer_norm = None\\n\\n        self.layers = nn.ModuleList(\\n            [OPTDecoderLayer(config) for _ in range(config.num_hidden_layers)])\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        positions: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> torch.Tensor:\\n        inputs_embeds = self.embed_tokens(input_ids)\\n        pos_embeds = self.embed_positions(positions)\\n        if self.project_in is not None:\\n            inputs_embeds = self.project_in(inputs_embeds)\\n        hidden_states = inputs_embeds + pos_embeds\\n\\n        for i in range(len(self.layers)):\\n            if cache_events is None:\\n                cache_event = None\\n            else:\\n                cache_event = cache_events[i]\\n            layer = self.layers[i]\\n            hidden_states = layer(hidden_states, kv_caches[i], input_metadata,\\n                                  cache_event)\\n\\n        if self.final_layer_norm is not None:\\n            hidden_states = self.final_layer_norm(hidden_states)\\n        if self.project_out is not None:\\n            hidden_states = self.project_out(hidden_states)\\n        return hidden_states\\n\\n\\nclass OPTModel(nn.Module):\\n\\n    def __init__(self, config: OPTConfig):\\n        super().__init__()\\n        self.decoder = OPTDecoder(config)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        positions: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> torch.Tensor:\\n        return self.decoder(input_ids, positions, kv_caches, input_metadata,\\n                            cache_events)\\n\\n\\nclass OPTForCausalLM(nn.Module):\\n\\n    def __init__(self, config):\\n        super().__init__()\\n        self.config = config\\n        self.model = OPTModel(config)\\n        # TODO(zhuohan): create a new weight after implementing pipeline\\n        #                parallelism\\n        self.lm_head_weight = self.model.decoder.embed_tokens.weight\\n        self.sampler = Sampler(config.vocab_size)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        positions: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> SamplerOutput:\\n        hidden_states = self.model(input_ids, positions, kv_caches,\\n                                   input_metadata, cache_events)\\n        next_tokens = self.sampler(self.lm_head_weight, hidden_states,\\n                                   input_metadata)\\n        return next_tokens\\n\\n    _column_parallel_weights = [\\n        \"embed_tokens.weight\", \"fc1.weight\", \"fc1.bias\"\\n    ]\\n    _row_parallel_weights = [\"out_proj.weight\", \"fc2.weight\"]\\n\\n    def load_weights(self,\\n                     model_name_or_path: str,\\n                     cache_dir: Optional[str] = None,\\n                     load_format: str = \"auto\",\\n                     revision: Optional[str] = None):\\n        tensor_model_parallel_rank = get_tensor_model_parallel_rank()\\n        state_dict = self.state_dict()\\n\\n        for name, loaded_weight in hf_model_weights_iterator(\\n                model_name_or_path, cache_dir, load_format, revision):\\n            if \"lm_head.weight\" in name:\\n                continue\\n\\n            if name.startswith(\"decoder.\"):\\n                name = \"model.\" + name\\n\\n            is_attention_weight = False\\n            for stride_id, att_weight_name in enumerate(\\n                [\"q_proj\", \"k_proj\", \"v_proj\"]):\\n                if att_weight_name not in name:\\n                    continue\\n                param = state_dict[name.replace(att_weight_name, \"qkv_proj\")]\\n                shard_size = param.shape[0] // 3\\n                loaded_weight = loaded_weight[\\n                    shard_size * tensor_model_parallel_rank:shard_size *\\n                    (tensor_model_parallel_rank + 1)]\\n                param_slice = param.data[shard_size * stride_id:shard_size *\\n                                         (stride_id + 1)]\\n                assert param_slice.shape == loaded_weight.shape\\n                param_slice.copy_(loaded_weight)\\n                is_attention_weight = True\\n                break\\n            if is_attention_weight:\\n                continue\\n\\n            param = state_dict[name]\\n            load_tensor_parallel_weights(param, loaded_weight, name,\\n                                         self._column_parallel_weights,\\n                                         self._row_parallel_weights,\\n                                         tensor_model_parallel_rank)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/models/llama.py\\n---------\\nContent:\\n# coding=utf-8\\n# Adapted from\\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/llama/modeling_llama.py\\n# Copyright 2023 The vLLM team.\\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\\n#\\n# This code is based on EleutherAI\\'s GPT-NeoX library and the GPT-NeoX\\n# and OPT implementations in this library. It has been modified from its\\n# original forms to accommodate minor architectural differences compared\\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\"\"\"Inference-only LLaMA model compatible with HuggingFace weights.\\n\\nThe input of the model is flattened to a 1D tensor of tokens. The model uses\\nInputMetadata to extract the original 2D shape of the input.\\n\"\"\"\\nfrom typing import List, Optional, Tuple\\n\\nimport torch\\nfrom torch import nn\\nfrom transformers import LlamaConfig\\n\\nfrom vllm.model_executor.input_metadata import InputMetadata\\nfrom vllm.model_executor.layers.activation import SiluAndMul\\nfrom vllm.model_executor.layers.layernorm import RMSNorm\\nfrom vllm.model_executor.layers.attention import PagedAttentionWithRoPE\\nfrom vllm.model_executor.layers.sampler import Sampler\\nfrom vllm.model_executor.layers.quantized_linear import ParallelLinear\\nfrom vllm.model_executor.parallel_utils.parallel_state import (\\n    get_tensor_model_parallel_rank, get_tensor_model_parallel_world_size)\\nfrom vllm.model_executor.parallel_utils.tensor_parallel import (\\n    VocabParallelEmbedding)\\nfrom vllm.model_executor.quantization_utils import QuantizationConfig\\nfrom vllm.model_executor.weight_utils import (\\n    load_tensor_parallel_weights, load_padded_tensor_parallel_vocab,\\n    hf_model_weights_iterator)\\nfrom vllm.sequence import SamplerOutput\\n\\nKVCache = Tuple[torch.Tensor, torch.Tensor]\\n\\n\\nclass LlamaMLP(nn.Module):\\n\\n    def __init__(\\n        self,\\n        hidden_size: int,\\n        intermediate_size: int,\\n        hidden_act: str,\\n        quant_config: Optional[QuantizationConfig] = None,\\n    ) -> None:\\n        super().__init__()\\n        self.gate_up_proj = ParallelLinear.column(hidden_size,\\n                                                  2 * intermediate_size,\\n                                                  bias=False,\\n                                                  gather_output=False,\\n                                                  perform_initialization=False,\\n                                                  quant_config=quant_config)\\n        self.down_proj = ParallelLinear.row(intermediate_size,\\n                                            hidden_size,\\n                                            bias=False,\\n                                            input_is_parallel=True,\\n                                            perform_initialization=False,\\n                                            quant_config=quant_config)\\n        if hidden_act != \"silu\":\\n            raise ValueError(f\"Unsupported activation: {hidden_act}. \"\\n                             \"Only silu is supported for now.\")\\n        self.act_fn = SiluAndMul()\\n\\n    def forward(self, x):\\n        gate_up, _ = self.gate_up_proj(x)\\n        x = self.act_fn(gate_up)\\n        x, _ = self.down_proj(x)\\n        return x\\n\\n\\nclass LlamaAttention(nn.Module):\\n\\n    def __init__(\\n        self,\\n        hidden_size: int,\\n        num_heads: int,\\n        num_kv_heads: int,\\n        rope_theta: float = 10000,\\n        quant_config: Optional[QuantizationConfig] = None,\\n    ) -> None:\\n        super().__init__()\\n        self.hidden_size = hidden_size\\n        tp_size = get_tensor_model_parallel_world_size()\\n        self.total_num_heads = num_heads\\n        assert self.total_num_heads % tp_size == 0\\n        self.num_heads = self.total_num_heads // tp_size\\n        self.total_num_kv_heads = num_kv_heads\\n        assert self.total_num_kv_heads % tp_size == 0\\n        self.num_kv_heads = self.total_num_kv_heads // tp_size\\n        self.head_dim = hidden_size // self.total_num_heads\\n        self.q_size = self.num_heads * self.head_dim\\n        self.kv_size = self.num_kv_heads * self.head_dim\\n        self.scaling = self.head_dim**-0.5\\n        self.rope_theta = rope_theta\\n\\n        self.qkv_proj = ParallelLinear.column(\\n            hidden_size,\\n            (self.total_num_heads + 2 * self.total_num_kv_heads) *\\n            self.head_dim,\\n            bias=False,\\n            gather_output=False,\\n            perform_initialization=False,\\n            quant_config=quant_config,\\n        )\\n        self.o_proj = ParallelLinear.row(\\n            self.total_num_heads * self.head_dim,\\n            hidden_size,\\n            bias=False,\\n            input_is_parallel=True,\\n            perform_initialization=False,\\n            quant_config=quant_config,\\n        )\\n        self.attn = PagedAttentionWithRoPE(self.num_heads,\\n                                           self.head_dim,\\n                                           self.scaling,\\n                                           base=self.rope_theta,\\n                                           rotary_dim=self.head_dim,\\n                                           num_kv_heads=self.num_kv_heads)\\n\\n    def forward(\\n        self,\\n        positions: torch.Tensor,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        qkv, _ = self.qkv_proj(hidden_states)\\n        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\\n        k_cache, v_cache = kv_cache\\n        attn_output = self.attn(positions, q, k, v, k_cache, v_cache,\\n                                input_metadata, cache_event)\\n        output, _ = self.o_proj(attn_output)\\n        return output\\n\\n\\nclass LlamaDecoderLayer(nn.Module):\\n\\n    def __init__(\\n        self,\\n        config: LlamaConfig,\\n        quant_config: Optional[QuantizationConfig] = None,\\n    ) -> None:\\n        super().__init__()\\n        self.hidden_size = config.hidden_size\\n        # Requires transformers > 4.32.0\\n        rope_theta = getattr(config, \"rope_theta\", 10000)\\n        self.self_attn = LlamaAttention(\\n            hidden_size=self.hidden_size,\\n            num_heads=config.num_attention_heads,\\n            num_kv_heads=config.num_key_value_heads,\\n            rope_theta=rope_theta,\\n            quant_config=quant_config,\\n        )\\n        self.mlp = LlamaMLP(\\n            hidden_size=self.hidden_size,\\n            intermediate_size=config.intermediate_size,\\n            hidden_act=config.hidden_act,\\n            quant_config=quant_config,\\n        )\\n        self.input_layernorm = RMSNorm(config.hidden_size,\\n                                       eps=config.rms_norm_eps)\\n        self.post_attention_layernorm = RMSNorm(config.hidden_size,\\n                                                eps=config.rms_norm_eps)\\n\\n    def forward(\\n        self,\\n        positions: torch.Tensor,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        # Self Attention\\n        residual = hidden_states\\n        hidden_states = self.input_layernorm(hidden_states)\\n        hidden_states = self.self_attn(\\n            positions=positions,\\n            hidden_states=hidden_states,\\n            kv_cache=kv_cache,\\n            input_metadata=input_metadata,\\n            cache_event=cache_event,\\n        )\\n        hidden_states = residual + hidden_states\\n\\n        # Fully Connected\\n        residual = hidden_states\\n        hidden_states = self.post_attention_layernorm(hidden_states)\\n        hidden_states = self.mlp(hidden_states)\\n        hidden_states = residual + hidden_states\\n        return hidden_states\\n\\n\\nclass LlamaModel(nn.Module):\\n\\n    def __init__(\\n        self,\\n        config: LlamaConfig,\\n        quant_config: Optional[QuantizationConfig] = None,\\n    ) -> None:\\n        super().__init__()\\n        self.config = config\\n        self.padding_idx = config.pad_token_id\\n        self.vocab_size = config.vocab_size\\n\\n        vocab_size = ((config.vocab_size + 63) // 64) * 64\\n        self.embed_tokens = VocabParallelEmbedding(\\n            vocab_size, config.hidden_size, perform_initialization=False)\\n        self.layers = nn.ModuleList([\\n            LlamaDecoderLayer(config, quant_config)\\n            for _ in range(config.num_hidden_layers)\\n        ])\\n        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        positions: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> torch.Tensor:\\n        hidden_states = self.embed_tokens(input_ids)\\n        for i in range(len(self.layers)):\\n            if cache_events is None:\\n                cache_event = None\\n            else:\\n                cache_event = cache_events[i]\\n            layer = self.layers[i]\\n            hidden_states = layer(\\n                positions,\\n                hidden_states,\\n                kv_caches[i],\\n                input_metadata,\\n                cache_event,\\n            )\\n        hidden_states = self.norm(hidden_states)\\n        return hidden_states\\n\\n\\nclass LlamaForCausalLM(nn.Module):\\n\\n    def __init__(\\n        self,\\n        config: LlamaConfig,\\n        quant_config: Optional[QuantizationConfig] = None,\\n    ) -> None:\\n        super().__init__()\\n        self.config = config\\n        self.quant_config = quant_config\\n        self.model = LlamaModel(config, quant_config)\\n        vocab_size = ((config.vocab_size + 63) // 64) * 64\\n        # NOTE: The LM head is not quantized.\\n        self.lm_head = ParallelLinear.column(config.hidden_size,\\n                                             vocab_size,\\n                                             bias=False,\\n                                             gather_output=False,\\n                                             perform_initialization=False,\\n                                             quant_config=None)\\n        self.sampler = Sampler(config.vocab_size)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        positions: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> SamplerOutput:\\n        hidden_states = self.model(input_ids, positions, kv_caches,\\n                                   input_metadata, cache_events)\\n        next_tokens = self.sampler(self.lm_head.weight, hidden_states,\\n                                   input_metadata)\\n        return next_tokens\\n\\n    _column_parallel_layers = []\\n    _row_parallel_layers = [\"o_proj\", \"down_proj\"]\\n\\n    def load_weights(self,\\n                     model_name_or_path: str,\\n                     cache_dir: Optional[str] = None,\\n                     load_format: str = \"auto\",\\n                     revision: Optional[str] = None):\\n        if self.quant_config is None:\\n            weight_suffixes = [\"weight\"]\\n        else:\\n            weight_suffixes = self.quant_config.get_tp_tensor_names()\\n\\n        column_parallel_weights: List[str] = []\\n        for layer in self._column_parallel_layers:\\n            for suffix in weight_suffixes:\\n                column_parallel_weights.append(f\"{layer}.{suffix}\")\\n        row_parallel_weights: List[str] = []\\n        for layer in self._row_parallel_layers:\\n            for suffix in weight_suffixes:\\n                row_parallel_weights.append(f\"{layer}.{suffix}\")\\n\\n        tp_size = get_tensor_model_parallel_world_size()\\n        tensor_model_parallel_rank = get_tensor_model_parallel_rank()\\n        q_proj_shard_size = (self.config.hidden_size // tp_size)\\n        kv_proj_shard_size = (self.config.hidden_size //\\n                              self.config.num_attention_heads *\\n                              self.config.num_key_value_heads // tp_size)\\n        attention_weight_specs = [\\n            # (weight_name, shard_size, offset)\\n            (\"q_proj\", q_proj_shard_size, 0),\\n            (\"k_proj\", kv_proj_shard_size, q_proj_shard_size),\\n            (\"v_proj\", kv_proj_shard_size,\\n             q_proj_shard_size + kv_proj_shard_size),\\n        ]\\n        state_dict = self.state_dict()\\n\\n        for name, loaded_weight in hf_model_weights_iterator(\\n                model_name_or_path, cache_dir, load_format, revision):\\n            if \"rotary_emb.inv_freq\" in name:\\n                continue\\n\\n            is_packed = False\\n            is_transposed = False\\n            if self.quant_config is not None:\\n                is_packed = self.quant_config.is_packed(name)\\n                is_transposed = self.quant_config.is_transposed(name)\\n            if is_transposed:\\n                loaded_weight = loaded_weight.T\\n\\n            is_attention_weight = False\\n            for weight_name, shard_size, offset in attention_weight_specs:\\n                if weight_name not in name:\\n                    continue\\n                param = state_dict[name.replace(weight_name, \"qkv_proj\")]\\n                if is_transposed:\\n                    param = param.T\\n\\n                if is_packed:\\n                    shard_size //= self.quant_config.pack_factor\\n                    offset //= self.quant_config.pack_factor\\n\\n                loaded_weight = loaded_weight[\\n                    shard_size * tensor_model_parallel_rank:shard_size *\\n                    (tensor_model_parallel_rank + 1)]\\n                param_slice = param.data[offset:offset + shard_size]\\n                assert param_slice.shape == loaded_weight.shape\\n\\n                param_slice.copy_(loaded_weight)\\n                is_attention_weight = True\\n                break\\n            if is_attention_weight:\\n                continue\\n\\n            is_gate_up_weight = False\\n            for stride_id, weight_name in enumerate([\"gate_proj\", \"up_proj\"]):\\n                if weight_name not in name:\\n                    continue\\n                param = state_dict[name.replace(weight_name, \"gate_up_proj\")]\\n                if is_transposed:\\n                    param = param.T\\n\\n                shard_size = param.shape[0] // 2\\n                loaded_weight = loaded_weight[\\n                    shard_size * tensor_model_parallel_rank:shard_size *\\n                    (tensor_model_parallel_rank + 1)]\\n                param_slice = param.data[shard_size * stride_id:shard_size *\\n                                         (stride_id + 1)]\\n                assert param_slice.shape == loaded_weight.shape\\n                param_slice.copy_(loaded_weight)\\n                is_gate_up_weight = True\\n                break\\n            if is_gate_up_weight:\\n                continue\\n\\n            param = state_dict[name]\\n            if is_transposed:\\n                param = param.T\\n\\n            if \"embed_tokens\" in name or \"lm_head\" in name:\\n                load_padded_tensor_parallel_vocab(param, loaded_weight,\\n                                                  tensor_model_parallel_rank)\\n                continue\\n\\n            load_tensor_parallel_weights(param, loaded_weight, name,\\n                                         column_parallel_weights,\\n                                         row_parallel_weights,\\n                                         tensor_model_parallel_rank)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/models/qwen.py\\n---------\\nContent:\\n# coding=utf-8\\n# Adapted from\\n# https://huggingface.co/Qwen/Qwen-7B/blob/main/modeling_qwen.py\\n# Copyright (c) Alibaba Cloud.\\n# LICENSE: https://huggingface.co/Qwen/Qwen-7B/blob/main/LICENSE\\n\"\"\"Inference-only QWen model compatible with HuggingFace weights.\\n\\nThe input of the model is flattened to a 1D tensor of tokens. The model uses\\nInputMetadata to extract the original 2D shape of the input.\\n\"\"\"\\nfrom typing import List, Optional, Tuple\\n\\nimport torch\\nfrom torch import nn\\n\\nfrom vllm.model_executor.input_metadata import InputMetadata\\nfrom vllm.model_executor.layers.activation import SiluAndMul\\nfrom vllm.model_executor.layers.layernorm import RMSNorm\\nfrom vllm.model_executor.layers.attention import PagedAttentionWithRoPE\\nfrom vllm.model_executor.layers.sampler import Sampler\\nfrom vllm.model_executor.weight_utils import (\\n    convert_pyslice_to_tensor,\\n    hf_model_weights_iterator,\\n    load_padded_tensor_parallel_vocab,\\n    load_tensor_parallel_weights,\\n)\\nfrom vllm.model_executor.parallel_utils.parallel_state import (\\n    get_tensor_model_parallel_rank,\\n    get_tensor_model_parallel_world_size,\\n)\\nfrom vllm.model_executor.parallel_utils.tensor_parallel import (\\n    VocabParallelEmbedding,\\n    ColumnParallelLinear,\\n    RowParallelLinear,\\n)\\nfrom vllm.sequence import SamplerOutput\\nfrom vllm.transformers_utils.configs.qwen import QWenConfig\\n\\nKVCache = Tuple[torch.Tensor, torch.Tensor]\\n\\n\\nclass QWenMLP(nn.Module):\\n\\n    def __init__(\\n        self,\\n        hidden_size: int,\\n        intermediate_size: int,\\n        hidden_act: str = \"silu\",\\n    ):\\n        super().__init__()\\n        self.gate_up_proj = ColumnParallelLinear(\\n            hidden_size,\\n            2 * intermediate_size,\\n            bias=False,\\n            gather_output=False,\\n            perform_initialization=False,\\n        )\\n        self.c_proj = RowParallelLinear(\\n            intermediate_size,\\n            hidden_size,\\n            bias=False,\\n            input_is_parallel=True,\\n            perform_initialization=False,\\n        )\\n        if hidden_act != \"silu\":\\n            raise ValueError(f\"Unsupported activation: {hidden_act}. \"\\n                             \"Only silu is supported for now.\")\\n        self.act_fn = SiluAndMul()\\n\\n    def forward(self, x):\\n        gate_up, _ = self.gate_up_proj(x)\\n        x = self.act_fn(gate_up)\\n        x, _ = self.c_proj(x)\\n        return x\\n\\n\\nclass QWenAttention(nn.Module):\\n\\n    def __init__(self, hidden_size: int, num_heads: int,\\n                 max_position_embeddings: int):\\n        super().__init__()\\n        self.hidden_size = hidden_size\\n        tensor_model_parallel_world_size = get_tensor_model_parallel_world_size(\\n        )\\n        self.total_num_heads = num_heads\\n        assert self.total_num_heads % tensor_model_parallel_world_size == 0\\n        self.num_heads = (self.total_num_heads //\\n                          tensor_model_parallel_world_size)\\n        self.head_dim = hidden_size // self.total_num_heads\\n\\n        # pylint: disable=invalid-name\\n        self.c_attn = ColumnParallelLinear(\\n            hidden_size,\\n            3 * hidden_size,\\n            bias=True,\\n            gather_output=False,\\n            perform_initialization=False,\\n        )\\n        self.c_proj = RowParallelLinear(\\n            self.total_num_heads * self.head_dim,\\n            hidden_size,\\n            bias=False,\\n            input_is_parallel=True,\\n            perform_initialization=False,\\n        )\\n        self.scaling = self.head_dim**-0.5\\n        self.attn = PagedAttentionWithRoPE(\\n            self.num_heads,\\n            self.head_dim,\\n            self.scaling,\\n            rotary_dim=self.head_dim,\\n            max_position=max_position_embeddings,\\n        )\\n\\n    def forward(\\n        self,\\n        positions: torch.Tensor,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        qkv, _ = self.c_attn(hidden_states)\\n        q, k, v = qkv.chunk(chunks=3, dim=-1)\\n\\n        k_cache, v_cache = kv_cache\\n        attn_output = self.attn(positions, q, k, v, k_cache, v_cache,\\n                                input_metadata, cache_event)\\n\\n        output, _ = self.c_proj(attn_output)\\n        return output\\n\\n\\nclass QWenBlock(nn.Module):\\n\\n    def __init__(self, config: QWenConfig):\\n        super().__init__()\\n        self.ln_1 = RMSNorm(config.n_embd, eps=config.layer_norm_epsilon)\\n\\n        self.attn = QWenAttention(config.n_embd, config.num_attention_heads,\\n                                  config.max_position_embeddings)\\n\\n        self.ln_2 = RMSNorm(config.n_embd, eps=config.layer_norm_epsilon)\\n\\n        self.mlp = QWenMLP(config.n_embd, config.ffn_hidden_size // 2)\\n\\n    def forward(\\n        self,\\n        positions: torch.Tensor,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        # Self Attention\\n        residual = hidden_states\\n        hidden_states = self.ln_1(hidden_states)\\n        hidden_states = self.attn(\\n            positions=positions,\\n            hidden_states=hidden_states,\\n            kv_cache=kv_cache,\\n            input_metadata=input_metadata,\\n            cache_event=cache_event,\\n        )\\n        hidden_states = residual + hidden_states\\n\\n        # Fully Connected\\n        residual = hidden_states\\n        hidden_states = self.ln_2(hidden_states)\\n        hidden_states = self.mlp(hidden_states)\\n        hidden_states = residual + hidden_states\\n        return hidden_states\\n\\n\\nclass QWenModel(nn.Module):\\n\\n    def __init__(self, config: QWenConfig):\\n        super().__init__()\\n        self.config = config\\n        self.vocab_size = config.vocab_size\\n\\n        vocab_size = ((config.vocab_size + 63) // 64) * 64\\n        self.wte = VocabParallelEmbedding(vocab_size,\\n                                          config.n_embd,\\n                                          perform_initialization=False)\\n        self.h = nn.ModuleList(\\n            [QWenBlock(config) for _ in range(config.num_hidden_layers)])\\n        self.ln_f = RMSNorm(config.n_embd, eps=config.layer_norm_epsilon)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        positions: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> torch.Tensor:\\n        hidden_states = self.wte(input_ids)\\n        for i in range(len(self.h)):\\n            if cache_events is None:\\n                cache_event = None\\n            else:\\n                cache_event = cache_events[i]\\n            layer = self.h[i]\\n            hidden_states = layer(\\n                positions,\\n                hidden_states,\\n                kv_caches[i],\\n                input_metadata,\\n                cache_event,\\n            )\\n        hidden_states = self.ln_f(hidden_states)\\n        return hidden_states\\n\\n\\nclass QWenLMHeadModel(nn.Module):\\n\\n    def __init__(self, config: QWenConfig):\\n        super().__init__()\\n        self.config = config\\n        self.transformer = QWenModel(config)\\n        vocab_size = ((config.vocab_size + 63) // 64) * 64\\n        self.lm_head = ColumnParallelLinear(\\n            config.n_embd,\\n            vocab_size,\\n            bias=False,\\n            gather_output=False,\\n            perform_initialization=False,\\n        )\\n        self.sampler = Sampler(config.vocab_size)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        positions: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> SamplerOutput:\\n        hidden_states = self.transformer(input_ids, positions, kv_caches,\\n                                         input_metadata, cache_events)\\n        next_tokens = self.sampler(self.lm_head.weight, hidden_states,\\n                                   input_metadata)\\n        return next_tokens\\n\\n    _column_parallel_weights = []\\n    _row_parallel_weights = [\"c_proj.weight\"]\\n\\n    def load_weights(\\n        self,\\n        model_name_or_path: str,\\n        cache_dir: Optional[str] = None,\\n        load_format: str = \"auto\",\\n        revision: Optional[str] = None,\\n    ):\\n        tp_world_size = get_tensor_model_parallel_world_size()\\n        tp_rank = get_tensor_model_parallel_rank()\\n        state_dict = self.state_dict()\\n\\n        for name, loaded_weight in hf_model_weights_iterator(\\n                model_name_or_path, cache_dir, load_format, revision):\\n            if \"rotary_emb.inv_freq\" in name:\\n                continue\\n\\n            loaded_weight = convert_pyslice_to_tensor(loaded_weight)\\n\\n            if \"c_attn\" in name:\\n                total_num_heads = self.config.num_attention_heads\\n                hidden_size = self.config.hidden_size\\n                head_size = hidden_size // total_num_heads\\n                num_heads = total_num_heads // tp_world_size\\n                head_start = tp_rank * num_heads\\n                head_end = (tp_rank + 1) * num_heads\\n\\n                if \"weight\" in name:\\n                    loaded_weight = loaded_weight.view(3, total_num_heads,\\n                                                       head_size, hidden_size)\\n                    loaded_weight = loaded_weight[:, head_start:head_end, :, :]\\n                    loaded_weight = loaded_weight.reshape(-1, hidden_size)\\n                elif \"bias\" in name:\\n                    loaded_weight = loaded_weight.view(3, total_num_heads,\\n                                                       head_size)\\n                    loaded_weight = loaded_weight[:, head_start:head_end, :]\\n                    loaded_weight = loaded_weight.reshape(-1)\\n\\n            is_gate_up_weight = False\\n            for stride_id, weight_name in enumerate([\"w2\", \"w1\"]):\\n                if weight_name not in name:\\n                    continue\\n                param = state_dict[name.replace(weight_name, \"gate_up_proj\")]\\n                shard_size = param.shape[0] // 2\\n                loaded_weight = loaded_weight[shard_size * tp_rank:shard_size *\\n                                              (tp_rank + 1)]\\n                param_slice = param.data[shard_size * stride_id:shard_size *\\n                                         (stride_id + 1)]\\n                assert param_slice.shape == loaded_weight.shape\\n                param_slice.copy_(loaded_weight)\\n                is_gate_up_weight = True\\n                break\\n            if is_gate_up_weight:\\n                continue\\n\\n            param = state_dict[name]\\n\\n            if \"wte\" in name or \"lm_head\" in name:\\n                load_padded_tensor_parallel_vocab(param, loaded_weight,\\n                                                  tp_rank)\\n                continue\\n\\n            load_tensor_parallel_weights(\\n                param,\\n                loaded_weight,\\n                name,\\n                self._column_parallel_weights,\\n                self._row_parallel_weights,\\n                tp_rank,\\n            )\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/models/mpt.py\\n---------\\nContent:\\n# coding=utf-8\\n# Adapted from https://huggingface.co/mosaicml/mpt-7b/tree/main\\nimport math\\nfrom typing import List, Optional, Tuple\\n\\nimport torch\\nimport torch.nn as nn\\n\\nfrom vllm.model_executor.input_metadata import InputMetadata\\nfrom vllm.model_executor.layers.activation import get_act_fn\\nfrom vllm.model_executor.layers.attention import PagedAttentionWithALiBi\\nfrom vllm.model_executor.layers.sampler import Sampler\\nfrom vllm.model_executor.weight_utils import (convert_pyslice_to_tensor,\\n                                              hf_model_weights_iterator,\\n                                              load_tensor_parallel_weights)\\nfrom vllm.model_executor.parallel_utils.parallel_state import (\\n    get_tensor_model_parallel_rank, get_tensor_model_parallel_world_size)\\nfrom vllm.model_executor.parallel_utils.tensor_parallel import (\\n    VocabParallelEmbedding, ColumnParallelLinear, RowParallelLinear)\\nfrom vllm.sequence import SamplerOutput\\nfrom vllm.transformers_utils.configs.mpt import MPTConfig\\n\\nKVCache = Tuple[torch.Tensor, torch.Tensor]\\n\\n\\ndef _get_alibi_slopes(\\n    total_num_heads: int,\\n    alibi_bias_max: int,\\n) -> torch.Tensor:\\n    next_power_of_2 = 2**math.ceil(math.log2(total_num_heads))\\n    m = torch.arange(1, next_power_of_2 + 1, dtype=torch.float32)\\n    m = m.mul(alibi_bias_max / next_power_of_2)\\n    slopes = 1.0 / torch.pow(2, m)\\n    if next_power_of_2 != total_num_heads:\\n        slopes = torch.concat([slopes[1::2], slopes[::2]])[:total_num_heads]\\n    return slopes\\n\\n\\nclass MPTAttention(nn.Module):\\n\\n    def __init__(self, config: MPTConfig):\\n        super().__init__()\\n        self.d_model = config.d_model\\n        self.total_num_heads = config.n_heads\\n        self.clip_qkv = config.attn_config[\"clip_qkv\"]\\n        self.qk_ln = config.attn_config[\"qk_ln\"]\\n        self.alibi_bias_max = config.attn_config[\"alibi_bias_max\"]\\n        assert not config.attn_config[\"prefix_lm\"]\\n        assert config.attn_config[\"alibi\"]\\n\\n        self.qkv_proj = ColumnParallelLinear(\\n            self.d_model,\\n            3 * self.d_model,\\n            bias=not config.no_bias,\\n            gather_output=False,\\n            perform_initialization=False,\\n        )\\n        if self.qk_ln:\\n            self.q_ln = nn.LayerNorm(self.d_model)\\n            self.k_ln = nn.LayerNorm(self.d_model)\\n        self.out_proj = RowParallelLinear(\\n            self.d_model,\\n            self.d_model,\\n            bias=not config.no_bias,\\n            input_is_parallel=True,\\n            perform_initialization=False,\\n        )\\n\\n        tp_world_size = get_tensor_model_parallel_world_size()\\n        assert self.total_num_heads % tp_world_size == 0\\n        self.num_heads = self.total_num_heads // tp_world_size\\n\\n        # Create the alibi slopes and slice them.\\n        tp_rank = get_tensor_model_parallel_rank()\\n        head_start = tp_rank * self.num_heads\\n        head_end = (tp_rank + 1) * self.num_heads\\n        alibi_slopes = _get_alibi_slopes(self.total_num_heads,\\n                                         self.alibi_bias_max)\\n        alibi_slopes = alibi_slopes[head_start:head_end].tolist()\\n\\n        self.head_dim = self.d_model // self.total_num_heads\\n        scaling = self.head_dim**-0.5\\n        self.attn = PagedAttentionWithALiBi(self.num_heads, self.head_dim,\\n                                            scaling, alibi_slopes)\\n\\n    def forward(\\n        self,\\n        position_ids: torch.Tensor,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        del position_ids  # unused.\\n        qkv, _ = self.qkv_proj(hidden_states)\\n        if self.clip_qkv is not None:\\n            qkv.clamp_(min=-self.clip_qkv, max=self.clip_qkv)\\n        q, k, v = qkv.chunk(chunks=3, dim=-1)\\n        if self.qk_ln:\\n            q = self.q_ln(q)\\n            k = self.k_ln(k)\\n        k_cache, v_cache = kv_cache\\n        attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata,\\n                                cache_event)\\n        output, _ = self.out_proj(attn_output)\\n        return output\\n\\n\\nclass MPTMLP(nn.Module):\\n\\n    def __init__(self, config: MPTConfig):\\n        super().__init__()\\n        hidden_size = config.d_model\\n        expansion_ratio = config.expansion_ratio\\n        intermediate_size = expansion_ratio * hidden_size\\n        self.up_proj = ColumnParallelLinear(hidden_size,\\n                                            intermediate_size,\\n                                            bias=not config.no_bias,\\n                                            gather_output=False,\\n                                            perform_initialization=False)\\n        self.act = get_act_fn(\"gelu\")\\n        self.down_proj = RowParallelLinear(intermediate_size,\\n                                           hidden_size,\\n                                           bias=not config.no_bias,\\n                                           input_is_parallel=True,\\n                                           perform_initialization=False)\\n\\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\\n        x, _ = self.up_proj(x)\\n        x = self.act(x)\\n        x, _ = self.down_proj(x)\\n        return x\\n\\n\\nclass MPTBlock(nn.Module):\\n\\n    def __init__(self, config: MPTConfig):\\n        super().__init__()\\n        hidden_size = config.d_model\\n        self.norm_1 = nn.LayerNorm(hidden_size)\\n        self.attn = MPTAttention(config)\\n        self.norm_2 = nn.LayerNorm(hidden_size)\\n        self.ffn = MPTMLP(config)\\n\\n    def forward(\\n        self,\\n        position_ids: torch.Tensor,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        x = self.norm_1(hidden_states)\\n        x = self.attn(\\n            position_ids=position_ids,\\n            hidden_states=x,\\n            kv_cache=kv_cache,\\n            input_metadata=input_metadata,\\n            cache_event=cache_event,\\n        )\\n        hidden_states = hidden_states + x\\n        x = self.norm_2(hidden_states)\\n        x = self.ffn(x)\\n        hidden_states = hidden_states + x\\n        return hidden_states\\n\\n\\nclass MPTModel(nn.Module):\\n\\n    def __init__(self, config: MPTConfig):\\n        super().__init__()\\n        assert config.embedding_fraction == 1.0\\n        assert config.norm_type == \"low_precision_layernorm\"\\n\\n        self.wte = VocabParallelEmbedding(config.vocab_size,\\n                                          config.d_model,\\n                                          perform_initialization=False)\\n        self.blocks = nn.ModuleList(\\n            [MPTBlock(config) for _ in range(config.n_layers)])\\n        self.norm_f = nn.LayerNorm(config.d_model)\\n        if config.no_bias:\\n            for module in self.modules():\\n                if hasattr(module, \"bias\"):\\n                    if isinstance(module.bias, nn.Parameter):\\n                        # Remove the bias term in Linear and LayerNorm.\\n                        module.register_parameter(\"bias\", None)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        position_ids: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> torch.Tensor:\\n        hidden_states = self.wte(input_ids)\\n        for i in range(len(self.blocks)):\\n            if cache_events is None:\\n                cache_event = None\\n            else:\\n                cache_event = cache_events[i]\\n            block = self.blocks[i]\\n            hidden_states = block(\\n                position_ids,\\n                hidden_states,\\n                kv_caches[i],\\n                input_metadata,\\n                cache_event,\\n            )\\n        hidden_states = self.norm_f(hidden_states)\\n        return hidden_states\\n\\n\\nclass MPTForCausalLM(nn.Module):\\n\\n    def __init__(self, config: MPTConfig):\\n        super().__init__()\\n        self.config = config\\n        assert config.tie_word_embeddings\\n\\n        self.transformer = MPTModel(config)\\n        # TODO(zhuohan): create a new weight after implementing pipeline\\n        #                parallelism\\n        self.lm_head_weight = self.transformer.wte.weight\\n        self.sampler = Sampler(config.vocab_size)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        positions: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> SamplerOutput:\\n        hidden_states = self.transformer(input_ids, positions, kv_caches,\\n                                         input_metadata, cache_events)\\n        next_tokens = self.sampler(self.lm_head_weight, hidden_states,\\n                                   input_metadata)\\n        return next_tokens\\n\\n    _column_parallel_weights = [\"wte.weight\", \"up_proj.weight\", \"up_proj.bias\"]\\n    _row_parallel_weights = [\"out_proj.weight\", \"down_proj.weight\"]\\n\\n    def load_weights(self,\\n                     model_name_or_path: str,\\n                     cache_dir: Optional[str] = None,\\n                     load_format: str = \"auto\",\\n                     revision: Optional[str] = None):\\n        tp_world_size = get_tensor_model_parallel_world_size()\\n        tp_rank = get_tensor_model_parallel_rank()\\n        state_dict = self.state_dict()\\n        for name, loaded_weight in hf_model_weights_iterator(\\n                model_name_or_path, cache_dir, load_format, revision):\\n            if \"Wqkv\" in name:\\n                # NOTE(woosuk): MPT\\'s fused QKV has the shape of\\n                # [3 * num_heads * head_size, hidden_size].\\n                # When tensor model parallelism is used, we need to shard\\n                # the weight along the hidden dimension.\\n                total_num_heads = self.config.num_attention_heads\\n                hidden_size = self.config.hidden_size\\n                head_size = hidden_size // total_num_heads\\n                num_heads = total_num_heads // tp_world_size\\n                head_start = tp_rank * num_heads\\n                head_end = (tp_rank + 1) * num_heads\\n                loaded_weight = convert_pyslice_to_tensor(loaded_weight)\\n                if name.endswith(\".weight\"):\\n                    loaded_weight = loaded_weight.view(3, total_num_heads,\\n                                                       head_size, hidden_size)\\n                    loaded_weight = loaded_weight[:, head_start:head_end, :, :]\\n                    loaded_weight = loaded_weight.reshape(-1, hidden_size)\\n                elif name.endswith(\".bias\"):\\n                    loaded_weight = loaded_weight.view(3, total_num_heads,\\n                                                       head_size)\\n                    loaded_weight = loaded_weight[:, head_start:head_end, :]\\n                    loaded_weight = loaded_weight.reshape(-1)\\n                else:\\n                    raise ValueError(f\"Unexpected parameter name {name}\")\\n                name = name.replace(\"Wqkv\", \"qkv_proj\")\\n            param = state_dict[name]\\n            load_tensor_parallel_weights(param, loaded_weight, name,\\n                                         self._column_parallel_weights,\\n                                         self._row_parallel_weights, tp_rank)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/models/baichuan.py\\n---------\\nContent:\\n# coding=utf-8\\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\\n#\\n# This code is based on EleutherAI\\'s GPT-NeoX library and the GPT-NeoX\\n# and OPT implementations in this library. It has been modified from its\\n# original forms to accommodate minor architectural differences compared\\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\"\"\"Inference-only BaiChuan model compatible with HuggingFace weights.\\n\\nThe input of the model is flattened to a 1D tensor of tokens. The model uses\\nInputMetadata to extract the original 2D shape of the input.\\n\"\"\"\\nimport math\\nfrom typing import List, Optional, Tuple\\n\\nimport torch\\nfrom torch import nn\\n\\nfrom vllm.model_executor.input_metadata import InputMetadata\\nfrom vllm.model_executor.layers.activation import SiluAndMul\\nfrom vllm.model_executor.layers.layernorm import RMSNorm\\nfrom vllm.model_executor.layers.attention import (PagedAttentionWithRoPE,\\n                                                  PagedAttentionWithALiBi)\\nfrom vllm.model_executor.layers.sampler import Sampler\\nfrom vllm.model_executor.weight_utils import (\\n    convert_pyslice_to_tensor, hf_model_weights_iterator,\\n    load_padded_tensor_parallel_vocab, load_tensor_parallel_weights)\\nfrom vllm.model_executor.parallel_utils.parallel_state import (\\n    get_tensor_model_parallel_rank, get_tensor_model_parallel_world_size)\\nfrom vllm.model_executor.parallel_utils.tensor_parallel import (\\n    VocabParallelEmbedding, ColumnParallelLinear, RowParallelLinear)\\nfrom vllm.sequence import SamplerOutput\\nfrom vllm.transformers_utils.configs.baichuan import BaiChuanConfig\\n\\nKVCache = Tuple[torch.Tensor, torch.Tensor]\\n\\n\\ndef _get_alibi_slopes(total_num_heads: int) -> torch.Tensor:\\n    closest_power_of_2 = 2**math.floor(math.log2(total_num_heads))\\n    base = torch.tensor(\\n        2**(-(2**-(math.log2(closest_power_of_2) - 3))),\\n        dtype=torch.float32,\\n    )\\n    powers = torch.arange(1, 1 + closest_power_of_2, dtype=torch.int32)\\n    slopes = torch.pow(base, powers)\\n\\n    if closest_power_of_2 != total_num_heads:\\n        extra_base = torch.tensor(\\n            2**(-(2**-(math.log2(2 * closest_power_of_2) - 3))),\\n            dtype=torch.float32,\\n        )\\n        num_remaining_heads = min(closest_power_of_2,\\n                                  total_num_heads - closest_power_of_2)\\n        extra_powers = torch.arange(start=1,\\n                                    end=1 + 2 * num_remaining_heads,\\n                                    step=2,\\n                                    dtype=torch.int32)\\n        slopes = torch.cat(\\n            [slopes, torch.pow(extra_base, extra_powers)], dim=0)\\n    return slopes\\n\\n\\nclass BaiChuanMLP(nn.Module):\\n\\n    def __init__(\\n        self,\\n        hidden_size: int,\\n        intermediate_size: int,\\n        hidden_act: str,\\n    ):\\n        super().__init__()\\n        self.gate_up_proj = ColumnParallelLinear(hidden_size,\\n                                                 2 * intermediate_size,\\n                                                 bias=False,\\n                                                 gather_output=False,\\n                                                 perform_initialization=False)\\n        self.down_proj = RowParallelLinear(intermediate_size,\\n                                           hidden_size,\\n                                           bias=False,\\n                                           input_is_parallel=True,\\n                                           perform_initialization=False)\\n        if hidden_act != \"silu\":\\n            raise ValueError(f\"Unsupported activation: {hidden_act}. \"\\n                             \"Only silu is supported for now.\")\\n        self.act_fn = SiluAndMul()\\n\\n    def forward(self, x):\\n        gate_up, _ = self.gate_up_proj(x)\\n        x = self.act_fn(gate_up)\\n        x, _ = self.down_proj(x)\\n        return x\\n\\n\\nclass BaiChuanAttention(nn.Module):\\n    \"\"\"Multi-headed attention from \\'Attention Is All You Need\\' paper\"\"\"\\n\\n    def __init__(\\n        self,\\n        hidden_size: int,\\n        num_heads: int,\\n        position_embedding: str,\\n    ):\\n        super().__init__()\\n        self.hidden_size = hidden_size\\n        tensor_model_parallel_world_size = get_tensor_model_parallel_world_size(\\n        )\\n        self.total_num_heads = num_heads\\n        assert self.total_num_heads % tensor_model_parallel_world_size == 0\\n        self.num_heads = (self.total_num_heads //\\n                          tensor_model_parallel_world_size)\\n        self.head_dim = hidden_size // self.total_num_heads\\n        self.postion_embedding = position_embedding\\n\\n        # pylint: disable=invalid-name\\n        self.W_pack = ColumnParallelLinear(\\n            hidden_size,\\n            3 * hidden_size,\\n            bias=False,\\n            gather_output=False,\\n            perform_initialization=False,\\n        )\\n        self.o_proj = RowParallelLinear(\\n            self.total_num_heads * self.head_dim,\\n            hidden_size,\\n            bias=False,\\n            input_is_parallel=True,\\n            perform_initialization=False,\\n        )\\n        # Create the alibi slopes and slice them.\\n        if self.postion_embedding == \"ALIBI\":\\n            tp_rank = get_tensor_model_parallel_rank()\\n            head_start = tp_rank * self.num_heads\\n            head_end = (tp_rank + 1) * self.num_heads\\n            alibi_slopes = _get_alibi_slopes(self.total_num_heads)\\n            alibi_slopes = alibi_slopes[head_start:head_end].tolist()\\n\\n            scaling = self.head_dim**-0.5\\n            self.attn = PagedAttentionWithALiBi(self.num_heads, self.head_dim,\\n                                                scaling, alibi_slopes)\\n        else:\\n            self.scaling = self.head_dim**-0.5\\n            self.attn = PagedAttentionWithRoPE(self.num_heads,\\n                                               self.head_dim,\\n                                               self.scaling,\\n                                               rotary_dim=self.head_dim)\\n\\n    def forward(\\n        self,\\n        positions: torch.Tensor,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        qkv, _ = self.W_pack(hidden_states)\\n        q, k, v = qkv.chunk(chunks=3, dim=-1)\\n        k_cache, v_cache = kv_cache\\n        if self.postion_embedding == \"ALIBI\":\\n            attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata,\\n                                    cache_event)\\n        else:\\n            attn_output = self.attn(positions, q, k, v, k_cache, v_cache,\\n                                    input_metadata, cache_event)\\n\\n        output, _ = self.o_proj(attn_output)\\n        return output\\n\\n\\nclass BaiChuanDecoderLayer(nn.Module):\\n\\n    def __init__(self, config: BaiChuanConfig, position_embedding: str):\\n        super().__init__()\\n        self.hidden_size = config.hidden_size\\n        self.self_attn = BaiChuanAttention(\\n            hidden_size=self.hidden_size,\\n            num_heads=config.num_attention_heads,\\n            position_embedding=position_embedding,\\n        )\\n        self.mlp = BaiChuanMLP(\\n            hidden_size=self.hidden_size,\\n            intermediate_size=config.intermediate_size,\\n            hidden_act=config.hidden_act,\\n        )\\n        self.input_layernorm = RMSNorm(config.hidden_size,\\n                                       eps=config.rms_norm_eps)\\n        self.post_attention_layernorm = RMSNorm(config.hidden_size,\\n                                                eps=config.rms_norm_eps)\\n\\n    def forward(\\n        self,\\n        positions: torch.Tensor,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        # Self Attention\\n        residual = hidden_states\\n        hidden_states = self.input_layernorm(hidden_states)\\n        hidden_states = self.self_attn(\\n            positions=positions,\\n            hidden_states=hidden_states,\\n            kv_cache=kv_cache,\\n            input_metadata=input_metadata,\\n            cache_event=cache_event,\\n        )\\n        hidden_states = residual + hidden_states\\n\\n        # Fully Connected\\n        residual = hidden_states\\n        hidden_states = self.post_attention_layernorm(hidden_states)\\n        hidden_states = self.mlp(hidden_states)\\n        hidden_states = residual + hidden_states\\n        return hidden_states\\n\\n\\nclass BaiChuanModel(nn.Module):\\n\\n    def __init__(self, config: BaiChuanConfig, position_embedding: str):\\n        super().__init__()\\n        self.config = config\\n        self.padding_idx = config.pad_token_id\\n        self.vocab_size = config.vocab_size\\n\\n        self.embed_tokens = VocabParallelEmbedding(\\n            config.vocab_size,\\n            config.hidden_size,\\n            perform_initialization=False)\\n        self.layers = nn.ModuleList([\\n            BaiChuanDecoderLayer(config, position_embedding)\\n            for _ in range(config.num_hidden_layers)\\n        ])\\n        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        positions: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> torch.Tensor:\\n        hidden_states = self.embed_tokens(input_ids)\\n        for i in range(len(self.layers)):\\n            if cache_events is None:\\n                cache_event = None\\n            else:\\n                cache_event = cache_events[i]\\n            layer = self.layers[i]\\n            hidden_states = layer(\\n                positions,\\n                hidden_states,\\n                kv_caches[i],\\n                input_metadata,\\n                cache_event,\\n            )\\n        hidden_states = self.norm(hidden_states)\\n        return hidden_states\\n\\n\\nclass BaiChuanBaseForCausalLM(nn.Module):\\n\\n    def __init__(self, config, position_embedding: str):\\n        super().__init__()\\n        self.config = config\\n        self.model = BaiChuanModel(config, position_embedding)\\n        self.lm_head = ColumnParallelLinear(config.hidden_size,\\n                                            config.vocab_size,\\n                                            bias=False,\\n                                            gather_output=False,\\n                                            perform_initialization=False)\\n        self.sampler = Sampler(config.vocab_size)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        positions: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> SamplerOutput:\\n        hidden_states = self.model(input_ids, positions, kv_caches,\\n                                   input_metadata, cache_events)\\n        next_tokens = self.sampler(self.lm_head.weight, hidden_states,\\n                                   input_metadata)\\n        return next_tokens\\n\\n    _column_parallel_weights = []\\n    _row_parallel_weights = [\"o_proj.weight\", \"down_proj.weight\"]\\n\\n    def load_weights(self,\\n                     model_name_or_path: str,\\n                     cache_dir: Optional[str] = None,\\n                     load_format: str = \"auto\",\\n                     revision: Optional[str] = None):\\n        tp_world_size = get_tensor_model_parallel_world_size()\\n        tp_rank = get_tensor_model_parallel_rank()\\n        state_dict = self.state_dict()\\n\\n        for name, loaded_weight in hf_model_weights_iterator(\\n                model_name_or_path, cache_dir, load_format, revision):\\n            if \"rotary_emb.inv_freq\" in name:\\n                continue\\n\\n            loaded_weight = convert_pyslice_to_tensor(loaded_weight)\\n\\n            if \"W_pack\" in name:\\n                total_num_heads = self.config.num_attention_heads\\n                hidden_size = self.config.hidden_size\\n                head_size = hidden_size // total_num_heads\\n                num_heads = total_num_heads // tp_world_size\\n                head_start = tp_rank * num_heads\\n                head_end = (tp_rank + 1) * num_heads\\n\\n                loaded_weight = loaded_weight.view(3, total_num_heads,\\n                                                   head_size, hidden_size)\\n                loaded_weight = loaded_weight[:, head_start:head_end, :, :]\\n                loaded_weight = loaded_weight.reshape(-1, hidden_size)\\n\\n            is_gate_up_weight = False\\n            for stride_id, weight_name in enumerate([\"gate_proj\", \"up_proj\"]):\\n                if weight_name not in name:\\n                    continue\\n                param = state_dict[name.replace(weight_name, \"gate_up_proj\")]\\n                shard_size = param.shape[0] // 2\\n                loaded_weight = loaded_weight[shard_size * tp_rank:shard_size *\\n                                              (tp_rank + 1)]\\n                param_slice = param.data[shard_size * stride_id:shard_size *\\n                                         (stride_id + 1)]\\n                assert param_slice.shape == loaded_weight.shape\\n                param_slice.copy_(loaded_weight)\\n                is_gate_up_weight = True\\n                break\\n            if is_gate_up_weight:\\n                continue\\n\\n            param = state_dict[name]\\n\\n            if \"embed_tokens\" in name or \"lm_head\" in name:\\n                load_padded_tensor_parallel_vocab(param, loaded_weight,\\n                                                  tp_rank)\\n                continue\\n\\n            load_tensor_parallel_weights(\\n                param,\\n                loaded_weight,\\n                name,\\n                self._column_parallel_weights,\\n                self._row_parallel_weights,\\n                tp_rank,\\n            )\\n\\n\\nclass BaichuanForCausalLM(BaiChuanBaseForCausalLM):  # baichuan 13b\\n\\n    def __init__(self, config):\\n        super().__init__(config, \"ALIBI\")\\n\\n\\nclass BaiChuanForCausalLM(BaiChuanBaseForCausalLM):  # baichuan 7b\\n\\n    def __init__(self, config):\\n        super().__init__(config, \"ROPE\")\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/models/gpt_bigcode.py\\n---------\\nContent:\\n# coding=utf-8\\n# Adapted from\\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/gpt2/modeling_gpt2.py\\n# Copyright 2023 The vLLM team.\\n# Copyright 2023 CTranslate2, and Michael Feil\\n# Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.\\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\"\"\"Inference-only GPTBigCode model compatible with HuggingFace weights.\\n\\nThe input of the model is flattened to a 1D tensor of tokens. The model uses\\nInputMetadata to extract the original 2D shape of the input.\\n\"\"\"\\nfrom typing import List, Optional, Tuple\\n\\nimport torch\\nfrom torch import nn\\nfrom transformers import GPTBigCodeConfig\\n\\nfrom vllm.model_executor.input_metadata import InputMetadata\\nfrom vllm.model_executor.layers.activation import get_act_fn\\nfrom vllm.model_executor.layers.attention import PagedAttention\\nfrom vllm.model_executor.layers.sampler import Sampler\\nfrom vllm.model_executor.weight_utils import (\\n    convert_pyslice_to_tensor, hf_model_weights_iterator,\\n    load_padded_tensor_parallel_vocab, load_tensor_parallel_weights)\\nfrom vllm.model_executor.parallel_utils.parallel_state import (\\n    get_tensor_model_parallel_rank, get_tensor_model_parallel_world_size)\\nfrom vllm.model_executor.parallel_utils.tensor_parallel import (\\n    VocabParallelEmbedding, ColumnParallelLinear, RowParallelLinear)\\nfrom vllm.sequence import SamplerOutput\\n\\nKVCache = Tuple[torch.Tensor, torch.Tensor]\\n\\n\\nclass GPTBigCodeAttention(nn.Module):\\n\\n    def __init__(self, config: GPTBigCodeConfig):\\n        super().__init__()\\n        self.hidden_size = config.hidden_size\\n        total_num_heads = config.num_attention_heads\\n        self.tensor_model_parallel_world_size = (\\n            get_tensor_model_parallel_world_size())\\n        assert total_num_heads % self.tensor_model_parallel_world_size == 0\\n        self.num_heads = (total_num_heads //\\n                          self.tensor_model_parallel_world_size)\\n        self.head_dim = self.hidden_size // total_num_heads\\n        self.scale = self.head_dim**-0.5\\n\\n        self.multi_query = config.multi_query\\n        if self.multi_query:\\n            self.num_kv_heads = 1\\n            self.kv_dim = self.head_dim\\n            self.c_attn_q = ColumnParallelLinear(self.hidden_size,\\n                                                 self.hidden_size,\\n                                                 bias=True,\\n                                                 gather_output=False,\\n                                                 perform_initialization=False)\\n            self.c_attn_kv = nn.Linear(self.hidden_size,\\n                                       2 * self.kv_dim,\\n                                       bias=True)\\n        else:\\n            self.num_kv_heads = self.num_heads\\n            self.kv_dim = self.num_kv_heads * self.head_dim\\n            self.c_attn = ColumnParallelLinear(self.hidden_size,\\n                                               self.hidden_size +\\n                                               2 * self.kv_dim,\\n                                               bias=True,\\n                                               gather_output=False,\\n                                               perform_initialization=False)\\n\\n        self.c_proj = RowParallelLinear(self.hidden_size,\\n                                        self.hidden_size,\\n                                        bias=True,\\n                                        input_is_parallel=True,\\n                                        perform_initialization=False)\\n        self.attn = PagedAttention(self.num_heads,\\n                                   self.head_dim,\\n                                   scale=self.scale,\\n                                   num_kv_heads=self.num_kv_heads)\\n\\n    def forward(\\n        self,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        if self.multi_query:\\n            q, _ = self.c_attn_q(hidden_states)\\n            kv = self.c_attn_kv(hidden_states)\\n            k, v = kv.split([self.kv_dim, self.kv_dim], dim=-1)\\n        else:\\n            qkv, _ = self.c_attn(hidden_states)\\n            q, k, v = qkv.split([\\n                self.hidden_size // self.tensor_model_parallel_world_size,\\n                self.kv_dim, self.kv_dim\\n            ],\\n                                dim=-1)\\n        key_cache, value_cache = kv_cache\\n        attn_output = self.attn(q, k, v, key_cache, value_cache,\\n                                input_metadata, cache_event)\\n        attn_output, _ = self.c_proj(attn_output)\\n        return attn_output\\n\\n\\nclass GPTBigMLP(nn.Module):\\n\\n    def __init__(\\n        self,\\n        intermediate_size: int,\\n        config: GPTBigCodeConfig,\\n    ):\\n        super().__init__()\\n        hidden_size = config.hidden_size\\n        self.c_fc = ColumnParallelLinear(hidden_size,\\n                                         intermediate_size,\\n                                         bias=True,\\n                                         gather_output=False,\\n                                         perform_initialization=False)\\n        self.c_proj = RowParallelLinear(intermediate_size,\\n                                        hidden_size,\\n                                        bias=True,\\n                                        input_is_parallel=True,\\n                                        perform_initialization=False)\\n        self.act = get_act_fn(config.activation_function)\\n\\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\\n        hidden_states, _ = self.c_fc(hidden_states)\\n        hidden_states = self.act(hidden_states)\\n        hidden_states, _ = self.c_proj(hidden_states)\\n        return hidden_states\\n\\n\\nclass GPTBigCodeBlock(nn.Module):\\n\\n    def __init__(self, config: GPTBigCodeConfig):\\n        super().__init__()\\n        hidden_size = config.hidden_size\\n        inner_dim = (config.n_inner if config.n_inner is not None else 4 *\\n                     hidden_size)\\n\\n        self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\\n        self.attn = GPTBigCodeAttention(config)\\n        self.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\\n        self.mlp = GPTBigMLP(inner_dim, config)\\n\\n    def forward(\\n        self,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        residual = hidden_states\\n        hidden_states = self.ln_1(hidden_states)\\n        attn_output = self.attn(\\n            hidden_states=hidden_states,\\n            kv_cache=kv_cache,\\n            input_metadata=input_metadata,\\n            cache_event=cache_event,\\n        )\\n        # residual connection\\n        hidden_states = attn_output + residual\\n\\n        residual = hidden_states\\n        hidden_states = self.ln_2(hidden_states)\\n        feed_forward_hidden_states = self.mlp(hidden_states)\\n        # residual connection\\n        hidden_states = residual + feed_forward_hidden_states\\n        return hidden_states\\n\\n\\nclass GPTBigCodeModel(nn.Module):\\n\\n    def __init__(self, config: GPTBigCodeConfig):\\n        super().__init__()\\n        self.config = config\\n        assert not config.add_cross_attention\\n\\n        self.embed_dim = config.hidden_size\\n\\n        # Optimization: While the vocab size of GPT-2 is 50257, we extend it\\n        # to 50304 in order to make it divisible by 64.\\n        # This improves performance since GPUs are faster if the dimension\\n        # is divisible by 64. In addition, it allows us to shard the embedding\\n        # layer across 2, 4, 8, or more GPUs.\\n        vocab_size = ((config.vocab_size + 63) // 64) * 64\\n        self.wte = VocabParallelEmbedding(vocab_size, self.embed_dim)\\n        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\\n        self.h = nn.ModuleList(\\n            [GPTBigCodeBlock(config) for _ in range(config.num_hidden_layers)])\\n        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        position_ids: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> torch.Tensor:\\n        inputs_embeds = self.wte(input_ids)\\n        position_embeds = self.wpe(position_ids)\\n        hidden_states = inputs_embeds + position_embeds\\n\\n        for i in range(len(self.h)):\\n            if cache_events is None:\\n                cache_event = None\\n            else:\\n                cache_event = cache_events[i]\\n            layer = self.h[i]\\n            hidden_states = layer(hidden_states, kv_caches[i], input_metadata,\\n                                  cache_event)\\n\\n        hidden_states = self.ln_f(hidden_states)\\n        return hidden_states\\n\\n\\nclass GPTBigCodeForCausalLM(nn.Module):\\n\\n    def __init__(self, config: GPTBigCodeConfig):\\n        super().__init__()\\n        self.config = config\\n        self.transformer = GPTBigCodeModel(config)\\n        # TODO(zhuohan): create a new weight after implementing pipeline\\n        #                parallelism\\n        self.lm_head_weight = self.transformer.wte.weight\\n        self.sampler = Sampler(config.vocab_size)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        positions: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> SamplerOutput:\\n        hidden_states = self.transformer(input_ids, positions, kv_caches,\\n                                         input_metadata, cache_events)\\n        next_tokens = self.sampler(self.lm_head_weight, hidden_states,\\n                                   input_metadata)\\n        return next_tokens\\n\\n    _column_parallel_weights = [\"c_fc.weight\", \"c_fc.bias\"]\\n    _row_parallel_weights = [\"c_proj.weight\"]\\n\\n    def load_weights(self,\\n                     model_name_or_path: str,\\n                     cache_dir: Optional[str] = None,\\n                     load_format: str = \"auto\",\\n                     revision: Optional[str] = None):\\n        tensor_model_parallel_world_size = (\\n            get_tensor_model_parallel_world_size())\\n        tensor_model_parallel_rank = get_tensor_model_parallel_rank()\\n        state_dict = self.state_dict()\\n\\n        for name, loaded_weight in hf_model_weights_iterator(\\n                model_name_or_path, cache_dir, load_format, revision):\\n            if \"lm_head.weight\" in name:\\n                # GPT-2 ties the weights of the embedding layer and the final\\n                # linear layer.\\n                continue\\n            if \".attn.bias\" in name:\\n                # Skip attention mask.\\n                # NOTE: \"c_attn.bias\" should not be skipped.\\n                continue\\n\\n            if not name.startswith(\"transformer.\"):\\n                name = \"transformer.\" + name\\n\\n            # For the fused QKV linear layer, manually shard the weights.\\n            if \"c_attn\" in name:\\n                # GPT-2\\'s fused QKV has the shape of\\n                # [3 * num_heads * head_size, hidden_size].\\n                # When tensor parallelism is used, we shard the weights along\\n                # the head dimension.\\n                total_num_heads = self.config.num_attention_heads\\n                total_num_kv_heads = (1 if self.config.multi_query else\\n                                      total_num_heads)\\n                hidden_size = self.config.hidden_size\\n                head_size = hidden_size // total_num_heads\\n                total_kv_size = head_size * total_num_kv_heads\\n                num_heads = total_num_heads // tensor_model_parallel_world_size\\n                head_start = tensor_model_parallel_rank * num_heads\\n                head_end = (tensor_model_parallel_rank + 1) * num_heads\\n\\n                loaded_weight = convert_pyslice_to_tensor(loaded_weight)\\n                wq, wk, wv = torch.split(\\n                    loaded_weight, [hidden_size, total_kv_size, total_kv_size],\\n                    dim=0)\\n\\n                wq = wq[head_size * head_start:head_size * head_end]\\n                if not self.config.multi_query:\\n                    # Split the heads when using normal multi-head attention\\n                    wk = wk[head_size * head_start:head_size * head_end]\\n                    wv = wv[head_size * head_start:head_size * head_end]\\n                    loaded_weight = torch.cat([wq, wk, wv], dim=0)\\n                else:\\n                    # For multi-query attention, we split the query\\n                    # but replicate the key and value.\\n                    loaded_weight_q = wq\\n                    loaded_weight_kv = torch.cat([wk, wv], dim=0)\\n                    q_weight_name = name.replace(\"c_attn\", \"c_attn_q\")\\n                    kv_weight_name = name.replace(\"c_attn\", \"c_attn_kv\")\\n                    load_tensor_parallel_weights(state_dict[q_weight_name],\\n                                                 loaded_weight_q,\\n                                                 q_weight_name,\\n                                                 self._column_parallel_weights,\\n                                                 self._row_parallel_weights,\\n                                                 tensor_model_parallel_rank)\\n                    load_tensor_parallel_weights(state_dict[kv_weight_name],\\n                                                 loaded_weight_kv,\\n                                                 kv_weight_name,\\n                                                 self._column_parallel_weights,\\n                                                 self._row_parallel_weights,\\n                                                 tensor_model_parallel_rank)\\n                    continue\\n\\n            param = state_dict[name]\\n\\n            if name == \"transformer.wte.weight\":\\n                load_padded_tensor_parallel_vocab(param, loaded_weight,\\n                                                  tensor_model_parallel_rank)\\n                continue\\n\\n            load_tensor_parallel_weights(param, loaded_weight, name,\\n                                         self._column_parallel_weights,\\n                                         self._row_parallel_weights,\\n                                         tensor_model_parallel_rank)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/models/falcon.py\\n---------\\nContent:\\n# coding=utf-8\\n# Adapted from\\n# https://github.com/huggingface/transformers/blob/a5cc30d72ae2dc19af534e4b35c986cc28db1275/src/transformers/models/falcon/modeling_falcon.py\\n# Copyright 2023 The vLLM team.\\n# Copyright 2023 the Falcon authors and HuggingFace Inc. team.  All rights\\n# reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\"\"\"PyTorch Falcon model.\"\"\"\\n\\nimport math\\nfrom typing import List, Optional, Tuple, Union\\n\\nimport torch\\nfrom torch import nn\\nfrom torch.nn import LayerNorm\\nfrom transformers import FalconConfig as HF_FalconConfig\\n\\nfrom vllm.model_executor.input_metadata import InputMetadata\\nfrom vllm.model_executor.layers.attention import (PagedAttention,\\n                                                  PagedAttentionWithALiBi,\\n                                                  PagedAttentionWithRoPE)\\nfrom vllm.model_executor.layers.sampler import Sampler\\nfrom vllm.model_executor.weight_utils import (convert_pyslice_to_tensor,\\n                                              hf_model_weights_iterator,\\n                                              load_tensor_parallel_weights)\\nfrom vllm.model_executor.parallel_utils.parallel_state import (\\n    get_tensor_model_parallel_rank, get_tensor_model_parallel_world_size)\\nfrom vllm.model_executor.parallel_utils.tensor_parallel import (\\n    VocabParallelEmbedding, ColumnParallelLinear, RowParallelLinear,\\n    reduce_from_tensor_model_parallel_region)\\nfrom vllm.sequence import SamplerOutput\\nfrom vllm.transformers_utils.configs import RWConfig\\n\\nKVCache = Tuple[torch.Tensor, torch.Tensor]\\nFalconConfig = Union[HF_FalconConfig, RWConfig]\\n\\n\\n# NOTE(Hesslow): Unfortunately we did not fuse matmul and bias during\\n# training, this means that there\\'s one additional quantization to bfloat16\\n# between the operations. In order not to degrade the quality of our HF-port,\\n# we keep these characteristics in the final model.\\nclass FalconLinear(nn.Linear):\\n\\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\\n        hidden_states = x @ self.weight.T\\n        if self.bias is None:\\n            return hidden_states\\n        return hidden_states + self.bias\\n\\n\\ndef _get_alibi_slopes(total_num_heads: int) -> torch.Tensor:\\n    closest_power_of_2 = 2**math.floor(math.log2(total_num_heads))\\n    base = torch.tensor(2**(-(2**-(math.log2(closest_power_of_2) - 3))),\\n                        dtype=torch.float32)\\n    powers = torch.arange(1, 1 + closest_power_of_2, dtype=torch.int32)\\n    slopes = torch.pow(base, powers)\\n\\n    if closest_power_of_2 != total_num_heads:\\n        extra_base = torch.tensor(\\n            2**(-(2**-(math.log2(2 * closest_power_of_2) - 3))),\\n            dtype=torch.float32)\\n        num_remaining_heads = min(closest_power_of_2,\\n                                  total_num_heads - closest_power_of_2)\\n        extra_powers = torch.arange(1,\\n                                    1 + 2 * num_remaining_heads,\\n                                    2,\\n                                    dtype=torch.int32)\\n        slopes = torch.cat(\\n            [slopes, torch.pow(extra_base, extra_powers)], dim=0)\\n\\n    return slopes\\n\\n\\nclass FalconAttention(nn.Module):\\n\\n    def __init__(self, config: FalconConfig):\\n        super().__init__()\\n\\n        self.hidden_size = config.hidden_size\\n        tp_size = get_tensor_model_parallel_world_size()\\n\\n        self.total_num_heads = config.num_attention_heads\\n        assert self.total_num_heads % tp_size == 0\\n        self.num_heads = self.total_num_heads // tp_size\\n        self.head_dim = self.hidden_size // self.total_num_heads\\n        assert self.head_dim * self.total_num_heads == self.hidden_size\\n\\n        self.new_decoder_architecture = config.new_decoder_architecture\\n        self.multi_query = config.multi_query\\n\\n        if self.new_decoder_architecture:\\n            self.total_num_kv_heads = config.num_kv_heads\\n            assert self.total_num_heads % tp_size == 0\\n            self.num_kv_heads = self.total_num_kv_heads // tp_size\\n            self.query_key_value = ColumnParallelLinear(\\n                self.hidden_size,\\n                (self.total_num_heads + 2 * self.total_num_kv_heads) *\\n                self.head_dim,\\n                bias=config.bias,\\n                gather_output=False,\\n                perform_initialization=False,\\n                skip_bias_add=True,\\n            )\\n        elif self.multi_query:\\n            self.total_num_kv_heads = 1\\n            self.num_kv_heads = 1\\n            self.query = ColumnParallelLinear(\\n                self.hidden_size,\\n                self.total_num_heads * self.head_dim,\\n                bias=config.bias,\\n                gather_output=False,\\n                perform_initialization=False,\\n                skip_bias_add=True,\\n            )\\n            self.key_value = FalconLinear(self.hidden_size,\\n                                          2 * self.head_dim,\\n                                          bias=config.bias)\\n        else:\\n            self.total_num_kv_heads = self.total_num_heads\\n            self.num_kv_heads = self.num_heads\\n            self.query_key_value = ColumnParallelLinear(\\n                self.hidden_size,\\n                (self.total_num_heads + 2 * self.total_num_kv_heads) *\\n                self.head_dim,\\n                bias=config.bias,\\n                gather_output=False,\\n                perform_initialization=False,\\n                skip_bias_add=True,\\n            )\\n\\n        self.q_size = self.num_heads * self.head_dim\\n        self.kv_size = self.num_kv_heads * self.head_dim\\n\\n        # Layer-wise attention scaling\\n        self.inv_norm_factor = 1.0 / math.sqrt(self.head_dim)\\n        self.reduce_row_parallel_results = not (config.new_decoder_architecture\\n                                                or config.parallel_attn)\\n        self.dense = RowParallelLinear(\\n            self.hidden_size,\\n            self.hidden_size,\\n            bias=config.bias,\\n            input_is_parallel=True,\\n            perform_initialization=False,\\n            skip_bias_add=True,\\n            reduce_results=self.reduce_row_parallel_results)\\n\\n        self.use_rotary = config.rotary\\n        self.use_alibi = config.alibi\\n        assert not (self.use_rotary and self.use_alibi), (\\n            \"Rotary and alibi are mutually exclusive.\")\\n\\n        if self.use_rotary:\\n            # TODO(zhuohan): Pass in correct `max_position``\\n            self.attn = PagedAttentionWithRoPE(self.num_heads,\\n                                               self.head_dim,\\n                                               self.inv_norm_factor,\\n                                               rotary_dim=self.head_dim,\\n                                               num_kv_heads=self.num_kv_heads)\\n        elif self.use_alibi:\\n            tp_rank = get_tensor_model_parallel_rank()\\n            head_start = tp_rank * self.num_heads\\n            head_end = (tp_rank + 1) * self.num_heads\\n            alibi_slopes = (_get_alibi_slopes(self.total_num_heads) *\\n                            self.inv_norm_factor)\\n            alibi_slopes = alibi_slopes[head_start:head_end].tolist()\\n            self.attn = PagedAttentionWithALiBi(self.num_heads,\\n                                                self.head_dim,\\n                                                self.inv_norm_factor,\\n                                                alibi_slopes,\\n                                                num_kv_heads=self.num_kv_heads)\\n        else:\\n            self.attn = PagedAttention(self.num_heads,\\n                                       self.head_dim,\\n                                       scale=self.inv_norm_factor,\\n                                       num_kv_heads=self.num_kv_heads)\\n\\n    def forward(\\n        self,\\n        positions: torch.Tensor,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        if not self.new_decoder_architecture and self.multi_query:\\n            q, bias = self.query(hidden_states)\\n            if bias is not None:\\n                q += bias\\n            kv = self.key_value(hidden_states)\\n            k, v = kv.split([self.kv_size, self.kv_size], dim=-1)\\n        else:\\n            qkv, bias = self.query_key_value(hidden_states)\\n            if bias is not None:\\n                qkv += bias\\n            q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size],\\n                                dim=-1)\\n        k_cache, v_cache = kv_cache\\n        if self.use_rotary:\\n            attn_output = self.attn(positions, q, k, v, k_cache, v_cache,\\n                                    input_metadata, cache_event)\\n        else:\\n            attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata,\\n                                    cache_event)\\n        attn_output, bias = self.dense(attn_output)\\n        return attn_output, bias\\n\\n\\nclass FalconMLP(nn.Module):\\n\\n    def __init__(self, config: FalconConfig):\\n        super().__init__()\\n        hidden_size = config.hidden_size\\n\\n        self.dense_h_to_4h = ColumnParallelLinear(hidden_size,\\n                                                  4 * hidden_size,\\n                                                  bias=config.bias,\\n                                                  gather_output=False,\\n                                                  perform_initialization=False,\\n                                                  skip_bias_add=True)\\n        self.act = nn.GELU()\\n        self.reduce_row_parallel_results = not (config.new_decoder_architecture\\n                                                or config.parallel_attn)\\n        self.dense_4h_to_h = RowParallelLinear(\\n            4 * hidden_size,\\n            hidden_size,\\n            bias=config.bias,\\n            input_is_parallel=True,\\n            perform_initialization=False,\\n            skip_bias_add=True,\\n            reduce_results=self.reduce_row_parallel_results)\\n\\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\\n        # NOTE(zhuohan): Following huggingface, we do not fuse bias add here.\\n        x, bias = self.dense_h_to_4h(x)\\n        if bias is not None:\\n            x += bias\\n        x = self.act(x)\\n        x, bias = self.dense_4h_to_h(x)\\n        return x, bias\\n\\n\\nclass FalconDecoderLayer(nn.Module):\\n\\n    def __init__(self, config: FalconConfig):\\n        super().__init__()\\n        hidden_size = config.hidden_size\\n        self.num_heads = config.num_attention_heads\\n        self.self_attention = FalconAttention(config)\\n        self.mlp = FalconMLP(config)\\n        self.config = config\\n\\n        if config.new_decoder_architecture:\\n            # The layer norm before self-attention\\n            self.ln_attn = LayerNorm(hidden_size,\\n                                     eps=config.layer_norm_epsilon)\\n            # The layer norm before the MLP\\n            self.ln_mlp = LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\\n        else:\\n            self.input_layernorm = LayerNorm(hidden_size,\\n                                             eps=config.layer_norm_epsilon)\\n            if not config.parallel_attn:\\n                self.post_attention_layernorm = LayerNorm(\\n                    hidden_size, eps=config.layer_norm_epsilon)\\n\\n        self.reduce_row_parallel_results = not (config.new_decoder_architecture\\n                                                or config.parallel_attn)\\n\\n    def forward(\\n        self,\\n        positions: torch.Tensor,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ):\\n        residual = hidden_states\\n\\n        if self.config.new_decoder_architecture:\\n            attention_layernorm_out = self.ln_attn(hidden_states)\\n            mlp_layernorm_out = self.ln_mlp(hidden_states)\\n        else:\\n            attention_layernorm_out = self.input_layernorm(hidden_states)\\n\\n        # Self attention.\\n        attention_output, attention_bias = self.self_attention(\\n            positions=positions,\\n            hidden_states=attention_layernorm_out,\\n            kv_cache=kv_cache,\\n            input_metadata=input_metadata,\\n            cache_event=cache_event,\\n        )\\n        if self.reduce_row_parallel_results and attention_bias is not None:\\n            attention_output += attention_bias\\n\\n        if not self.config.new_decoder_architecture:\\n            if self.config.parallel_attn:\\n                mlp_layernorm_out = attention_layernorm_out\\n            else:\\n                residual += attention_output\\n                mlp_layernorm_out = self.post_attention_layernorm(residual)\\n\\n        # MLP.\\n        mlp_output, mlp_bias = self.mlp(mlp_layernorm_out)\\n        if self.reduce_row_parallel_results and mlp_bias is not None:\\n            mlp_output += mlp_bias\\n\\n        if not self.reduce_row_parallel_results:\\n            # When MLP and Attention layers are parallel, we can use\\n            # only one all-reduce operator to reduce the results from\\n            # both MLP and Attention layers.\\n            mlp_output += attention_output\\n            mlp_output = reduce_from_tensor_model_parallel_region(mlp_output)\\n            if attention_bias is not None:\\n                mlp_output += attention_bias\\n            if mlp_bias is not None:\\n                mlp_output += mlp_bias\\n\\n        output = mlp_output + residual\\n\\n        return output\\n\\n\\nclass FalconModel(nn.Module):\\n\\n    def __init__(self, config: FalconConfig):\\n        super().__init__()\\n        self.config = config\\n        self.embed_dim = config.hidden_size\\n        self.num_heads = config.num_attention_heads\\n        self.use_alibi = config.alibi\\n\\n        # Embedding + LN Embedding\\n        self.word_embeddings = VocabParallelEmbedding(\\n            config.vocab_size, self.embed_dim, perform_initialization=False)\\n\\n        # Transformer blocks\\n        self.h = nn.ModuleList([\\n            FalconDecoderLayer(config) for _ in range(config.num_hidden_layers)\\n        ])\\n\\n        # Final Layer Norm\\n        self.ln_f = LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.LongTensor,\\n        positions: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> torch.Tensor:\\n        hidden_states = self.word_embeddings(input_ids)\\n        for i in range(len(self.h)):\\n            if cache_events is None:\\n                cache_event = None\\n            else:\\n                cache_event = cache_events[i]\\n            layer = self.h[i]\\n            hidden_states = layer(\\n                positions,\\n                hidden_states,\\n                kv_caches[i],\\n                input_metadata,\\n                cache_event,\\n            )\\n        hidden_states = self.ln_f(hidden_states)\\n        return hidden_states\\n\\n\\nclass FalconForCausalLM(nn.Module):\\n\\n    def __init__(self, config: FalconConfig):\\n        super().__init__()\\n        self.config = config\\n        self.transformer = FalconModel(config)\\n        self.lm_head = ColumnParallelLinear(config.hidden_size,\\n                                            config.vocab_size,\\n                                            bias=False,\\n                                            gather_output=False,\\n                                            perform_initialization=False)\\n        self.sampler = Sampler(config.vocab_size)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.LongTensor,\\n        positions: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> SamplerOutput:\\n        hidden_states = self.transformer(\\n            input_ids,\\n            positions,\\n            kv_caches,\\n            input_metadata,\\n            cache_events,\\n        )\\n        next_tokens = self.sampler(self.lm_head.weight, hidden_states,\\n                                   input_metadata)\\n\\n        return next_tokens\\n\\n    _column_parallel_weights = [\\n        \"word_embeddings.weight\", \"lm_head.weight\", \"dense_h_to_4h.weight\",\\n        \"dense_h_to_4h.bias\"\\n    ]\\n    _row_parallel_weights = [\"dense.weight\", \"dense_4h_to_h.weight\"]\\n\\n    def load_weights(self,\\n                     model_name_or_path: str,\\n                     cache_dir: Optional[str] = None,\\n                     load_format: str = \"auto\",\\n                     revision: Optional[str] = None):\\n        tp_size = (get_tensor_model_parallel_world_size())\\n        tp_rank = get_tensor_model_parallel_rank()\\n\\n        hidden_size = self.config.hidden_size\\n        total_num_heads = self.config.num_attention_heads\\n        num_heads = total_num_heads // tp_size\\n        head_size = hidden_size // total_num_heads\\n        head_start = tp_rank * num_heads\\n        head_end = (tp_rank + 1) * num_heads\\n        if self.config.new_decoder_architecture:\\n            total_num_kv_heads = self.config.num_kv_heads\\n            num_kv_heads = total_num_kv_heads // tp_size\\n            separated_q_kv = False\\n            kv_head_start = tp_rank * num_kv_heads\\n            kv_head_end = (tp_rank + 1) * num_kv_heads\\n        elif self.config.multi_query:\\n            total_num_kv_heads = 1\\n            num_kv_heads = 1\\n            separated_q_kv = True\\n            kv_head_start = 0\\n            kv_head_end = 1\\n        else:\\n            total_num_kv_heads = total_num_heads\\n            num_kv_heads = total_num_kv_heads // tp_size\\n            separated_q_kv = False\\n            kv_head_start = tp_rank * num_kv_heads\\n            kv_head_end = (tp_rank + 1) * num_kv_heads\\n        num_query_heads_per_kv_head = total_num_heads // total_num_kv_heads\\n        state_dict = self.state_dict()\\n\\n        for name, loaded_weight in hf_model_weights_iterator(\\n                model_name_or_path, cache_dir, load_format, revision):\\n            if \"query_key_value\" in name:\\n                loaded_weight = convert_pyslice_to_tensor(loaded_weight)\\n                loaded_weight_size = loaded_weight.size()\\n                loaded_weight = loaded_weight.view(\\n                    total_num_kv_heads, num_query_heads_per_kv_head + 2,\\n                    head_size, *loaded_weight_size[1:])\\n\\n                wq = loaded_weight[:, :-2].reshape(-1, *loaded_weight_size[1:])\\n                wk = loaded_weight[:, [-2]].reshape(-1,\\n                                                    *loaded_weight_size[1:])\\n                wv = loaded_weight[:, [-1]].reshape(-1,\\n                                                    *loaded_weight_size[1:])\\n\\n                wq = wq[head_size * head_start:head_size * head_end]\\n                wk = wk[head_size * kv_head_start:head_size * kv_head_end]\\n                wv = wv[head_size * kv_head_start:head_size * kv_head_end]\\n\\n                if separated_q_kv:\\n                    loaded_weight_q = wq\\n                    loaded_weight_kv = torch.cat([wk, wv], dim=0)\\n                    q_weight_name = name.replace(\"query_key_value\", \"query\")\\n                    kv_weight_name = name.replace(\"query_key_value\",\\n                                                  \"key_value\")\\n                    load_tensor_parallel_weights(state_dict[q_weight_name],\\n                                                 loaded_weight_q,\\n                                                 q_weight_name,\\n                                                 self._column_parallel_weights,\\n                                                 self._row_parallel_weights,\\n                                                 tp_rank)\\n                    load_tensor_parallel_weights(state_dict[kv_weight_name],\\n                                                 loaded_weight_kv,\\n                                                 kv_weight_name,\\n                                                 self._column_parallel_weights,\\n                                                 self._row_parallel_weights,\\n                                                 tp_rank)\\n                    continue\\n                else:\\n                    loaded_weight = torch.cat([wq, wk, wv], dim=0)\\n\\n            param = state_dict[name]\\n            load_tensor_parallel_weights(param, loaded_weight, name,\\n                                         self._column_parallel_weights,\\n                                         self._row_parallel_weights, tp_rank)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/models/gpt_neox.py\\n---------\\nContent:\\n# coding=utf-8\\n# Adapted from\\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/gpt_neox/modeling_gpt_neox.py\\n# Copyright 2023 The vLLM team.\\n# Copyright 2022 EleutherAI The HuggingFace Inc. team. All rights reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\"\"\"Inference-only GPT-NeoX model compatible with HuggingFace weights.\\n\\nThe input of the model is flattened to a 1D tensor of tokens. The model uses\\nInputMetadata to extract the original 2D shape of the input.\\n\"\"\"\\nfrom typing import List, Optional, Tuple\\n\\nimport torch\\nfrom torch import nn\\nfrom transformers import GPTNeoXConfig\\n\\nfrom vllm.model_executor.input_metadata import InputMetadata\\nfrom vllm.model_executor.layers.activation import get_act_fn\\nfrom vllm.model_executor.layers.attention import PagedAttentionWithRoPE\\nfrom vllm.model_executor.layers.sampler import Sampler\\nfrom vllm.model_executor.weight_utils import (hf_model_weights_iterator,\\n                                              load_tensor_parallel_weights)\\nfrom vllm.model_executor.parallel_utils.parallel_state import (\\n    get_tensor_model_parallel_rank, get_tensor_model_parallel_world_size)\\nfrom vllm.model_executor.parallel_utils.tensor_parallel import (\\n    VocabParallelEmbedding, ColumnParallelLinear, RowParallelLinear)\\nfrom vllm.sequence import SamplerOutput\\n\\nKVCache = Tuple[torch.Tensor, torch.Tensor]\\n\\n\\nclass GPTNeoXAttention(nn.Module):\\n\\n    def __init__(self, config: GPTNeoXConfig):\\n        super().__init__()\\n        self.total_num_heads = config.num_attention_heads\\n        self.hidden_size = config.hidden_size\\n        self.head_size = self.hidden_size // self.total_num_heads\\n\\n        tensor_model_parallel_world_size = (\\n            get_tensor_model_parallel_world_size())\\n        assert self.total_num_heads % tensor_model_parallel_world_size == 0\\n        self.num_heads = (self.total_num_heads //\\n                          tensor_model_parallel_world_size)\\n\\n        self.query_key_value = ColumnParallelLinear(\\n            config.hidden_size,\\n            3 * config.hidden_size,\\n            gather_output=False,\\n            perform_initialization=False)\\n        self.dense = RowParallelLinear(config.hidden_size,\\n                                       config.hidden_size,\\n                                       input_is_parallel=True,\\n                                       perform_initialization=False)\\n\\n        scaling = self.head_size**-0.5\\n        rotary_dim = int(self.head_size * config.rotary_pct)\\n        assert rotary_dim % 2 == 0\\n        self.attn = PagedAttentionWithRoPE(self.num_heads, self.head_size,\\n                                           scaling, rotary_dim)\\n\\n    def forward(\\n        self,\\n        position_ids: torch.Tensor,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        qkv, _ = self.query_key_value(hidden_states)\\n        q, k, v = qkv.chunk(chunks=3, dim=-1)\\n        k_cache, v_cache = kv_cache\\n        attn_output = self.attn(position_ids, q, k, v, k_cache, v_cache,\\n                                input_metadata, cache_event)\\n        output, _ = self.dense(attn_output)\\n        return output\\n\\n\\nclass GPTNeoXMLP(nn.Module):\\n\\n    def __init__(self, config: GPTNeoXConfig):\\n        super().__init__()\\n        self.dense_h_to_4h = ColumnParallelLinear(config.hidden_size,\\n                                                  config.intermediate_size,\\n                                                  gather_output=False,\\n                                                  perform_initialization=False)\\n        self.dense_4h_to_h = RowParallelLinear(config.intermediate_size,\\n                                               config.hidden_size,\\n                                               input_is_parallel=True,\\n                                               perform_initialization=False)\\n        self.act = get_act_fn(config.hidden_act)\\n\\n    def forward(self, hidden_states):\\n        hidden_states, _ = self.dense_h_to_4h(hidden_states)\\n        hidden_states = self.act(hidden_states)\\n        hidden_states, _ = self.dense_4h_to_h(hidden_states)\\n        return hidden_states\\n\\n\\nclass GPTNeoXLayer(nn.Module):\\n\\n    def __init__(self, config: GPTNeoXConfig):\\n        super().__init__()\\n        self.use_parallel_residual = config.use_parallel_residual\\n        self.input_layernorm = nn.LayerNorm(config.hidden_size,\\n                                            eps=config.layer_norm_eps)\\n        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size,\\n                                                     eps=config.layer_norm_eps)\\n        self.attention = GPTNeoXAttention(config)\\n        self.mlp = GPTNeoXMLP(config)\\n\\n    def forward(\\n        self,\\n        position_ids: torch.Tensor,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        attn_input = self.input_layernorm(hidden_states)\\n        attn_output = self.attention(\\n            position_ids=position_ids,\\n            hidden_states=attn_input,\\n            kv_cache=kv_cache,\\n            input_metadata=input_metadata,\\n            cache_event=cache_event,\\n        )\\n\\n        if self.use_parallel_residual:\\n            # pseudocode:\\n            # x = x + attn(ln1(x)) + mlp(ln2(x))\\n            mlp_input = self.post_attention_layernorm(hidden_states)\\n            mlp_output = self.mlp(mlp_input)\\n            hidden_states = mlp_output + attn_output + hidden_states\\n        else:\\n            # pseudocode:\\n            # x = x + attn(ln1(x))\\n            # x = x + mlp(ln2(x))\\n            attn_output = attn_output + hidden_states\\n            mlp_input = self.post_attention_layernorm(attn_output)\\n            mlp_output = self.mlp(mlp_input)\\n            hidden_states = mlp_output + attn_output\\n        return hidden_states\\n\\n\\nclass GPTNeoXModel(nn.Module):\\n\\n    def __init__(self, config: GPTNeoXConfig):\\n        super().__init__()\\n        self.config = config\\n\\n        self.embed_in = VocabParallelEmbedding(config.vocab_size,\\n                                               config.hidden_size,\\n                                               perform_initialization=False)\\n        self.layers = nn.ModuleList(\\n            [GPTNeoXLayer(config) for _ in range(config.num_hidden_layers)])\\n        self.final_layer_norm = nn.LayerNorm(config.hidden_size,\\n                                             eps=config.layer_norm_eps)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        position_ids: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> torch.Tensor:\\n        hidden_states = self.embed_in(input_ids)\\n        for i in range(len(self.layers)):\\n            if cache_events is None:\\n                cache_event = None\\n            else:\\n                cache_event = cache_events[i]\\n            layer = self.layers[i]\\n            hidden_states = layer(\\n                position_ids,\\n                hidden_states,\\n                kv_caches[i],\\n                input_metadata,\\n                cache_event,\\n            )\\n        hidden_states = self.final_layer_norm(hidden_states)\\n        return hidden_states\\n\\n\\nclass GPTNeoXForCausalLM(nn.Module):\\n\\n    def __init__(self, config):\\n        super().__init__()\\n        self.config = config\\n        self.gpt_neox = GPTNeoXModel(config)\\n        self.embed_out = ColumnParallelLinear(config.hidden_size,\\n                                              config.vocab_size,\\n                                              bias=False,\\n                                              gather_output=False,\\n                                              perform_initialization=False)\\n        self.sampler = Sampler(config.vocab_size)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        positions: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> SamplerOutput:\\n        hidden_states = self.gpt_neox(input_ids, positions, kv_caches,\\n                                      input_metadata, cache_events)\\n        next_tokens = self.sampler(self.embed_out.weight, hidden_states,\\n                                   input_metadata)\\n        return next_tokens\\n\\n    _column_parallel_weights = [\\n        \"embed_in.weight\", \"embed_out.weight\", \"dense_h_to_4h.weight\",\\n        \"dense_h_to_4h.bias\"\\n    ]\\n    _row_parallel_weights = [\"dense.weight\", \"dense_4h_to_h.weight\"]\\n\\n    def load_weights(self,\\n                     model_name_or_path: str,\\n                     cache_dir: Optional[str] = None,\\n                     load_format: str = \"auto\",\\n                     revision: Optional[str] = None):\\n        tensor_model_parallel_rank = get_tensor_model_parallel_rank()\\n        state_dict = self.state_dict()\\n        for name, loaded_weight in hf_model_weights_iterator(\\n                model_name_or_path, cache_dir, load_format, revision):\\n            if (\"attention.bias\" in name or \"attention.masked_bias\" in name\\n                    or \"rotary_emb.inv_freq\" in name):\\n                continue\\n            param = state_dict[name]\\n            if \"query_key_value\" in name:\\n                # NOTE(woosuk): GPT-NeoX\\'s fused QKV has the shape of\\n                # [num_heads * 3 * head_size, hidden_size], while the\\n                # required shape is [3 * num_heads * head_size, hidden_size].\\n                # Thus, we need weight conversion.\\n                shard_size = param.shape[0]\\n                loaded_weight = loaded_weight[\\n                    shard_size * tensor_model_parallel_rank:shard_size *\\n                    (tensor_model_parallel_rank + 1)]\\n\\n                num_heads = self.config.num_attention_heads\\n                hidden_size = self.config.hidden_size\\n                head_size = hidden_size // num_heads\\n                if \"query_key_value.weight\" in name:\\n                    loaded_weight = loaded_weight.view(-1, 3, head_size,\\n                                                       hidden_size)\\n                    loaded_weight = loaded_weight.transpose(0, 1)\\n                    loaded_weight = loaded_weight.reshape(-1, hidden_size)\\n                elif \"query_key_value.bias\" in name:\\n                    loaded_weight = loaded_weight.view(-1, 3, head_size)\\n                    loaded_weight = loaded_weight.transpose(0, 1)\\n                    loaded_weight = loaded_weight.reshape(-1)\\n                else:\\n                    raise ValueError(f\"Unexpected weight name: {name}\")\\n            load_tensor_parallel_weights(param, loaded_weight, name,\\n                                         self._column_parallel_weights,\\n                                         self._row_parallel_weights,\\n                                         tensor_model_parallel_rank)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/models/internlm.py\\n---------\\nContent:\\n# -*- coding: utf-8 -*-\\nfrom typing import List, Optional, Tuple\\n\\nimport torch\\nfrom torch import nn\\nfrom transformers import LlamaConfig\\n\\nfrom vllm.model_executor.input_metadata import InputMetadata\\nfrom vllm.model_executor.layers.activation import SiluAndMul\\nfrom vllm.model_executor.layers.attention import PagedAttentionWithRoPE\\nfrom vllm.model_executor.layers.layernorm import RMSNorm\\nfrom vllm.model_executor.layers.sampler import Sampler\\nfrom vllm.model_executor.parallel_utils.parallel_state import (\\n    get_tensor_model_parallel_rank, get_tensor_model_parallel_world_size)\\nfrom vllm.model_executor.parallel_utils.tensor_parallel import (\\n    ColumnParallelLinear, RowParallelLinear, VocabParallelEmbedding)\\nfrom vllm.model_executor.weight_utils import (\\n    hf_model_weights_iterator, load_padded_tensor_parallel_vocab,\\n    load_tensor_parallel_weights)\\nfrom vllm.sequence import SamplerOutput\\n\\nKVCache = Tuple[torch.Tensor, torch.Tensor]\\n\\n\\nclass InternLMMLP(nn.Module):\\n\\n    def __init__(\\n        self,\\n        hidden_size: int,\\n        intermediate_size: int,\\n        hidden_act: str,\\n    ):\\n        super().__init__()\\n        self.gate_up_proj = ColumnParallelLinear(hidden_size,\\n                                                 2 * intermediate_size,\\n                                                 bias=False,\\n                                                 gather_output=False,\\n                                                 perform_initialization=False)\\n        self.down_proj = RowParallelLinear(intermediate_size,\\n                                           hidden_size,\\n                                           bias=False,\\n                                           input_is_parallel=True,\\n                                           perform_initialization=False)\\n        if hidden_act != \"silu\":\\n            raise ValueError(f\"Unsupported activation: {hidden_act}. \"\\n                             \"Only silu is supported for now.\")\\n        self.act_fn = SiluAndMul()\\n\\n    def forward(self, x):\\n        gate_up, _ = self.gate_up_proj(x)\\n        x = self.act_fn(gate_up)\\n        x, _ = self.down_proj(x)\\n        return x\\n\\n\\nclass InternLMAttention(nn.Module):\\n\\n    def __init__(\\n        self,\\n        hidden_size: int,\\n        num_heads: int,\\n    ):\\n        super().__init__()\\n        self.hidden_size = hidden_size\\n        tensor_model_parallel_world_size = (\\n            get_tensor_model_parallel_world_size())\\n        self.total_num_heads = num_heads\\n        assert self.total_num_heads % tensor_model_parallel_world_size == 0\\n        self.num_heads = (self.total_num_heads //\\n                          tensor_model_parallel_world_size)\\n        self.head_dim = hidden_size // self.total_num_heads\\n        self.scaling = self.head_dim**-0.5\\n\\n        self.qkv_proj = ColumnParallelLinear(\\n            hidden_size,\\n            3 * self.total_num_heads * self.head_dim,\\n            bias=True,\\n            gather_output=False,\\n            perform_initialization=False,\\n        )\\n        self.o_proj = RowParallelLinear(\\n            self.total_num_heads * self.head_dim,\\n            hidden_size,\\n            bias=True,\\n            input_is_parallel=True,\\n            perform_initialization=False,\\n        )\\n        self.attn = PagedAttentionWithRoPE(self.num_heads,\\n                                           self.head_dim,\\n                                           self.scaling,\\n                                           rotary_dim=self.head_dim)\\n\\n    def forward(\\n        self,\\n        positions: torch.Tensor,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        qkv, _ = self.qkv_proj(hidden_states)\\n        q, k, v = qkv.chunk(chunks=3, dim=-1)\\n        k_cache, v_cache = kv_cache\\n        attn_output = self.attn(positions, q, k, v, k_cache, v_cache,\\n                                input_metadata, cache_event)\\n        output, _ = self.o_proj(attn_output)\\n        return output\\n\\n\\nclass InternLMDecoderLayer(nn.Module):\\n\\n    def __init__(self, config: LlamaConfig):\\n        super().__init__()\\n        self.hidden_size = config.hidden_size\\n        self.self_attn = InternLMAttention(\\n            hidden_size=self.hidden_size,\\n            num_heads=config.num_attention_heads,\\n        )\\n        self.mlp = InternLMMLP(\\n            hidden_size=self.hidden_size,\\n            intermediate_size=config.intermediate_size,\\n            hidden_act=config.hidden_act,\\n        )\\n        self.input_layernorm = RMSNorm(config.hidden_size,\\n                                       eps=config.rms_norm_eps)\\n        self.post_attention_layernorm = RMSNorm(config.hidden_size,\\n                                                eps=config.rms_norm_eps)\\n\\n    def forward(\\n        self,\\n        positions: torch.Tensor,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        # Self Attention\\n        residual = hidden_states\\n        hidden_states = self.input_layernorm(hidden_states)\\n        hidden_states = self.self_attn(\\n            positions=positions,\\n            hidden_states=hidden_states,\\n            kv_cache=kv_cache,\\n            input_metadata=input_metadata,\\n            cache_event=cache_event,\\n        )\\n        hidden_states = residual + hidden_states\\n\\n        # Fully Connected\\n        residual = hidden_states\\n        hidden_states = self.post_attention_layernorm(hidden_states)\\n        hidden_states = self.mlp(hidden_states)\\n        hidden_states = residual + hidden_states\\n        return hidden_states\\n\\n\\nclass InternLMModel(nn.Module):\\n\\n    def __init__(self, config: LlamaConfig):\\n        super().__init__()\\n        self.config = config\\n        self.padding_idx = config.pad_token_id\\n        self.vocab_size = config.vocab_size\\n\\n        vocab_size = ((config.vocab_size + 63) // 64) * 64\\n        self.embed_tokens = VocabParallelEmbedding(\\n            vocab_size, config.hidden_size, perform_initialization=False)\\n        self.layers = nn.ModuleList([\\n            InternLMDecoderLayer(config)\\n            for _ in range(config.num_hidden_layers)\\n        ])\\n        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        positions: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> torch.Tensor:\\n        hidden_states = self.embed_tokens(input_ids)\\n        for i in range(len(self.layers)):\\n            if cache_events is None:\\n                cache_event = None\\n            else:\\n                cache_event = cache_events[i]\\n            layer = self.layers[i]\\n            hidden_states = layer(\\n                positions,\\n                hidden_states,\\n                kv_caches[i],\\n                input_metadata,\\n                cache_event,\\n            )\\n        hidden_states = self.norm(hidden_states)\\n        return hidden_states\\n\\n\\nclass InternLMForCausalLM(nn.Module):\\n\\n    def __init__(self, config):\\n        super().__init__()\\n        self.config = config\\n        self.model = InternLMModel(config)\\n        vocab_size = ((config.vocab_size + 63) // 64) * 64\\n        self.lm_head = ColumnParallelLinear(config.hidden_size,\\n                                            vocab_size,\\n                                            bias=False,\\n                                            gather_output=False,\\n                                            perform_initialization=False)\\n        self.sampler = Sampler(config.vocab_size)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        positions: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> SamplerOutput:\\n        hidden_states = self.model(input_ids, positions, kv_caches,\\n                                   input_metadata, cache_events)\\n        next_tokens = self.sampler(self.lm_head.weight, hidden_states,\\n                                   input_metadata)\\n        return next_tokens\\n\\n    _column_parallel_weights = [\\n        \"qkv_proj.weight\", \"gate_proj.weight\", \"up_proj.weight\"\\n    ]\\n    _row_parallel_weights = [\"o_proj.weight\", \"down_proj.weight\"]\\n\\n    def load_weights(self,\\n                     model_name_or_path: str,\\n                     cache_dir: Optional[str] = None,\\n                     load_format: str = \"auto\",\\n                     revision: Optional[str] = None):\\n        tensor_model_parallel_rank = get_tensor_model_parallel_rank()\\n        state_dict = self.state_dict()\\n\\n        for name, loaded_weight in hf_model_weights_iterator(\\n                model_name_or_path, cache_dir, load_format, revision):\\n            if \"rotary_emb.inv_freq\" in name:\\n                continue\\n\\n            if \"embed_tokens\" in name or \"lm_head\" in name:\\n                param = state_dict[name]\\n                load_padded_tensor_parallel_vocab(param, loaded_weight,\\n                                                  tensor_model_parallel_rank)\\n                continue\\n\\n            is_attention_weight = False\\n            for stride_id, att_weight_name in enumerate(\\n                [\"q_proj\", \"k_proj\", \"v_proj\"]):\\n                if att_weight_name not in name:\\n                    continue\\n                param = state_dict[name.replace(att_weight_name, \"qkv_proj\")]\\n                shard_size = param.shape[0] // 3\\n                loaded_weight = loaded_weight[\\n                    shard_size * tensor_model_parallel_rank:shard_size *\\n                    (tensor_model_parallel_rank + 1)]\\n                param_slice = param.data[shard_size * stride_id:shard_size *\\n                                         (stride_id + 1)]\\n                assert param_slice.shape == loaded_weight.shape\\n                param_slice.copy_(loaded_weight)\\n                is_attention_weight = True\\n                break\\n            if is_attention_weight:\\n                continue\\n\\n            is_gate_up_weight = False\\n            for stride_id, weight_name in enumerate([\"gate_proj\", \"up_proj\"]):\\n                if weight_name not in name:\\n                    continue\\n                param = state_dict[name.replace(weight_name, \"gate_up_proj\")]\\n                shard_size = param.shape[0] // 2\\n                loaded_weight = loaded_weight[\\n                    shard_size * tensor_model_parallel_rank:shard_size *\\n                    (tensor_model_parallel_rank + 1)]\\n                param_slice = param.data[shard_size * stride_id:shard_size *\\n                                         (stride_id + 1)]\\n                assert param_slice.shape == loaded_weight.shape\\n                param_slice.copy_(loaded_weight)\\n                is_gate_up_weight = True\\n                break\\n            if is_gate_up_weight:\\n                continue\\n\\n            param = state_dict[name]\\n            load_tensor_parallel_weights(param, loaded_weight, name,\\n                                         self._column_parallel_weights,\\n                                         self._row_parallel_weights,\\n                                         tensor_model_parallel_rank)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/models/bloom.py\\n---------\\nContent:\\n# coding=utf-8\\n# Adapted from\\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/bloom/modeling_bloom.py\\n# Copyright 2023 The CacheFlow team.\\n# Copyright 2022 HuggingFace Inc. team and BigScience workshop.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\"\"\"Inference-only BLOOM model compatible with HuggingFace weights.\\n\\nThe input of the model is flattened to a 1D tensor of tokens. The model uses\\nInputMetadata to extract the original 2D shape of the input.\\n\"\"\"\\nimport math\\nfrom typing import List, Optional, Tuple\\n\\nimport torch\\nfrom torch import nn\\nfrom transformers import BloomConfig\\n\\nfrom vllm.model_executor.input_metadata import InputMetadata\\nfrom vllm.model_executor.layers.activation import get_act_fn\\nfrom vllm.model_executor.layers.attention import PagedAttentionWithALiBi\\nfrom vllm.model_executor.layers.sampler import Sampler\\nfrom vllm.model_executor.weight_utils import (hf_model_weights_iterator,\\n                                              load_tensor_parallel_weights)\\nfrom vllm.model_executor.parallel_utils.parallel_state import (\\n    get_tensor_model_parallel_rank, get_tensor_model_parallel_world_size)\\nfrom vllm.model_executor.parallel_utils.tensor_parallel import (\\n    VocabParallelEmbedding, ColumnParallelLinear, RowParallelLinear)\\nfrom vllm.sequence import SamplerOutput\\n\\nKVCache = Tuple[torch.Tensor, torch.Tensor]\\n\\n\\ndef _get_alibi_slopes(total_num_heads: int) -> torch.Tensor:\\n    closest_power_of_2 = 2**math.floor(math.log2(total_num_heads))\\n    base = torch.tensor(\\n        2**(-(2**-(math.log2(closest_power_of_2) - 3))),\\n        dtype=torch.float32,\\n    )\\n    powers = torch.arange(1, 1 + closest_power_of_2, dtype=torch.int32)\\n    slopes = torch.pow(base, powers)\\n\\n    if closest_power_of_2 != total_num_heads:\\n        extra_base = torch.tensor(\\n            2**(-(2**-(math.log2(2 * closest_power_of_2) - 3))),\\n            dtype=torch.float32,\\n        )\\n        num_remaining_heads = min(closest_power_of_2,\\n                                  total_num_heads - closest_power_of_2)\\n        extra_powers = torch.arange(start=1,\\n                                    end=1 + 2 * num_remaining_heads,\\n                                    step=2,\\n                                    dtype=torch.int32)\\n        slopes = torch.cat(\\n            [slopes, torch.pow(extra_base, extra_powers)], dim=0)\\n    return slopes\\n\\n\\nclass BloomAttention(nn.Module):\\n\\n    def __init__(self, config: BloomConfig):\\n        super().__init__()\\n        self.hidden_size = config.hidden_size\\n        self.total_num_heads = config.n_head\\n        self.head_dim = self.hidden_size // self.total_num_heads\\n        assert self.head_dim * self.total_num_heads == self.hidden_size\\n\\n        tp_world_size = get_tensor_model_parallel_world_size()\\n        assert self.total_num_heads % tp_world_size == 0\\n        self.num_heads = self.total_num_heads // tp_world_size\\n\\n        self.query_key_value = ColumnParallelLinear(\\n            self.hidden_size,\\n            3 * self.hidden_size,\\n            bias=True,\\n            gather_output=False,\\n            perform_initialization=False,\\n        )\\n        self.dense = RowParallelLinear(\\n            self.hidden_size,\\n            self.hidden_size,\\n            bias=True,\\n            input_is_parallel=True,\\n            perform_initialization=False,\\n        )\\n\\n        # Create the alibi slopes and slice them.\\n        tp_rank = get_tensor_model_parallel_rank()\\n        head_start = tp_rank * self.num_heads\\n        head_end = (tp_rank + 1) * self.num_heads\\n        alibi_slopes = _get_alibi_slopes(self.total_num_heads)\\n        alibi_slopes = alibi_slopes[head_start:head_end].tolist()\\n\\n        scaling = self.head_dim**-0.5\\n        self.attn = PagedAttentionWithALiBi(self.num_heads, self.head_dim,\\n                                            scaling, alibi_slopes)\\n\\n    def forward(\\n        self,\\n        position_ids: torch.Tensor,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        del position_ids  # Unused.\\n        qkv, _ = self.query_key_value(hidden_states)\\n        q, k, v = qkv.chunk(chunks=3, dim=-1)\\n        k_cache, v_cache = kv_cache\\n        attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata,\\n                                cache_event)\\n        output, _ = self.dense(attn_output)\\n        return output\\n\\n\\nclass BloomMLP(nn.Module):\\n\\n    def __init__(self, config: BloomConfig):\\n        super().__init__()\\n        hidden_size = config.hidden_size\\n        self.dense_h_to_4h = ColumnParallelLinear(hidden_size,\\n                                                  4 * hidden_size,\\n                                                  gather_output=False,\\n                                                  perform_initialization=False)\\n        self.act = get_act_fn(\"gelu\")\\n        self.dense_4h_to_h = RowParallelLinear(4 * hidden_size,\\n                                               hidden_size,\\n                                               input_is_parallel=True,\\n                                               perform_initialization=False)\\n\\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\\n        x, _ = self.dense_h_to_4h(x)\\n        x = self.act(x)\\n        x, _ = self.dense_4h_to_h(x)\\n        return x\\n\\n\\nclass BloomBlock(nn.Module):\\n\\n    def __init__(self, config: BloomConfig):\\n        super().__init__()\\n        hidden_size = config.hidden_size\\n\\n        self.input_layernorm = nn.LayerNorm(hidden_size,\\n                                            eps=config.layer_norm_epsilon)\\n        self.self_attention = BloomAttention(config)\\n        self.post_attention_layernorm = nn.LayerNorm(\\n            hidden_size, eps=config.layer_norm_epsilon)\\n        self.mlp = BloomMLP(config)\\n        self.apply_residual_connection_post_layernorm = (\\n            config.apply_residual_connection_post_layernorm)\\n\\n    def forward(\\n        self,\\n        position_ids: torch.Tensor,\\n        hidden_states: torch.Tensor,\\n        kv_cache: KVCache,\\n        input_metadata: InputMetadata,\\n        cache_event: Optional[torch.cuda.Event],\\n    ) -> torch.Tensor:\\n        # Layer norm at the beginning of the transformer layer.\\n        layernorm_output = self.input_layernorm(hidden_states)\\n\\n        # Layer norm post the self attention.\\n        if self.apply_residual_connection_post_layernorm:\\n            residual = layernorm_output\\n        else:\\n            residual = hidden_states\\n\\n        # Self attention.\\n        attention_output = self.self_attention(\\n            position_ids=position_ids,\\n            hidden_states=layernorm_output,\\n            kv_cache=kv_cache,\\n            input_metadata=input_metadata,\\n            cache_event=cache_event,\\n        )\\n        attention_output = attention_output + residual\\n        layernorm_output = self.post_attention_layernorm(attention_output)\\n\\n        # Get residual\\n        if self.apply_residual_connection_post_layernorm:\\n            residual = layernorm_output\\n        else:\\n            residual = attention_output\\n\\n        # MLP.\\n        output = self.mlp(layernorm_output) + residual\\n        return output\\n\\n\\nclass BloomModel(nn.Module):\\n\\n    def __init__(self, config: BloomConfig):\\n        super().__init__()\\n        self.embed_dim = config.hidden_size\\n\\n        # Embedding + LN Embedding\\n        self.word_embeddings = VocabParallelEmbedding(\\n            config.vocab_size, self.embed_dim, perform_initialization=False)\\n        self.word_embeddings_layernorm = nn.LayerNorm(\\n            self.embed_dim, eps=config.layer_norm_epsilon)\\n\\n        # Transformer blocks\\n        self.h = nn.ModuleList(\\n            [BloomBlock(config) for _ in range(config.num_hidden_layers)])\\n\\n        # Final Layer Norm\\n        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        position_ids: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> torch.Tensor:\\n        hidden_states = self.word_embeddings(input_ids)\\n        hidden_states = self.word_embeddings_layernorm(hidden_states)\\n        for i in range(len(self.h)):\\n            if cache_events is None:\\n                cache_event = None\\n            else:\\n                cache_event = cache_events[i]\\n            layer = self.h[i]\\n            hidden_states = layer(\\n                position_ids,\\n                hidden_states,\\n                kv_caches[i],\\n                input_metadata,\\n                cache_event,\\n            )\\n        hidden_states = self.ln_f(hidden_states)\\n        return hidden_states\\n\\n\\nclass BloomForCausalLM(nn.Module):\\n\\n    def __init__(self, config: BloomConfig):\\n        super().__init__()\\n        self.config = config\\n        self.transformer = BloomModel(config)\\n        # TODO(zhuohan): create a new weight after implementing pipeline\\n        #                parallelism\\n        self.lm_head_weight = self.transformer.word_embeddings.weight\\n        self.sampler = Sampler(config.vocab_size)\\n\\n    def forward(\\n        self,\\n        input_ids: torch.Tensor,\\n        positions: torch.Tensor,\\n        kv_caches: List[KVCache],\\n        input_metadata: InputMetadata,\\n        cache_events: Optional[List[torch.cuda.Event]],\\n    ) -> SamplerOutput:\\n        hidden_states = self.transformer(input_ids, positions, kv_caches,\\n                                         input_metadata, cache_events)\\n        next_tokens = self.sampler(self.lm_head_weight, hidden_states,\\n                                   input_metadata)\\n        return next_tokens\\n\\n    _column_parallel_weights = [\\n        \"word_embeddings.weight\", \"dense_h_to_4h.weight\", \"dense_h_to_4h.bias\"\\n    ]\\n    _row_parallel_weights = [\"dense.weight\", \"dense_4h_to_h.weight\"]\\n\\n    def load_weights(self,\\n                     model_name_or_path: str,\\n                     cache_dir: Optional[str] = None,\\n                     load_format: str = \"auto\",\\n                     revision: Optional[str] = None):\\n        tp_rank = get_tensor_model_parallel_rank()\\n        state_dict = self.state_dict()\\n        for name, loaded_weight in hf_model_weights_iterator(\\n                model_name_or_path, cache_dir, load_format, revision):\\n            if name == \"lm_head.weight\":\\n                # Since hidden_states are parallelized, we need to\\n                # load lm_head.weight in parallel.\\n                self._column_parallel_weights.append(name)\\n                # If lm_head is provided, use it instead.\\n                param = self.lm_head_weight\\n            else:\\n                if not name.startswith(\"transformer.\"):\\n                    name = \"transformer.\" + name\\n                param = state_dict[name]\\n\\n            if \"query_key_value\" in name:\\n                # NOTE(woosuk): BLOOM\\'s fused QKV has the shape of\\n                # [num_heads * 3 * head_size, hidden_size], while the\\n                # required shape is [3 * num_heads * head_size, hidden_size].\\n                # Thus, we need weight conversion.\\n                shard_size = param.shape[0]\\n                start = shard_size * tp_rank\\n                end = shard_size * (tp_rank + 1)\\n                loaded_weight = loaded_weight[start:end]\\n\\n                num_heads = self.config.num_attention_heads\\n                hidden_size = self.config.hidden_size\\n                head_size = hidden_size // num_heads\\n                if \"query_key_value.weight\" in name:\\n                    loaded_weight = loaded_weight.view(-1, 3, head_size,\\n                                                       hidden_size)\\n                    loaded_weight = loaded_weight.transpose(0, 1)\\n                    loaded_weight = loaded_weight.reshape(-1, hidden_size)\\n                elif \"query_key_value.bias\" in name:\\n                    loaded_weight = loaded_weight.view(-1, 3, head_size)\\n                    loaded_weight = loaded_weight.transpose(0, 1)\\n                    loaded_weight = loaded_weight.reshape(-1)\\n                else:\\n                    raise ValueError(f\"Unexpected weight name: {name}\")\\n            load_tensor_parallel_weights(param, loaded_weight, name,\\n                                         self._column_parallel_weights,\\n                                         self._row_parallel_weights, tp_rank)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/parallel_utils/parallel_state.py\\n---------\\nContent:\\n# Copyright 2023 The vLLM team.\\n# Adapted from https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/parallel_state.py\\n# Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.\\n\\n\"\"\"Model and data parallel groups.\"\"\"\\n\\nimport torch\\nfrom typing import Optional\\n\\n# Intra-layer model parallel group that the current rank belongs to.\\n_TENSOR_MODEL_PARALLEL_GROUP = None\\n# Inter-layer model parallel group that the current rank belongs to.\\n_PIPELINE_MODEL_PARALLEL_GROUP = None\\n# Model parallel group (both intra- and pipeline) that the current rank belongs to.\\n_MODEL_PARALLEL_GROUP = None\\n# Embedding group.\\n_EMBEDDING_GROUP = None\\n# Position embedding group.\\n_POSITION_EMBEDDING_GROUP = None\\n# Data parallel group that the current rank belongs to.\\n_DATA_PARALLEL_GROUP = None\\n\\n_VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK = None\\n_VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE = None\\n_PIPELINE_MODEL_PARALLEL_SPLIT_RANK = None\\n\\n# These values enable us to change the mpu sizes on the fly.\\n_MPU_TENSOR_MODEL_PARALLEL_WORLD_SIZE = None\\n_MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE = None\\n_MPU_TENSOR_MODEL_PARALLEL_RANK = None\\n_MPU_PIPELINE_MODEL_PARALLEL_RANK = None\\n\\n# A list of ranks that have a copy of the embedding.\\n_EMBEDDING_GLOBAL_RANKS = None\\n\\n# A list of ranks that have a copy of the position embedding.\\n_POSITION_EMBEDDING_GLOBAL_RANKS = None\\n\\n# A list of global ranks for each pipeline group to ease calculation of the source\\n# rank when broadcasting from the first or last pipeline stage.\\n_PIPELINE_GLOBAL_RANKS = None\\n\\n# A list of global ranks for each data parallel group to ease calculation of the source\\n# rank when broadcasting weights from src to all other data parallel ranks\\n_DATA_PARALLEL_GLOBAL_RANKS = None\\n\\n\\ndef initialize_model_parallel(\\n    tensor_model_parallel_size: int = 1,\\n    pipeline_model_parallel_size: int = 1,\\n    virtual_pipeline_model_parallel_size: Optional[int] = None,\\n    pipeline_model_parallel_split_rank: Optional[int] = None,\\n) -> None:\\n    \"\"\"\\n    Initialize model data parallel groups.\\n\\n    Arguments:\\n        tensor_model_parallel_size: number of GPUs used for tensor model parallelism.\\n        pipeline_model_parallel_size: number of GPUs used for pipeline model parallelism.\\n        virtual_pipeline_model_parallel_size: number of virtual stages (interleaved\\n                                              pipeline).\\n        pipeline_model_parallel_split_rank: for models with both encoder and decoder,\\n                                            rank in pipeline with split point.\\n\\n    Let\\'s say we have a total of 16 GPUs denoted by g0 ... g15 and we\\n    use 2 GPUs to parallelize the model tensor, and 4 GPUs to parallelize\\n    the model pipeline. The present function will\\n    create 8 tensor model-parallel groups, 4 pipeline model-parallel groups\\n    and 8 data-parallel groups as:\\n        8 data_parallel groups:\\n            [g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]\\n        8 tensor model-parallel groups:\\n            [g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]\\n        4 pipeline model-parallel groups:\\n            [g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]\\n    Note that for efficiency, the caller should make sure adjacent ranks\\n    are on the same DGX box. For example if we are using 2 DGX-1 boxes\\n    with a total of 16 GPUs, rank 0 to 7 belong to the first box and\\n    ranks 8 to 15 belong to the second box.\\n    \"\"\"\\n    # Get world size and rank. Ensure some consistencies.\\n    assert torch.distributed.is_initialized()\\n    world_size: int = torch.distributed.get_world_size()\\n\\n    if world_size % (tensor_model_parallel_size * pipeline_model_parallel_size) != 0:\\n        raise RuntimeError(\\n            f\"world_size ({world_size}) is not divisible by tensor_model_parallel_size \"\\n            f\"({tensor_model_parallel_size}) x pipeline_model_parallel_size ({pipeline_model_parallel_size})\"\\n        )\\n\\n    data_parallel_size: int = world_size // (tensor_model_parallel_size *\\n                                             pipeline_model_parallel_size)\\n\\n    num_tensor_model_parallel_groups: int  = world_size // tensor_model_parallel_size\\n    num_pipeline_model_parallel_groups: int = world_size // pipeline_model_parallel_size\\n    num_data_parallel_groups: int = world_size // data_parallel_size\\n\\n    if virtual_pipeline_model_parallel_size is not None:\\n        if not pipeline_model_parallel_size > 2:\\n            raise RuntimeError(\"pipeline-model-parallel size should be greater than 2 with \"\\n                               \"interleaved schedule\")\\n        global _VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK\\n        global _VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE\\n        _VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK = 0\\n        _VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE = virtual_pipeline_model_parallel_size\\n\\n    if pipeline_model_parallel_split_rank is not None:\\n        global _PIPELINE_MODEL_PARALLEL_SPLIT_RANK\\n        _PIPELINE_MODEL_PARALLEL_SPLIT_RANK = pipeline_model_parallel_split_rank\\n\\n    rank = torch.distributed.get_rank()\\n\\n    # Build the data-parallel groups.\\n    global _DATA_PARALLEL_GROUP\\n    global _DATA_PARALLEL_GLOBAL_RANKS\\n    assert _DATA_PARALLEL_GROUP is None, \\'data parallel group is already initialized\\'\\n    all_data_parallel_group_ranks = []\\n    for i in range(pipeline_model_parallel_size):\\n        start_rank = i * num_pipeline_model_parallel_groups\\n        end_rank = (i + 1) * num_pipeline_model_parallel_groups\\n        for j in range(tensor_model_parallel_size):\\n            ranks = range(start_rank + j, end_rank, tensor_model_parallel_size)\\n            all_data_parallel_group_ranks.append(list(ranks))\\n            group = torch.distributed.new_group(ranks)\\n            if rank in ranks:\\n                _DATA_PARALLEL_GROUP = group\\n                _DATA_PARALLEL_GLOBAL_RANKS = ranks\\n\\n    # Build the model-parallel groups.\\n    global _MODEL_PARALLEL_GROUP\\n    assert _MODEL_PARALLEL_GROUP is None, \\'model parallel group is already initialized\\'\\n    for i in range(data_parallel_size):\\n        ranks = [data_parallel_group_ranks[i]\\n                 for data_parallel_group_ranks in all_data_parallel_group_ranks]\\n        group = torch.distributed.new_group(ranks)\\n        if rank in ranks:\\n            _MODEL_PARALLEL_GROUP = group\\n\\n    # Build the tensor model-parallel groups.\\n    global _TENSOR_MODEL_PARALLEL_GROUP\\n    assert _TENSOR_MODEL_PARALLEL_GROUP is None, \\\\\\n        \\'tensor model parallel group is already initialized\\'\\n    for i in range(num_tensor_model_parallel_groups):\\n        ranks = range(i * tensor_model_parallel_size,\\n                      (i + 1) * tensor_model_parallel_size)\\n        group = torch.distributed.new_group(ranks)\\n        if rank in ranks:\\n            _TENSOR_MODEL_PARALLEL_GROUP = group\\n\\n    # Build the pipeline model-parallel groups and embedding groups\\n    # (first and last rank in each pipeline model-parallel group).\\n    global _PIPELINE_MODEL_PARALLEL_GROUP\\n    global _PIPELINE_GLOBAL_RANKS\\n    assert _PIPELINE_MODEL_PARALLEL_GROUP is None, \\\\\\n        \\'pipeline model parallel group is already initialized\\'\\n    global _EMBEDDING_GROUP\\n    global _EMBEDDING_GLOBAL_RANKS\\n    assert _EMBEDDING_GROUP is None, \\'embedding group is already initialized\\'\\n    global _POSITION_EMBEDDING_GROUP\\n    global _POSITION_EMBEDDING_GLOBAL_RANKS\\n    assert _POSITION_EMBEDDING_GROUP is None, \\\\\\n        \\'position embedding group is already initialized\\'\\n    for i in range(num_pipeline_model_parallel_groups):\\n        ranks = range(i, world_size, num_pipeline_model_parallel_groups)\\n        group = torch.distributed.new_group(ranks)\\n        if rank in ranks:\\n            _PIPELINE_MODEL_PARALLEL_GROUP = group\\n            _PIPELINE_GLOBAL_RANKS = ranks\\n        # Setup embedding group (to exchange gradients between\\n        # first and last stages).\\n        if len(ranks) > 1:\\n            embedding_ranks = [ranks[0], ranks[-1]]\\n            position_embedding_ranks = [ranks[0]]\\n            if pipeline_model_parallel_split_rank is not None:\\n                if ranks[pipeline_model_parallel_split_rank] not in embedding_ranks:\\n                    embedding_ranks = [ranks[0],\\n                                       ranks[pipeline_model_parallel_split_rank],\\n                                       ranks[-1]]\\n                if ranks[pipeline_model_parallel_split_rank] not in position_embedding_ranks:\\n                    position_embedding_ranks = [ranks[0],\\n                                       ranks[pipeline_model_parallel_split_rank]]\\n        else:\\n            embedding_ranks = ranks\\n            position_embedding_ranks = ranks\\n\\n        group = torch.distributed.new_group(embedding_ranks)\\n        if rank in embedding_ranks:\\n            _EMBEDDING_GROUP = group\\n        if rank in ranks:\\n            _EMBEDDING_GLOBAL_RANKS = embedding_ranks\\n\\n        group = torch.distributed.new_group(position_embedding_ranks)\\n        if rank in position_embedding_ranks:\\n            _POSITION_EMBEDDING_GROUP = group\\n        if rank in ranks:\\n            _POSITION_EMBEDDING_GLOBAL_RANKS = position_embedding_ranks\\n\\ndef model_parallel_is_initialized():\\n    \"\"\"Check if model and data parallel groups are initialized.\"\"\"\\n    if _TENSOR_MODEL_PARALLEL_GROUP is None or \\\\\\n        _PIPELINE_MODEL_PARALLEL_GROUP is None or \\\\\\n        _DATA_PARALLEL_GROUP is None:\\n        return False\\n    return True\\n\\n\\ndef get_model_parallel_group():\\n    \"\"\"Get the model parallel group the caller rank belongs to.\"\"\"\\n    assert _MODEL_PARALLEL_GROUP is not None, \\\\\\n        \\'model parallel group is not initialized\\'\\n    return _MODEL_PARALLEL_GROUP\\n\\n\\ndef get_tensor_model_parallel_group():\\n    \"\"\"Get the tensor model parallel group the caller rank belongs to.\"\"\"\\n    assert _TENSOR_MODEL_PARALLEL_GROUP is not None, \\\\\\n        \\'intra_layer_model parallel group is not initialized\\'\\n    return _TENSOR_MODEL_PARALLEL_GROUP\\n\\n\\ndef get_pipeline_model_parallel_group():\\n    \"\"\"Get the pipeline model parallel group the caller rank belongs to.\"\"\"\\n    assert _PIPELINE_MODEL_PARALLEL_GROUP is not None, \\\\\\n        \\'pipeline_model parallel group is not initialized\\'\\n    return _PIPELINE_MODEL_PARALLEL_GROUP\\n\\n\\ndef get_data_parallel_group():\\n    \"\"\"Get the data parallel group the caller rank belongs to.\"\"\"\\n    assert _DATA_PARALLEL_GROUP is not None, \\\\\\n        \\'data parallel group is not initialized\\'\\n    return _DATA_PARALLEL_GROUP\\n\\n\\ndef get_embedding_group():\\n    \"\"\"Get the embedding group the caller rank belongs to.\"\"\"\\n    assert _EMBEDDING_GROUP is not None, \\\\\\n        \\'embedding group is not initialized\\'\\n    return _EMBEDDING_GROUP\\n\\n\\ndef get_position_embedding_group():\\n    \"\"\"Get the position embedding group the caller rank belongs to.\"\"\"\\n    assert _POSITION_EMBEDDING_GROUP is not None, \\\\\\n        \\'position embedding group is not initialized\\'\\n    return _POSITION_EMBEDDING_GROUP\\n\\n\\ndef set_tensor_model_parallel_world_size(world_size):\\n    \"\"\"Set the tensor model parallel size\"\"\"\\n    global _MPU_TENSOR_MODEL_PARALLEL_WORLD_SIZE\\n    _MPU_TENSOR_MODEL_PARALLEL_WORLD_SIZE = world_size\\n\\n\\ndef set_pipeline_model_parallel_world_size(world_size):\\n    \"\"\"Set the pipeline model parallel size\"\"\"\\n    global _MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE\\n    _MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE = world_size\\n\\n\\ndef get_tensor_model_parallel_world_size():\\n    \"\"\"Return world size for the tensor model parallel group.\"\"\"\\n    global _MPU_TENSOR_MODEL_PARALLEL_WORLD_SIZE\\n    if _MPU_TENSOR_MODEL_PARALLEL_WORLD_SIZE is not None:\\n        return _MPU_TENSOR_MODEL_PARALLEL_WORLD_SIZE\\n    return torch.distributed.get_world_size(group=get_tensor_model_parallel_group())\\n\\n\\ndef get_pipeline_model_parallel_world_size():\\n    \"\"\"Return world size for the pipeline model parallel group.\"\"\"\\n    global _MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE\\n    if _MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE is not None:\\n        return _MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE\\n    return torch.distributed.get_world_size(group=get_pipeline_model_parallel_group())\\n\\n\\ndef set_tensor_model_parallel_rank(rank):\\n    \"\"\"Set tensor model parallel rank.\"\"\"\\n    global _MPU_TENSOR_MODEL_PARALLEL_RANK\\n    _MPU_TENSOR_MODEL_PARALLEL_RANK = rank\\n\\n\\ndef set_pipeline_model_parallel_rank(rank):\\n    \"\"\"Set pipeline model parallel rank.\"\"\"\\n    global _MPU_PIPELINE_MODEL_PARALLEL_RANK\\n    _MPU_PIPELINE_MODEL_PARALLEL_RANK = rank\\n\\n\\ndef set_pipeline_model_parallel_split_rank(rank):\\n    \"\"\"Set pipeline model parallel split rank.\"\"\"\\n    global _MPU_PIPELINE_MODEL_PARALLEL_SPLIT_RANK\\n    _MPU_PIPELINE_MODEL_PARALLEL_SPLIT_RANK = rank\\n\\n\\ndef get_tensor_model_parallel_rank():\\n    \"\"\"Return my rank for the tensor model parallel group.\"\"\"\\n    global _MPU_TENSOR_MODEL_PARALLEL_RANK\\n    if _MPU_TENSOR_MODEL_PARALLEL_RANK is not None:\\n        return _MPU_TENSOR_MODEL_PARALLEL_RANK\\n    return torch.distributed.get_rank(group=get_tensor_model_parallel_group())\\n\\n\\ndef get_pipeline_model_parallel_rank():\\n    \"\"\"Return my rank for the pipeline model parallel group.\"\"\"\\n    global _MPU_PIPELINE_MODEL_PARALLEL_RANK\\n    if _MPU_PIPELINE_MODEL_PARALLEL_RANK is not None:\\n        return _MPU_PIPELINE_MODEL_PARALLEL_RANK\\n    return torch.distributed.get_rank(group=get_pipeline_model_parallel_group())\\n\\n\\n\\ndef is_pipeline_first_stage(ignore_virtual=False):\\n    \"\"\"Return True if in the first pipeline model-parallel stage, False otherwise.\"\"\"\\n    if not ignore_virtual:\\n        if get_virtual_pipeline_model_parallel_world_size() is not None and \\\\\\n            get_virtual_pipeline_model_parallel_rank() != 0:\\n            return False\\n    return get_pipeline_model_parallel_rank() == 0\\n\\n\\ndef is_pipeline_last_stage(ignore_virtual=False):\\n    \"\"\"Return True if in the last pipeline model-parallel stage, False otherwise.\"\"\"\\n    if not ignore_virtual:\\n        virtual_pipeline_model_parallel_world_size = \\\\\\n            get_virtual_pipeline_model_parallel_world_size()\\n        if virtual_pipeline_model_parallel_world_size is not None and \\\\\\n            get_virtual_pipeline_model_parallel_rank() != (\\n                virtual_pipeline_model_parallel_world_size - 1):\\n            return False\\n    return get_pipeline_model_parallel_rank() == (\\n        get_pipeline_model_parallel_world_size() - 1)\\n\\n\\ndef is_rank_in_embedding_group(ignore_virtual=False):\\n    \"\"\"Return true if current rank is in embedding group, False otherwise.\"\"\"\\n    rank = torch.distributed.get_rank()\\n    global _EMBEDDING_GLOBAL_RANKS\\n    if ignore_virtual:\\n        return rank in _EMBEDDING_GLOBAL_RANKS\\n    if rank in _EMBEDDING_GLOBAL_RANKS:\\n        if rank == _EMBEDDING_GLOBAL_RANKS[0]:\\n            return is_pipeline_first_stage(ignore_virtual=False)\\n        elif rank == _EMBEDDING_GLOBAL_RANKS[-1]:\\n            return is_pipeline_last_stage(ignore_virtual=False)\\n        else:\\n            return True\\n    return False\\n\\n\\ndef is_rank_in_position_embedding_group():\\n    \"\"\"Return true if current rank is in position embedding group, False otherwise.\"\"\"\\n    rank = torch.distributed.get_rank()\\n    global _POSITION_EMBEDDING_GLOBAL_RANKS\\n    return rank in _POSITION_EMBEDDING_GLOBAL_RANKS\\n\\n\\ndef is_pipeline_stage_before_split(rank=None):\\n    \"\"\"Return True if pipeline stage executes encoder block for a model\\n    with both encoder and decoder.\"\"\"\\n    if get_pipeline_model_parallel_world_size() == 1:\\n        return True\\n    if rank is None:\\n        rank = get_pipeline_model_parallel_rank()\\n    global _PIPELINE_MODEL_PARALLEL_SPLIT_RANK\\n    if _PIPELINE_MODEL_PARALLEL_SPLIT_RANK is None:\\n        return True\\n    if rank < _PIPELINE_MODEL_PARALLEL_SPLIT_RANK:\\n        return True\\n    return False\\n\\n\\ndef is_pipeline_stage_after_split(rank=None):\\n    \"\"\"Return True if pipeline stage executes decoder block for a model\\n    with both encoder and decoder.\"\"\"\\n    if get_pipeline_model_parallel_world_size() == 1:\\n        return True\\n    if rank is None:\\n        rank = get_pipeline_model_parallel_rank()\\n    global _PIPELINE_MODEL_PARALLEL_SPLIT_RANK\\n    if _PIPELINE_MODEL_PARALLEL_SPLIT_RANK is None:\\n        return True\\n    if rank >= _PIPELINE_MODEL_PARALLEL_SPLIT_RANK:\\n        return True\\n    return False\\n\\n\\ndef is_pipeline_stage_at_split():\\n    \"\"\"Return true if pipeline stage executes decoder block and next\\n    stage executes encoder block for a model with both encoder and\\n    decoder.\"\"\"\\n    rank = get_pipeline_model_parallel_rank()\\n    return is_pipeline_stage_before_split(rank) and \\\\\\n            is_pipeline_stage_after_split(rank+1)\\n\\n\\ndef get_virtual_pipeline_model_parallel_rank():\\n    \"\"\"Return the virtual pipeline-parallel rank.\"\"\"\\n    global _VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK\\n    return _VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK\\n\\n\\ndef set_virtual_pipeline_model_parallel_rank(rank):\\n    \"\"\"Set the virtual pipeline-parallel rank.\"\"\"\\n    global _VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK\\n    _VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK = rank\\n\\n\\ndef get_virtual_pipeline_model_parallel_world_size():\\n    \"\"\"Return the virtual pipeline-parallel world size.\"\"\"\\n    global _VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE\\n    return _VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE\\n\\n\\ndef get_tensor_model_parallel_src_rank():\\n    \"\"\"Calculate the global rank corresponding to the first local rank\\n    in the tensor model parallel group.\"\"\"\\n    global_rank = torch.distributed.get_rank()\\n    local_world_size = get_tensor_model_parallel_world_size()\\n    return (global_rank // local_world_size) * local_world_size\\n\\n\\ndef get_data_parallel_src_rank():\\n    \"\"\"Calculate the global rank corresponding to the first local rank\\n    in the data parallel group.\"\"\"\\n    assert _DATA_PARALLEL_GLOBAL_RANKS is not None, \\\\\\n        \"Data parallel group is not initialized\"\\n    return _DATA_PARALLEL_GLOBAL_RANKS[0]\\n\\n\\ndef get_pipeline_model_parallel_first_rank():\\n    \"\"\"Return the global rank of the first process in the pipeline for the\\n    current tensor parallel group\"\"\"\\n    assert _PIPELINE_GLOBAL_RANKS is not None, \\\\\\n        \"Pipeline parallel group is not initialized\"\\n    return _PIPELINE_GLOBAL_RANKS[0]\\n\\n\\ndef get_pipeline_model_parallel_last_rank():\\n    \"\"\"Return the global rank of the last process in the pipeline for the\\n    current tensor parallel group\"\"\"\\n    assert _PIPELINE_GLOBAL_RANKS is not None, \\\\\\n        \"Pipeline parallel group is not initialized\"\\n    last_rank_local = get_pipeline_model_parallel_world_size() - 1\\n    return _PIPELINE_GLOBAL_RANKS[last_rank_local]\\n\\n\\ndef get_pipeline_model_parallel_next_rank():\\n    \"\"\"Return the global rank that follows the caller in the pipeline\"\"\"\\n    assert _PIPELINE_GLOBAL_RANKS is not None, \\\\\\n        \"Pipeline parallel group is not initialized\"\\n    rank_in_pipeline = get_pipeline_model_parallel_rank()\\n    world_size = get_pipeline_model_parallel_world_size()\\n    return _PIPELINE_GLOBAL_RANKS[(rank_in_pipeline + 1) % world_size]\\n\\n\\ndef get_pipeline_model_parallel_prev_rank():\\n    \"\"\"Return the global rank that preceeds the caller in the pipeline\"\"\"\\n    assert _PIPELINE_GLOBAL_RANKS is not None, \\\\\\n        \"Pipeline parallel group is not initialized\"\\n    rank_in_pipeline = get_pipeline_model_parallel_rank()\\n    world_size = get_pipeline_model_parallel_world_size()\\n    return _PIPELINE_GLOBAL_RANKS[(rank_in_pipeline - 1) % world_size]\\n\\n\\ndef get_data_parallel_world_size():\\n    \"\"\"Return world size for the data parallel group.\"\"\"\\n    return torch.distributed.get_world_size(group=get_data_parallel_group())\\n\\n\\ndef get_data_parallel_rank():\\n    \"\"\"Return my rank for the data parallel group.\"\"\"\\n    return torch.distributed.get_rank(group=get_data_parallel_group())\\n\\ndef destroy_model_parallel():\\n    \"\"\"Set the groups to none.\"\"\"\\n    global _MODEL_PARALLEL_GROUP\\n    _MODEL_PARALLEL_GROUP = None\\n    global _TENSOR_MODEL_PARALLEL_GROUP\\n    _TENSOR_MODEL_PARALLEL_GROUP = None\\n    global _PIPELINE_MODEL_PARALLEL_GROUP\\n    _PIPELINE_MODEL_PARALLEL_GROUP = None\\n    global _DATA_PARALLEL_GROUP\\n    _DATA_PARALLEL_GROUP = None\\n    global _EMBEDDING_GROUP\\n    _EMBEDDING_GROUP = None\\n    global _POSITION_EMBEDDING_GROUP\\n    _POSITION_EMBEDDING_GROUP = None\\n    global _VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK\\n    _VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK = None\\n    global _VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE\\n    _VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE = None\\n    global _MPU_TENSOR_MODEL_PARALLEL_WORLD_SIZE\\n    _MPU_TENSOR_MODEL_PARALLEL_WORLD_SIZE = None\\n    global _MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE\\n    _MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE = None\\n    global _MPU_TENSOR_MODEL_PARALLEL_RANK\\n    _MPU_TENSOR_MODEL_PARALLEL_RANK = None\\n    global _MPU_PIPELINE_MODEL_PARALLEL_RANK\\n    _MPU_PIPELINE_MODEL_PARALLEL_RANK = None\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/parallel_utils/__init__.py\\n---------\\nContent:\\nimport vllm.model_executor.parallel_utils.parallel_state\\nimport vllm.model_executor.parallel_utils.tensor_parallel\\n\\n__all__ = [\\n    \"parallel_state\",\\n    \"tensor_parallel\",\\n]\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/parallel_utils/tensor_parallel/__init__.py\\n---------\\nContent:\\nfrom .layers import (\\n    ColumnParallelLinear,\\n    RowParallelLinear,\\n    VocabParallelEmbedding,\\n    set_tensor_model_parallel_attributes,\\n    set_defaults_if_not_set_tensor_model_parallel_attributes,\\n    copy_tensor_model_parallel_attributes,\\n    param_is_not_tensor_parallel_duplicate,\\n)\\n\\nfrom .mappings import (\\n    copy_to_tensor_model_parallel_region,\\n    gather_from_tensor_model_parallel_region,\\n    gather_from_sequence_parallel_region,\\n    reduce_from_tensor_model_parallel_region,\\n    scatter_to_tensor_model_parallel_region,\\n    scatter_to_sequence_parallel_region,\\n)\\n\\nfrom .random import (\\n    get_cuda_rng_tracker,\\n    model_parallel_cuda_manual_seed,\\n)\\n\\nfrom .utils import (\\n    split_tensor_along_last_dim,\\n)\\n\\n__all__ = [\\n    #layers.py\\n    \"ColumnParallelLinear\",\\n    \"RowParallelLinear\",\\n    \"VocabParallelEmbedding\",\\n    \"set_tensor_model_parallel_attributes\",\\n    \"set_defaults_if_not_set_tensor_model_parallel_attributes\",\\n    \"copy_tensor_model_parallel_attributes\",\\n    \"param_is_not_tensor_parallel_duplicate\",\\n    # mappings.py\\n    \"copy_to_tensor_model_parallel_region\",\\n    \"gather_from_tensor_model_parallel_region\",\\n    \"gather_from_sequence_parallel_region\",\\n    \"reduce_from_tensor_model_parallel_region\",\\n    \"scatter_to_tensor_model_parallel_region\",\\n    \"scatter_to_sequence_parallel_region\",\\n    # random.py\\n    \"get_cuda_rng_tracker\",\\n    \"model_parallel_cuda_manual_seed\",\\n    # utils.py\\n    \"split_tensor_along_last_dim\",\\n]\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/parallel_utils/tensor_parallel/random.py\\n---------\\nContent:\\n# Copyright 2023 The vLLM team.\\n# Adapted from https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/tensor_parallel/random.py\\n# Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.\\n\\n# Parts of the code here are adapted from PyTorch\\n# repo: https://github.com/pytorch/pytorch\\n\\nimport contextlib\\n\\nimport torch\\nfrom torch import _C\\nfrom torch.cuda import _lazy_call, device as device_ctx_manager\\n\\nfrom vllm.model_executor.parallel_utils.parallel_state import (\\n    get_tensor_model_parallel_rank,\\n)\\n\\n# Default name for the model parallel rng tracker.\\n_MODEL_PARALLEL_RNG_TRACKER_NAME = \\'model-parallel-rng\\'\\n\\n\\ndef _set_cuda_rng_state(new_state, device=-1):\\n    \"\"\"Sets the random number generator state of the current GPU.\\n\\n    Argumentss:\\n        new_state (torch.ByteTensor): The desired state\\n    This function is adapted from PyTorch repo (torch.cuda.set_rng_state)\\n    with a single change: the input state is not cloned. Cloning caused\\n    major performance issues for +4 GPU cases.\\n    \"\"\"\\n    if hasattr(_C, \\'_cuda_setRNGState\\') and callable(_C._cuda_setRNGState):\\n        # older PyTorch\\n        def cb():\\n            with device_ctx_manager(device):\\n                _C._cuda_setRNGState(new_state)\\n    else:\\n        # newer PyTorch\\n        if device == -1:\\n            device = torch.device(\\'cuda\\')\\n        elif isinstance(device, str):\\n            device = torch.device(device)\\n        elif isinstance(device, int):\\n            device = torch.device(\\'cuda\\', device)\\n\\n        def cb():\\n            idx = device.index\\n            if idx is None:\\n                idx = torch.cuda.current_device()\\n            default_generator = torch.cuda.default_generators[idx]\\n            default_generator.set_state(new_state)\\n\\n    _lazy_call(cb)\\n\\n\\n\\nclass CudaRNGStatesTracker:\\n    \"\"\"Tracker for the cuda RNG states.\\n\\n    Using the `add` method, a cuda rng state is initialized based on\\n    the input `seed` and is assigned to `name`. Later, by forking the\\n    rng state, we can perform operations and return to our starting\\n    cuda state.\\n    \"\"\"\\n\\n    def __init__(self):\\n        # Map from a string name to the cuda rng state.\\n        self.states_ = {}\\n        # Seeds are just for book keeping and ensure no seed is set twice.\\n        self.seeds_ = set()\\n\\n    def reset(self):\\n        \"\"\"Set to the initial state (no tracker).\"\"\"\\n        self.states_ = {}\\n        self.seeds_ = set()\\n\\n    def get_states(self):\\n        \"\"\"Get rng states. Copy the dictionary so we have direct\\n        pointers to the states, not just a pointer to the dictionary.\"\"\"\\n        states = {}\\n        for name in self.states_:\\n            states[name] = self.states_[name]\\n        return states\\n\\n    def set_states(self, states):\\n        \"\"\"Set the rng states. For efficiency purposes, we do not check\\n        the size of seed for compatibility.\"\"\"\\n        self.states_ = states\\n\\n    def add(self, name, seed):\\n        \"\"\"Track the rng state.\"\"\"\\n        # Check seed is not already used.\\n        if seed in self.seeds_:\\n            raise Exception(\\'seed {} already exists\\'.format(seed))\\n        self.seeds_.add(seed)\\n        # Check that state is not already defined.\\n        if name in self.states_:\\n            raise Exception(\\'cuda rng state {} already exists\\'.format(name))\\n        # Get the current rng state.\\n        orig_rng_state = torch.cuda.get_rng_state()\\n        # Set the new state and store it.\\n        torch.cuda.manual_seed(seed)\\n        self.states_[name] = torch.cuda.get_rng_state()\\n        # Reset rng state to what it was.\\n        _set_cuda_rng_state(orig_rng_state)\\n\\n    @contextlib.contextmanager\\n    def fork(self, name=_MODEL_PARALLEL_RNG_TRACKER_NAME):\\n        \"\"\"Fork the cuda rng state, perform operations, and exit with\\n        the original state.\"\"\"\\n        # Check if we have added the state\\n        if name not in self.states_:\\n            raise Exception(\\'cuda rng state {} is not added\\'.format(name))\\n        # Store current rng state.\\n        orig_cuda_rng_state = torch.cuda.get_rng_state()\\n        # Set rng state to the desired one\\n        _set_cuda_rng_state(self.states_[name])\\n        # Do the stuff we wanted to do.\\n        try:\\n            yield\\n        finally:\\n            # Update the current rng state for later use.\\n            self.states_[name] = torch.cuda.get_rng_state()\\n            # And set the state to the original state we started with.\\n            _set_cuda_rng_state(orig_cuda_rng_state)\\n\\n\\n# RNG tracker object.\\n_CUDA_RNG_STATE_TRACKER = CudaRNGStatesTracker()\\n\\n\\ndef get_cuda_rng_tracker():\\n    \"\"\"Get cuda rng tracker.\"\"\"\\n    return _CUDA_RNG_STATE_TRACKER\\n\\n\\ndef model_parallel_cuda_manual_seed(seed):\\n    \"\"\"Initialize model parallel cuda seed.\\n\\n    This function should be called after the model parallel is\\n    initialized. Also, no torch.cuda.manual_seed should be called\\n    after this function. Basically, this is replacement for that\\n    function.\\n    Two set of RNG states are tracked:\\n        default state: This is for data parallelism and is the same among a\\n                       set of model parallel GPUs but different across\\n                       different model paralle groups. This is used for\\n                       example for dropout in the non-tensor-model-parallel regions.\\n        tensor-model-parallel state: This state is different among a set of model\\n                              parallel GPUs, but the same across data parallel\\n                              groups. This is used for example for dropout in\\n                              model parallel regions.\\n    \"\"\"\\n    # 2718 is just for fun and any POSITIVE value will work.\\n    offset = seed + 2718\\n    tensor_model_parallel_seed = offset + get_tensor_model_parallel_rank()\\n    # Data parallel gets the original seed.\\n    data_parallel_seed = seed\\n\\n    _CUDA_RNG_STATE_TRACKER.reset()\\n    # Set the default state.\\n    torch.cuda.manual_seed(data_parallel_seed)\\n    # and model parallel state.\\n    _CUDA_RNG_STATE_TRACKER.add(_MODEL_PARALLEL_RNG_TRACKER_NAME,\\n                                tensor_model_parallel_seed)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/parallel_utils/tensor_parallel/utils.py\\n---------\\nContent:\\n# Copyright 2023 The vLLM team.\\n# Adapted from https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/tensor_parallel/utils.py\\n# Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.\\n\\nimport torch\\nfrom typing import List, Sequence\\n\\ndef ensure_divisibility(numerator, denominator):\\n    \"\"\"Ensure that numerator is divisible by the denominator.\"\"\"\\n    assert numerator % denominator == 0, \"{} is not divisible by {}\".format(\\n        numerator, denominator\\n    )\\n\\n\\ndef divide(numerator, denominator):\\n    \"\"\"Ensure that numerator is divisible by the denominator and return\\n    the division value.\"\"\"\\n    ensure_divisibility(numerator, denominator)\\n    return numerator // denominator\\n\\n\\ndef split_tensor_along_last_dim(\\n    tensor: torch.Tensor,\\n    num_partitions: int,\\n    contiguous_split_chunks: bool = False,\\n) -> List[torch.Tensor]:\\n    \"\"\" Split a tensor along its last dimension.\\n\\n        Arguments:\\n            tensor: input tensor.\\n            num_partitions: number of partitions to split the tensor\\n            contiguous_split_chunks: If True, make each chunk contiguous\\n                                     in memory.\\n\\n        Returns:\\n            A list of Tensors\\n    \"\"\"\\n    # Get the size and dimension.\\n    last_dim = tensor.dim() - 1\\n    last_dim_size = divide(tensor.size()[last_dim], num_partitions)\\n    # Split.\\n    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)\\n    # Note: torch.split does not create contiguous tensors by default.\\n    if contiguous_split_chunks:\\n        return tuple(chunk.contiguous() for chunk in tensor_list)\\n\\n    return tensor_list\\n\\n\\nclass VocabUtility:\\n    \"\"\" Split the vocabulary into `world_size` chunks and return the first\\n        and last index of the vocabulary belonging to the `rank`\\n        partition: Note that indices in [fist, last)\\n\\n    \"\"\"\\n\\n    @staticmethod\\n    def vocab_range_from_per_partition_vocab_size(\\n        per_partition_vocab_size: int, rank, world_size: int\\n    ) -> Sequence[int]:\\n        index_f = rank * per_partition_vocab_size\\n        index_l = index_f + per_partition_vocab_size\\n        return index_f, index_l\\n\\n    @staticmethod\\n    def vocab_range_from_global_vocab_size(global_vocab_size: int, rank: int, world_size: int) -> Sequence[int]:\\n        per_partition_vocab_size = divide(global_vocab_size, world_size)\\n        return VocabUtility.vocab_range_from_per_partition_vocab_size(\\n            per_partition_vocab_size, rank, world_size\\n        )\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/parallel_utils/tensor_parallel/layers.py\\n---------\\nContent:\\n# Copyright 2023 The vLLM team.\\n# Adapted from https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/tensor_parallel/layers.py\\n# Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.\\n\\n# Parts of the code here are adapted from PyTorch\\n# repo: https://github.com/pytorch/pytorch\\nfrom typing import Optional\\n\\nimport torch\\nimport torch.nn.functional as F\\nimport torch.nn.init as init\\nfrom torch.nn.parameter import Parameter\\n\\nfrom vllm.model_executor.parallel_utils.parallel_state import (\\n    get_tensor_model_parallel_rank,\\n    get_tensor_model_parallel_world_size,\\n)\\nfrom .mappings import (\\n    gather_from_tensor_model_parallel_region,\\n    reduce_from_tensor_model_parallel_region,\\n    scatter_to_tensor_model_parallel_region,\\n)\\n\\nfrom .utils import (\\n    divide,\\n    VocabUtility,\\n)\\n\\n_MODEL_PARALLEL_ATTRIBUTE_DEFAULTS = {\\'tensor_model_parallel\\': False,\\n                                      \\'partition_dim\\': -1,\\n                                      \\'partition_stride\\': 1}\\n\\ndef param_is_not_tensor_parallel_duplicate(param):\\n    return (hasattr(param, \\'tensor_model_parallel\\') and\\n            param.tensor_model_parallel) or (\\n                get_tensor_model_parallel_rank() == 0)\\n\\n\\ndef set_tensor_model_parallel_attributes(tensor, is_parallel, dim, stride):\\n    # Make sure the attributes are not set.\\n    for attribute in _MODEL_PARALLEL_ATTRIBUTE_DEFAULTS:\\n        assert not hasattr(tensor, attribute)\\n    # Set the attributes.\\n    setattr(tensor, \\'tensor_model_parallel\\', is_parallel)\\n    setattr(tensor, \\'partition_dim\\', dim)\\n    setattr(tensor, \\'partition_stride\\', stride)\\n\\n\\ndef set_defaults_if_not_set_tensor_model_parallel_attributes(tensor):\\n    def maybe_set(attribute, value):\\n        if not hasattr(tensor, attribute):\\n            setattr(tensor, attribute, value)\\n    for attribute in _MODEL_PARALLEL_ATTRIBUTE_DEFAULTS:\\n        maybe_set(attribute, _MODEL_PARALLEL_ATTRIBUTE_DEFAULTS[attribute])\\n\\n\\ndef copy_tensor_model_parallel_attributes(destination_tensor, source_tensor):\\n    def maybe_copy(attribute):\\n        if hasattr(source_tensor, attribute):\\n            setattr(destination_tensor, attribute,\\n                    getattr(source_tensor, attribute))\\n    for attribute in _MODEL_PARALLEL_ATTRIBUTE_DEFAULTS:\\n        maybe_copy(attribute)\\n\\n\\nclass VocabParallelEmbedding(torch.nn.Module):\\n    \"\"\"Embedding parallelized in the vocabulary dimension.\\n\\n    This is mainly adapted from torch.nn.Embedding and all the default\\n    values are kept.\\n    Arguments:\\n        num_embeddings: vocabulary size.\\n        embedding_dim: size of hidden state.\\n\\n    Keyword Arguments:\\n        init_method: method to initialize weights.\\n        params_dtype\\n        use_cpu_initialization\\n        perform_initialization\\n    \"\"\"\\n\\n    def __init__(self, num_embeddings: int, embedding_dim: int, *,\\n                 init_method=init.xavier_normal_,\\n                 params_dtype: torch.dtype=None,\\n                 use_cpu_initialization: bool=False,\\n                 perform_initialization: bool=False):\\n        super(VocabParallelEmbedding, self).__init__()\\n        assert not perform_initialization\\n        assert not use_cpu_initialization\\n\\n        # Keep the input dimensions.\\n        self.num_embeddings = num_embeddings\\n        self.embedding_dim = embedding_dim\\n        if params_dtype is None:\\n            params_dtype = torch.get_default_dtype()\\n\\n        # Set the defaults for compatibility.\\n        self.padding_idx = None\\n        self.max_norm = None\\n        self.norm_type = 2.\\n        self.scale_grad_by_freq = False\\n        self.sparse = False\\n        self._weight = None\\n        self.tensor_model_parallel_size = get_tensor_model_parallel_world_size()\\n        # Divide the weight matrix along the vocaburaly dimension.\\n        self.vocab_start_index, self.vocab_end_index = \\\\\\n            VocabUtility.vocab_range_from_global_vocab_size(\\n                self.num_embeddings, get_tensor_model_parallel_rank(),\\n                self.tensor_model_parallel_size)\\n        self.num_embeddings_per_partition = self.vocab_end_index - \\\\\\n            self.vocab_start_index\\n\\n        self.weight = Parameter(torch.empty(\\n            self.num_embeddings_per_partition, self.embedding_dim,\\n            device=torch.cuda.current_device(), dtype=params_dtype))\\n\\n    def forward(self, input_):\\n        if self.tensor_model_parallel_size > 1:\\n            # Build the mask.\\n            input_mask = (input_ < self.vocab_start_index) | \\\\\\n                         (input_ >= self.vocab_end_index)\\n            # Mask the input.\\n            masked_input = input_.clone() - self.vocab_start_index\\n            masked_input[input_mask] = 0\\n        else:\\n            masked_input = input_\\n            # Get the embeddings.\\n        output_parallel = F.embedding(masked_input, self.weight,\\n                                      self.padding_idx, self.max_norm,\\n                                      self.norm_type, self.scale_grad_by_freq,\\n                                      self.sparse)\\n        # Mask the output embedding.\\n        if self.tensor_model_parallel_size > 1:\\n            output_parallel[input_mask, :] = 0.0\\n        # Reduce across all the model parallel GPUs.\\n        output = reduce_from_tensor_model_parallel_region(output_parallel)\\n        return output\\n\\n\\nclass ColumnParallelLinear(torch.nn.Module):\\n    \"\"\"Linear layer with column parallelism.\\n\\n    The linear layer is defined as Y = XA + b. A is parallelized along\\n    its second dimension as A = [A_1, ..., A_p].\\n\\n    Arguments:\\n        input_size: first dimension of matrix A.\\n        output_size: second dimension of matrix A.\\n\\n    Keyword Arguments\\n        bias: If true, add bias\\n        gather_output: If true, call all-gather on output and make Y available\\n                       to all GPUs, otherwise, every GPU will have its output\\n                       which is Y_i = XA_i\\n        init_method: method to initialize weights. Note that bias is always set\\n                     to zero.\\n        stride: For the strided linear layers.\\n        keep_master_weight_for_test: This was added for testing and should be\\n                                     set to False. It returns the master weights\\n                                     used for initialization.\\n        skip_bias_add: This was added to enable performance optimations where bias\\n                       can be fused with other elementwise operations. we skip\\n                       adding bias but instead return it.\\n        params_dtype:\\n        use_cpu_initialization:\\n    \"\"\"\\n\\n    def __init__(self, input_size, output_size, *,\\n                 bias=True, gather_output=True,\\n                 init_method=init.xavier_normal_, stride=1,\\n                 keep_master_weight_for_test=False,\\n                 skip_bias_add=False,\\n                 params_dtype=None,\\n                 use_cpu_initialization=False,\\n                 perform_initialization=False,\\n                 quant_config=None,\\n                 ):\\n        super(ColumnParallelLinear, self).__init__()\\n        assert not perform_initialization\\n        assert not use_cpu_initialization\\n\\n        # Keep input parameters\\n        self.input_size = input_size\\n        self.output_size = output_size\\n        self.gather_output = gather_output\\n        # Divide the weight matrix along the last dimension.\\n        self.world_size = get_tensor_model_parallel_world_size()\\n        self.output_size_per_partition = divide(output_size, self.world_size)\\n        self.skip_bias_add = skip_bias_add\\n        self.quant_config = quant_config\\n\\n        if params_dtype is None:\\n            params_dtype = torch.get_default_dtype()\\n\\n        # Parameters.\\n        # Note: torch.nn.functional.linear performs XA^T + b and as a result\\n        # we allocate the transpose.\\n        self.create_weights(params_dtype)\\n\\n        if bias:\\n            self.bias = Parameter(torch.empty(\\n                self.output_size_per_partition,\\n                device=torch.cuda.current_device(),\\n                dtype=params_dtype))\\n            set_tensor_model_parallel_attributes(self.bias, True, 0, stride)\\n            # Always initialize bias to zero.\\n            with torch.no_grad():\\n                self.bias.zero_()\\n        else:\\n            self.register_parameter(\\'bias\\', None)\\n\\n    def create_weights(self, dtype: torch.dtype) -> None:\\n        self.weight = Parameter(torch.empty(\\n            self.output_size_per_partition, self.input_size,\\n            device=torch.cuda.current_device(), dtype=dtype))\\n\\n    def apply_weights(\\n        self,\\n        x: torch.Tensor,\\n        bias: Optional[torch.Tensor],\\n    ) -> torch.Tensor:\\n        return F.linear(x, self.weight, bias)\\n\\n    def forward(self, input_):\\n        \"\"\"Forward of ColumnParallelLinear\\n\\n        Args:\\n            input_: 3D tensor whose order of dimension is [sequence, batch, hidden]\\n\\n        Returns:\\n            - output\\n            - bias\\n        \"\"\"\\n        bias = self.bias if not self.skip_bias_add else None\\n\\n        input_parallel = input_\\n        # Matrix multiply.\\n        output_parallel = self.apply_weights(input_parallel, bias)\\n        if self.gather_output:\\n            # All-gather across the partitions.\\n            output = gather_from_tensor_model_parallel_region(output_parallel)\\n        else:\\n            output = output_parallel\\n        output_bias = self.bias if self.skip_bias_add else None\\n        return output, output_bias\\n\\n\\nclass RowParallelLinear(torch.nn.Module):\\n    \"\"\"Linear layer with row parallelism.\\n\\n    The linear layer is defined as Y = XA + b. A is parallelized along\\n    its first dimension and X along its second dimension as:\\n               -   -\\n              | A_1 |\\n              | .   |\\n          A = | .   |        X = [X_1, ..., X_p]\\n              | .   |\\n              | A_p |\\n               -   -\\n    Arguments:\\n        input_size: first dimension of matrix A.\\n        output_size: second dimension of matrix A.\\n\\n    Keyword Arguments:\\n        bias: If true, add bias. Note that bias is not parallelized.\\n        input_is_parallel: If true, we assume that the input is already\\n                           split across the GPUs and we do not split\\n                           again.\\n        init_method: method to initialize weights. Note that bias is always set\\n                     to zero.\\n        stride: For the strided linear layers.\\n        keep_master_weight_for_test: This was added for testing and should be\\n                                     set to False. It returns the master weights\\n                                     used for initialization.\\n        skip_bias_add: This was added to enable performance optimization where bias\\n                       can be fused with other elementwise operations. We skip\\n                       adding bias but instead return it.\\n        params_dtype:\\n        use_cpu_initialization:\\n        perform_initialization:\\n        reduce_results:\\n    \"\"\"\\n\\n    def __init__(self, input_size, output_size, *,\\n                 bias=True, input_is_parallel=False,\\n                 init_method=init.xavier_normal_, stride=1,\\n                 keep_master_weight_for_test=False,\\n                 skip_bias_add=False,\\n                 params_dtype=None,\\n                 use_cpu_initialization=False,\\n                 perform_initialization=False,\\n                 reduce_results=True,\\n                 quant_config=None,\\n                 ):\\n        super(RowParallelLinear, self).__init__()\\n        assert not perform_initialization\\n        assert not use_cpu_initialization\\n\\n        # Keep input parameters\\n        self.input_size = input_size\\n        self.output_size = output_size\\n        self.input_is_parallel = input_is_parallel\\n        self.reduce_results = reduce_results\\n        if params_dtype is None:\\n            params_dtype = torch.get_default_dtype()\\n\\n        # Divide the weight matrix along the last dimension.\\n        self.world_size = get_tensor_model_parallel_world_size()\\n        self.input_size_per_partition = divide(input_size, self.world_size)\\n        self.skip_bias_add = skip_bias_add\\n        self.quant_config = quant_config\\n\\n        self.create_weights(params_dtype)\\n\\n        if not reduce_results and (bias and not skip_bias_add):\\n            raise ValueError(\"When not reduce the results, adding bias to the \"\\n                             \"results can lead to incorrect results\")\\n\\n        if bias:\\n            self.bias = Parameter(torch.empty(\\n                self.output_size, device=torch.cuda.current_device(),\\n                dtype=params_dtype))\\n\\n            # Always initialize bias to zero.\\n            with torch.no_grad():\\n                self.bias.zero_()\\n        else:\\n            self.register_parameter(\\'bias\\', None)\\n\\n    def create_weights(self, dtype: torch.dtype) -> None:\\n        self.weight = Parameter(torch.empty(\\n                self.output_size, self.input_size_per_partition,\\n                device=torch.cuda.current_device(), dtype=dtype))\\n\\n    def apply_weights(self, x: torch.Tensor) -> torch.Tensor:\\n        return F.linear(x, self.weight)\\n\\n    def forward(self, input_):\\n        \"\"\"Forward of RowParallelLinear\\n\\n        Args:\\n            input_: 3D tensor whose order of dimension is [sequence, batch, hidden]\\n\\n        Returns:\\n            - output\\n            - bias\\n        \"\"\"\\n        # Set up backprop all-reduce.\\n        if self.input_is_parallel:\\n            input_parallel = input_\\n        else:\\n            input_parallel = scatter_to_tensor_model_parallel_region(input_)\\n        # Matrix multiply.\\n        output_parallel = self.apply_weights(input_parallel)\\n        if self.reduce_results and self.world_size > 1:\\n            output_ = reduce_from_tensor_model_parallel_region(output_parallel)\\n        else:\\n            output_ = output_parallel\\n\\n        if not self.skip_bias_add:\\n            output = output_ + self.bias if self.bias is not None else output_\\n            output_bias = None\\n        else:\\n            output = output_\\n            output_bias = self.bias\\n        return output, output_bias\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/model_executor/parallel_utils/tensor_parallel/mappings.py\\n---------\\nContent:\\n# Copyright 2023 The vLLM team.\\n# Adapted from https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/tensor_parallel/mappings.py\\n# Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.\\n\\nimport torch\\n\\nfrom vllm.model_executor.parallel_utils.parallel_state import (\\n    get_tensor_model_parallel_rank,\\n    get_tensor_model_parallel_world_size,\\n    get_tensor_model_parallel_group,\\n)\\nfrom .utils import split_tensor_along_last_dim\\n\\n\\ndef _reduce(input_):\\n    \"\"\"All-reduce the input tensor across model parallel group.\"\"\"\\n\\n    # Bypass the function if we are using only 1 GPU.\\n    if get_tensor_model_parallel_world_size()==1:\\n        return input_\\n\\n    # All-reduce.\\n    torch.distributed.all_reduce(input_, group=get_tensor_model_parallel_group())\\n\\n    return input_\\n\\n\\ndef _split_along_last_dim(input_):\\n    \"\"\"Split the tensor along its last dimension and keep the\\n    corresponding slice.\"\"\"\\n\\n    world_size = get_tensor_model_parallel_world_size()\\n    # Bypass the function if we are using only 1 GPU.\\n    if world_size == 1:\\n        return input_\\n\\n    # Split along last dimension.\\n    input_list = split_tensor_along_last_dim(input_, world_size)\\n\\n    # Note: torch.split does not create contiguous tensors by default.\\n    rank = get_tensor_model_parallel_rank()\\n    output = input_list[rank].contiguous()\\n\\n    return output\\n\\n\\ndef _split_along_first_dim(input_):\\n    \"\"\"Split the tensor along its first dimension and keep the\\n    corresponding slice.\"\"\"\\n\\n    world_size = get_tensor_model_parallel_world_size()\\n    # Bypass the function if we are using only 1 GPU.\\n    if world_size == 1:\\n        return input_\\n\\n    # Split along first dimension.\\n    dim_size = input_.size()[0]\\n    assert dim_size % world_size == 0, \\\\\\n        \"First dimension of the tensor should be divisible by tensor parallel size\"\\n    local_dim_size = dim_size // world_size\\n    rank = get_tensor_model_parallel_rank()\\n    dim_offset = rank * local_dim_size\\n\\n    output = input_[dim_offset:dim_offset+local_dim_size].contiguous()\\n\\n    return output\\n\\n\\ndef _gather_along_last_dim(input_):\\n    \"\"\"Gather tensors and concatinate along the last dimension.\"\"\"\\n\\n    world_size = get_tensor_model_parallel_world_size()\\n    # Bypass the function if we are using only 1 GPU.\\n    if world_size == 1:\\n        return input_\\n\\n    # Size and dimension.\\n    last_dim = input_.dim() - 1\\n    rank = get_tensor_model_parallel_rank()\\n\\n    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]\\n    tensor_list[rank] = input_\\n    torch.distributed.all_gather(tensor_list, input_, group=get_tensor_model_parallel_group())\\n\\n    # Note: torch.cat already creates a contiguous tensor.\\n    output = torch.cat(tensor_list, dim=last_dim).contiguous()\\n\\n    return output\\n\\n\\ndef _gather_along_first_dim(input_):\\n    \"\"\"Gather tensors and concatinate along the first dimension.\"\"\"\\n\\n    world_size = get_tensor_model_parallel_world_size()\\n    # Bypass the function if we are using only 1 GPU.\\n    if world_size == 1:\\n        return input_\\n\\n    dim_size = list(input_.size())\\n    dim_size[0] = dim_size[0] * world_size\\n\\n    output = torch.empty(dim_size, dtype=input_.dtype,\\n                         device=torch.cuda.current_device())\\n    torch.distributed._all_gather_base(output, input_.contiguous(),\\n                                       group=get_tensor_model_parallel_group())\\n\\n    return output\\n\\ndef _reduce_scatter_along_first_dim(input_):\\n    \"\"\"Reduce-scatter the input tensor across model parallel group.\"\"\"\\n    world_size = get_tensor_model_parallel_world_size()\\n    # Bypass the function if we are using only 1 GPU.\\n    if world_size == 1:\\n        return input_\\n\\n    dim_size = list(input_.size())\\n    assert dim_size[0] % world_size == 0, \\\\\\n        \"First dimension of the tensor should be divisible by tensor parallel size\"\\n\\n    dim_size[0] = dim_size[0] // world_size\\n\\n    output = torch.empty(dim_size, dtype=input_.dtype,\\n                         device=torch.cuda.current_device())\\n    torch.distributed._reduce_scatter_base(output, input_.contiguous(),\\n                                           group=get_tensor_model_parallel_group())\\n    return output\\n\\n\\nclass _CopyToModelParallelRegion(torch.autograd.Function):\\n    \"\"\"Pass the input to the model parallel region.\"\"\"\\n\\n    @staticmethod\\n    def symbolic(graph, input_):\\n        return input_\\n\\n    @staticmethod\\n    def forward(ctx, input_):\\n        return input_\\n\\n    @staticmethod\\n    def backward(ctx, grad_output):\\n        return _reduce(grad_output)\\n\\n\\nclass _ReduceFromModelParallelRegion(torch.autograd.Function):\\n    \"\"\"All-reduce the input from the model parallel region.\"\"\"\\n\\n    @staticmethod\\n    def symbolic(graph, input_):\\n        return _reduce(input_)\\n\\n    @staticmethod\\n    def forward(ctx, input_):\\n        return _reduce(input_)\\n\\n    @staticmethod\\n    def backward(ctx, grad_output):\\n        return grad_output\\n\\n\\nclass _ScatterToModelParallelRegion(torch.autograd.Function):\\n    \"\"\"Split the input and keep only the corresponding chuck to the rank.\"\"\"\\n\\n    @staticmethod\\n    def symbolic(graph, input_):\\n        return _split_along_last_dim(input_)\\n\\n    @staticmethod\\n    def forward(ctx, input_):\\n        return _split_along_last_dim(input_)\\n\\n    @staticmethod\\n    def backward(ctx, grad_output):\\n        return _gather_along_last_dim(grad_output)\\n\\n\\nclass _GatherFromModelParallelRegion(torch.autograd.Function):\\n    \"\"\"Gather the input from model parallel region and concatinate.\"\"\"\\n\\n    @staticmethod\\n    def symbolic(graph, input_):\\n        return _gather_along_last_dim(input_)\\n\\n    @staticmethod\\n    def forward(ctx, input_):\\n        return _gather_along_last_dim(input_)\\n\\n    @staticmethod\\n    def backward(ctx, grad_output):\\n        return _split_along_last_dim(grad_output)\\n\\n\\nclass _ScatterToSequenceParallelRegion(torch.autograd.Function):\\n    \"\"\"Split the input and keep only the corresponding chuck to the rank.\"\"\"\\n\\n    @staticmethod\\n    def symbolic(graph, input_):\\n        return _split_along_first_dim(input_)\\n\\n    @staticmethod\\n    def forward(ctx, input_):\\n        return _split_along_first_dim(input_)\\n\\n    @staticmethod\\n    def backward(ctx, grad_output):\\n        return _gather_along_first_dim(grad_output)\\n\\n\\nclass _GatherFromSequenceParallelRegion(torch.autograd.Function):\\n    \"\"\"Gather the input from sequence parallel region and concatinate.\"\"\"\\n\\n    @staticmethod\\n    def symbolic(graph, input_, tensor_parallel_output_grad=True):\\n        return _gather_along_first_dim(input_)\\n\\n    @staticmethod\\n    def forward(ctx, input_, tensor_parallel_output_grad=True):\\n        ctx.tensor_parallel_output_grad = tensor_parallel_output_grad\\n        return _gather_along_first_dim(input_)\\n\\n    @staticmethod\\n    def backward(ctx, grad_output):\\n        tensor_parallel_output_grad = ctx.tensor_parallel_output_grad\\n\\n        # If the computation graph after the gather operation is\\n        # in the tensor parallel mode, output gradients need to reduce\\n        # scattered and whereas if the computation is duplicated,\\n        # output gradients need to be scattered.\\n        if tensor_parallel_output_grad:\\n            return _reduce_scatter_along_first_dim(grad_output), None\\n        else:\\n            return _split_along_first_dim(grad_output), None\\n\\n\\nclass _ReduceScatterToSequenceParallelRegion(torch.autograd.Function):\\n    \"\"\"Reduce scatter the input from the model parallel region.\"\"\"\\n\\n    @staticmethod\\n    def symbolic(graph, input_):\\n        return _reduce_scatter_along_first_dim(input_)\\n\\n    @staticmethod\\n    def forward(ctx, input_):\\n        return _reduce_scatter_along_first_dim(input_)\\n\\n    @staticmethod\\n    def backward(ctx, grad_output):\\n        return _gather_along_first_dim(grad_output)\\n\\n\\n# -----------------\\n# Helper functions.\\n# -----------------\\n\\ndef copy_to_tensor_model_parallel_region(input_):\\n    return _CopyToModelParallelRegion.apply(input_)\\n\\n\\ndef reduce_from_tensor_model_parallel_region(input_):\\n    return _ReduceFromModelParallelRegion.apply(input_)\\n\\n\\ndef scatter_to_tensor_model_parallel_region(input_):\\n    return _ScatterToModelParallelRegion.apply(input_)\\n\\n\\ndef gather_from_tensor_model_parallel_region(input_):\\n    return _GatherFromModelParallelRegion.apply(input_)\\n\\n\\ndef scatter_to_sequence_parallel_region(input_):\\n    return _ScatterToSequenceParallelRegion.apply(input_)\\n\\n\\ndef gather_from_sequence_parallel_region(input_, tensor_parallel_output_grad=True):\\n    return _GatherFromSequenceParallelRegion.apply(input_, tensor_parallel_output_grad)\\n\\n\\ndef reduce_scatter_to_sequence_parallel_region(input_):\\n    return _ReduceScatterToSequenceParallelRegion.apply(input_)\\n\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/entrypoints/__init__.py\\n---------\\nContent:\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/entrypoints/llm.py\\n---------\\nContent:\\nfrom typing import List, Optional, Union\\n\\nfrom tqdm import tqdm\\nfrom transformers import PreTrainedTokenizer, PreTrainedTokenizerFast\\n\\nfrom vllm.engine.arg_utils import EngineArgs\\nfrom vllm.engine.llm_engine import LLMEngine\\nfrom vllm.outputs import RequestOutput\\nfrom vllm.sampling_params import SamplingParams\\nfrom vllm.utils import Counter\\n\\n\\nclass LLM:\\n    \"\"\"An LLM for generating texts from given prompts and sampling parameters.\\n\\n    This class includes a tokenizer, a language model (possibly distributed\\n    across multiple GPUs), and GPU memory space allocated for intermediate\\n    states (aka KV cache). Given a batch of prompts and sampling parameters,\\n    this class generates texts from the model, using an intelligent batching\\n    mechanism and efficient memory management.\\n\\n    NOTE: This class is intended to be used for offline inference. For online\\n    serving, use the `AsyncLLMEngine` class instead.\\n    NOTE: For the comprehensive list of arguments, see `EngineArgs`.\\n\\n    Args:\\n        model: The name or path of a HuggingFace Transformers model.\\n        tokenizer: The name or path of a HuggingFace Transformers tokenizer.\\n        tokenizer_mode: The tokenizer mode. \"auto\" will use the fast tokenizer\\n            if available, and \"slow\" will always use the slow tokenizer.\\n        trust_remote_code: Trust remote code (e.g., from HuggingFace) when\\n            downloading the model and tokenizer.\\n        tensor_parallel_size: The number of GPUs to use for distributed\\n            execution with tensor parallelism.\\n        dtype: The data type for the model weights and activations. Currently,\\n            we support `float32`, `float16`, and `bfloat16`. If `auto`, we use\\n            the `torch_dtype` attribute specified in the model config file.\\n            However, if the `torch_dtype` in the config is `float32`, we will\\n            use `float16` instead.\\n        seed: The seed to initialize the random number generator for sampling.\\n        revision: The specific model version to use. It can be a branch name,\\n            a tag name, or a commit id.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        model: str,\\n        tokenizer: Optional[str] = None,\\n        tokenizer_mode: str = \"auto\",\\n        trust_remote_code: bool = False,\\n        tensor_parallel_size: int = 1,\\n        dtype: str = \"auto\",\\n        seed: int = 0,\\n        **kwargs,\\n    ) -> None:\\n        if \"disable_log_stats\" not in kwargs:\\n            kwargs[\"disable_log_stats\"] = True\\n        engine_args = EngineArgs(\\n            model=model,\\n            tokenizer=tokenizer,\\n            tokenizer_mode=tokenizer_mode,\\n            trust_remote_code=trust_remote_code,\\n            tensor_parallel_size=tensor_parallel_size,\\n            dtype=dtype,\\n            seed=seed,\\n            **kwargs,\\n        )\\n        self.llm_engine = LLMEngine.from_engine_args(engine_args)\\n        self.request_counter = Counter()\\n\\n    def get_tokenizer(\\n            self) -> Union[PreTrainedTokenizer, PreTrainedTokenizerFast]:\\n        return self.llm_engine.tokenizer\\n\\n    def set_tokenizer(\\n        self,\\n        tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],\\n    ) -> None:\\n        self.llm_engine.tokenizer = tokenizer\\n\\n    def generate(\\n        self,\\n        prompts: Optional[Union[str, List[str]]] = None,\\n        sampling_params: Optional[SamplingParams] = None,\\n        prompt_token_ids: Optional[List[List[int]]] = None,\\n        use_tqdm: bool = True,\\n    ) -> List[RequestOutput]:\\n        \"\"\"Generates the completions for the input prompts.\\n\\n        NOTE: This class automatically batches the given prompts, considering\\n        the memory constraint. For the best performance, put all of your prompts\\n        into a single list and pass it to this method.\\n\\n        Args:\\n            prompts: A list of prompts to generate completions for.\\n            sampling_params: The sampling parameters for text generation. If\\n                None, we use the default sampling parameters.\\n            prompt_token_ids: A list of token IDs for the prompts. If None, we\\n                use the tokenizer to convert the prompts to token IDs.\\n            use_tqdm: Whether to use tqdm to display the progress bar.\\n\\n        Returns:\\n            A list of `RequestOutput` objects containing the generated\\n            completions in the same order as the input prompts.\\n        \"\"\"\\n        if prompts is None and prompt_token_ids is None:\\n            raise ValueError(\"Either prompts or prompt_token_ids must be \"\\n                             \"provided.\")\\n        if isinstance(prompts, str):\\n            # Convert a single prompt to a list.\\n            prompts = [prompts]\\n        if prompts is not None and prompt_token_ids is not None:\\n            if len(prompts) != len(prompt_token_ids):\\n                raise ValueError(\"The lengths of prompts and prompt_token_ids \"\\n                                 \"must be the same.\")\\n        if sampling_params is None:\\n            # Use default sampling params.\\n            sampling_params = SamplingParams()\\n\\n        # Add requests to the engine.\\n        if prompts is not None:\\n            num_requests = len(prompts)\\n        else:\\n            num_requests = len(prompt_token_ids)\\n        for i in range(num_requests):\\n            prompt = prompts[i] if prompts is not None else None\\n            if prompt_token_ids is None:\\n                token_ids = None\\n            else:\\n                token_ids = prompt_token_ids[i]\\n            self._add_request(prompt, sampling_params, token_ids)\\n        return self._run_engine(use_tqdm)\\n\\n    def _add_request(\\n        self,\\n        prompt: Optional[str],\\n        sampling_params: SamplingParams,\\n        prompt_token_ids: Optional[List[int]],\\n    ) -> None:\\n        request_id = str(next(self.request_counter))\\n        self.llm_engine.add_request(request_id, prompt, sampling_params,\\n                                    prompt_token_ids)\\n\\n    def _run_engine(self, use_tqdm: bool) -> List[RequestOutput]:\\n        # Initialize tqdm.\\n        if use_tqdm:\\n            num_requests = self.llm_engine.get_num_unfinished_requests()\\n            pbar = tqdm(total=num_requests, desc=\"Processed prompts\")\\n        # Run the engine.\\n        outputs: List[RequestOutput] = []\\n        while self.llm_engine.has_unfinished_requests():\\n            step_outputs = self.llm_engine.step()\\n            for output in step_outputs:\\n                if output.finished:\\n                    outputs.append(output)\\n                    if use_tqdm:\\n                        pbar.update(1)\\n        if use_tqdm:\\n            pbar.close()\\n        # Sort the outputs by request ID.\\n        # This is necessary because some requests may be finished earlier than\\n        # its previous requests.\\n        outputs = sorted(outputs, key=lambda x: int(x.request_id))\\n        return outputs\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/entrypoints/api_server.py\\n---------\\nContent:\\nimport argparse\\nimport json\\nfrom typing import AsyncGenerator\\n\\nfrom fastapi import BackgroundTasks, FastAPI, Request\\nfrom fastapi.responses import JSONResponse, Response, StreamingResponse\\nimport uvicorn\\n\\nfrom vllm.engine.arg_utils import AsyncEngineArgs\\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine\\nfrom vllm.sampling_params import SamplingParams\\nfrom vllm.utils import random_uuid\\n\\nTIMEOUT_KEEP_ALIVE = 5  # seconds.\\nTIMEOUT_TO_PREVENT_DEADLOCK = 1  # seconds.\\napp = FastAPI()\\nengine = None\\n\\n\\n@app.post(\"/generate\")\\nasync def generate(request: Request) -> Response:\\n    \"\"\"Generate completion for the request.\\n\\n    The request should be a JSON object with the following fields:\\n    - prompt: the prompt to use for the generation.\\n    - stream: whether to stream the results or not.\\n    - other fields: the sampling parameters (See `SamplingParams` for details).\\n    \"\"\"\\n    request_dict = await request.json()\\n    prompt = request_dict.pop(\"prompt\")\\n    stream = request_dict.pop(\"stream\", False)\\n    sampling_params = SamplingParams(**request_dict)\\n    request_id = random_uuid()\\n\\n    results_generator = engine.generate(prompt, sampling_params, request_id)\\n\\n    # Streaming case\\n    async def stream_results() -> AsyncGenerator[bytes, None]:\\n        async for request_output in results_generator:\\n            prompt = request_output.prompt\\n            text_outputs = [\\n                prompt + output.text for output in request_output.outputs\\n            ]\\n            ret = {\"text\": text_outputs}\\n            yield (json.dumps(ret) + \"\\\\0\").encode(\"utf-8\")\\n\\n    async def abort_request() -> None:\\n        await engine.abort(request_id)\\n\\n    if stream:\\n        background_tasks = BackgroundTasks()\\n        # Abort the request if the client disconnects.\\n        background_tasks.add_task(abort_request)\\n        return StreamingResponse(stream_results(), background=background_tasks)\\n\\n    # Non-streaming case\\n    final_output = None\\n    async for request_output in results_generator:\\n        if await request.is_disconnected():\\n            # Abort the request if the client disconnects.\\n            await engine.abort(request_id)\\n            return Response(status_code=499)\\n        final_output = request_output\\n\\n    assert final_output is not None\\n    prompt = final_output.prompt\\n    text_outputs = [prompt + output.text for output in final_output.outputs]\\n    ret = {\"text\": text_outputs}\\n    return JSONResponse(ret)\\n\\n\\nif __name__ == \"__main__\":\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\\n    parser.add_argument(\"--port\", type=int, default=8000)\\n    parser = AsyncEngineArgs.add_cli_args(parser)\\n    args = parser.parse_args()\\n\\n    engine_args = AsyncEngineArgs.from_cli_args(args)\\n    engine = AsyncLLMEngine.from_engine_args(engine_args)\\n\\n    uvicorn.run(app,\\n                host=args.host,\\n                port=args.port,\\n                log_level=\"debug\",\\n                timeout_keep_alive=TIMEOUT_KEEP_ALIVE)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/entrypoints/openai/protocol.py\\n---------\\nContent:\\n# Adapted from\\n# https://github.com/lm-sys/FastChat/blob/168ccc29d3f7edc50823016105c024fe2282732a/fastchat/protocol/openai_api_protocol.py\\nimport time\\nfrom typing import Dict, List, Literal, Optional, Union\\n\\nfrom pydantic import BaseModel, Field\\n\\nfrom vllm.utils import random_uuid\\n\\n\\nclass ErrorResponse(BaseModel):\\n    object: str = \"error\"\\n    message: str\\n    type: str\\n    param: Optional[str] = None\\n    code: Optional[str] = None\\n\\n\\nclass ModelPermission(BaseModel):\\n    id: str = Field(default_factory=lambda: f\"modelperm-{random_uuid()}\")\\n    object: str = \"model_permission\"\\n    created: int = Field(default_factory=lambda: int(time.time()))\\n    allow_create_engine: bool = False\\n    allow_sampling: bool = True\\n    allow_logprobs: bool = True\\n    allow_search_indices: bool = False\\n    allow_view: bool = True\\n    allow_fine_tuning: bool = False\\n    organization: str = \"*\"\\n    group: Optional[str] = None\\n    is_blocking: str = False\\n\\n\\nclass ModelCard(BaseModel):\\n    id: str\\n    object: str = \"model\"\\n    created: int = Field(default_factory=lambda: int(time.time()))\\n    owned_by: str = \"vllm\"\\n    root: Optional[str] = None\\n    parent: Optional[str] = None\\n    permission: List[ModelPermission] = Field(default_factory=list)\\n\\n\\nclass ModelList(BaseModel):\\n    object: str = \"list\"\\n    data: List[ModelCard] = Field(default_factory=list)\\n\\n\\nclass UsageInfo(BaseModel):\\n    prompt_tokens: int = 0\\n    total_tokens: int = 0\\n    completion_tokens: Optional[int] = 0\\n\\n\\nclass ChatCompletionRequest(BaseModel):\\n    model: str\\n    messages: Union[str, List[Dict[str, str]]]\\n    temperature: Optional[float] = 0.7\\n    top_p: Optional[float] = 1.0\\n    n: Optional[int] = 1\\n    max_tokens: Optional[int] = 16\\n    stop: Optional[Union[str, List[str]]] = Field(default_factory=list)\\n    stream: Optional[bool] = False\\n    presence_penalty: Optional[float] = 0.0\\n    frequency_penalty: Optional[float] = 0.0\\n    logit_bias: Optional[Dict[str, float]] = None\\n    user: Optional[str] = None\\n    # Additional parameters supported by vLLM\\n    best_of: Optional[int] = None\\n    top_k: Optional[int] = -1\\n    ignore_eos: Optional[bool] = False\\n    use_beam_search: Optional[bool] = False\\n\\n\\nclass CompletionRequest(BaseModel):\\n    model: str\\n    # a string, array of strings, array of tokens, or array of token arrays\\n    prompt: Union[List[int], List[List[int]], str, List[str]]\\n    suffix: Optional[str] = None\\n    max_tokens: Optional[int] = 16\\n    temperature: Optional[float] = 1.0\\n    top_p: Optional[float] = 1.0\\n    n: Optional[int] = 1\\n    stream: Optional[bool] = False\\n    logprobs: Optional[int] = None\\n    echo: Optional[bool] = False\\n    stop: Optional[Union[str, List[str]]] = Field(default_factory=list)\\n    presence_penalty: Optional[float] = 0.0\\n    frequency_penalty: Optional[float] = 0.0\\n    best_of: Optional[int] = None\\n    logit_bias: Optional[Dict[str, float]] = None\\n    user: Optional[str] = None\\n    # Additional parameters supported by vLLM\\n    top_k: Optional[int] = -1\\n    ignore_eos: Optional[bool] = False\\n    use_beam_search: Optional[bool] = False\\n\\n\\nclass LogProbs(BaseModel):\\n    text_offset: List[int] = Field(default_factory=list)\\n    token_logprobs: List[Optional[float]] = Field(default_factory=list)\\n    tokens: List[str] = Field(default_factory=list)\\n    top_logprobs: List[Optional[Dict[str,\\n                                     float]]] = Field(default_factory=list)\\n\\n\\nclass CompletionResponseChoice(BaseModel):\\n    index: int\\n    text: str\\n    logprobs: Optional[LogProbs] = None\\n    finish_reason: Optional[Literal[\"stop\", \"length\"]] = None\\n\\n\\nclass CompletionResponse(BaseModel):\\n    id: str = Field(default_factory=lambda: f\"cmpl-{random_uuid()}\")\\n    object: str = \"text_completion\"\\n    created: int = Field(default_factory=lambda: int(time.time()))\\n    model: str\\n    choices: List[CompletionResponseChoice]\\n    usage: UsageInfo\\n\\n\\nclass CompletionResponseStreamChoice(BaseModel):\\n    index: int\\n    text: str\\n    logprobs: Optional[LogProbs] = None\\n    finish_reason: Optional[Literal[\"stop\", \"length\"]] = None\\n\\n\\nclass CompletionStreamResponse(BaseModel):\\n    id: str = Field(default_factory=lambda: f\"cmpl-{random_uuid()}\")\\n    object: str = \"text_completion\"\\n    created: int = Field(default_factory=lambda: int(time.time()))\\n    model: str\\n    choices: List[CompletionResponseStreamChoice]\\n\\n\\nclass ChatMessage(BaseModel):\\n    role: str\\n    content: str\\n\\n\\nclass ChatCompletionResponseChoice(BaseModel):\\n    index: int\\n    message: ChatMessage\\n    finish_reason: Optional[Literal[\"stop\", \"length\"]] = None\\n\\n\\nclass ChatCompletionResponse(BaseModel):\\n    id: str = Field(default_factory=lambda: f\"chatcmpl-{random_uuid()}\")\\n    object: str = \"chat.completion\"\\n    created: int = Field(default_factory=lambda: int(time.time()))\\n    model: str\\n    choices: List[ChatCompletionResponseChoice]\\n    usage: UsageInfo\\n\\n\\nclass DeltaMessage(BaseModel):\\n    role: Optional[str] = None\\n    content: Optional[str] = None\\n\\n\\nclass ChatCompletionResponseStreamChoice(BaseModel):\\n    index: int\\n    delta: DeltaMessage\\n    finish_reason: Optional[Literal[\"stop\", \"length\"]] = None\\n\\n\\nclass ChatCompletionStreamResponse(BaseModel):\\n    id: str = Field(default_factory=lambda: f\"chatcmpl-{random_uuid()}\")\\n    object: str = \"chat.completion.chunk\"\\n    created: int = Field(default_factory=lambda: int(time.time()))\\n    model: str\\n    choices: List[ChatCompletionResponseStreamChoice]\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/entrypoints/openai/__init__.py\\n---------\\nContent:\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/entrypoints/openai/api_server.py\\n---------\\nContent:\\n# Adapted from\\n# https://github.com/lm-sys/FastChat/blob/168ccc29d3f7edc50823016105c024fe2282732a/fastchat/serve/openai_api_server.py\\n\\nimport argparse\\nimport asyncio\\nimport json\\nimport time\\nfrom http import HTTPStatus\\nfrom typing import AsyncGenerator, Dict, List, Optional, Tuple, Union\\n\\nimport fastapi\\nimport uvicorn\\nfrom fastapi import BackgroundTasks, Request\\nfrom fastapi.exceptions import RequestValidationError\\nfrom fastapi.middleware.cors import CORSMiddleware\\nfrom fastapi.responses import JSONResponse, StreamingResponse\\nfrom packaging import version\\n\\nfrom vllm.engine.arg_utils import AsyncEngineArgs\\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine\\nfrom vllm.entrypoints.openai.protocol import (\\n    CompletionRequest, CompletionResponse, CompletionResponseChoice,\\n    CompletionResponseStreamChoice, CompletionStreamResponse,\\n    ChatCompletionRequest, ChatCompletionResponse,\\n    ChatCompletionResponseChoice, ChatCompletionResponseStreamChoice,\\n    ChatCompletionStreamResponse, ChatMessage, DeltaMessage, ErrorResponse,\\n    LogProbs, ModelCard, ModelList, ModelPermission, UsageInfo)\\nfrom vllm.logger import init_logger\\nfrom vllm.outputs import RequestOutput\\nfrom vllm.sampling_params import SamplingParams\\nfrom vllm.transformers_utils.tokenizer import get_tokenizer\\nfrom vllm.utils import random_uuid\\n\\ntry:\\n    import fastchat\\n    from fastchat.conversation import Conversation, SeparatorStyle\\n    from fastchat.model.model_adapter import get_conversation_template\\n    _fastchat_available = True\\nexcept ImportError:\\n    _fastchat_available = False\\n\\nTIMEOUT_KEEP_ALIVE = 5  # seconds\\n\\nlogger = init_logger(__name__)\\nserved_model = None\\napp = fastapi.FastAPI()\\nengine = None\\n\\n\\ndef create_error_response(status_code: HTTPStatus,\\n                          message: str) -> JSONResponse:\\n    return JSONResponse(ErrorResponse(message=message,\\n                                      type=\"invalid_request_error\").dict(),\\n                        status_code=status_code.value)\\n\\n\\n@app.exception_handler(RequestValidationError)\\nasync def validation_exception_handler(request, exc):  # pylint: disable=unused-argument\\n    return create_error_response(HTTPStatus.BAD_REQUEST, str(exc))\\n\\n\\nasync def check_model(request) -> Optional[JSONResponse]:\\n    if request.model == served_model:\\n        return\\n    ret = create_error_response(\\n        HTTPStatus.NOT_FOUND,\\n        f\"The model `{request.model}` does not exist.\",\\n    )\\n    return ret\\n\\n\\nasync def get_gen_prompt(request) -> str:\\n    if not _fastchat_available:\\n        raise ModuleNotFoundError(\\n            \"fastchat is not installed. Please install fastchat to use \"\\n            \"the chat completion and conversation APIs: `$ pip install fschat`\"\\n        )\\n    if version.parse(fastchat.__version__) < version.parse(\"0.2.23\"):\\n        raise ImportError(\\n            f\"fastchat version is low. Current version: {fastchat.__version__} \"\\n            \"Please upgrade fastchat to use: `$ pip install -U fschat`\")\\n\\n    conv = get_conversation_template(request.model)\\n    conv = Conversation(\\n        name=conv.name,\\n        system_template=conv.system_template,\\n        system_message=conv.system_message,\\n        roles=conv.roles,\\n        messages=list(conv.messages),  # prevent in-place modification\\n        offset=conv.offset,\\n        sep_style=SeparatorStyle(conv.sep_style),\\n        sep=conv.sep,\\n        sep2=conv.sep2,\\n        stop_str=conv.stop_str,\\n        stop_token_ids=conv.stop_token_ids,\\n    )\\n\\n    if isinstance(request.messages, str):\\n        prompt = request.messages\\n    else:\\n        for message in request.messages:\\n            msg_role = message[\"role\"]\\n            if msg_role == \"system\":\\n                conv.system_message = message[\"content\"]\\n            elif msg_role == \"user\":\\n                conv.append_message(conv.roles[0], message[\"content\"])\\n            elif msg_role == \"assistant\":\\n                conv.append_message(conv.roles[1], message[\"content\"])\\n            else:\\n                raise ValueError(f\"Unknown role: {msg_role}\")\\n\\n        # Add a blank message for the assistant.\\n        conv.append_message(conv.roles[1], None)\\n        prompt = conv.get_prompt()\\n\\n    return prompt\\n\\n\\nasync def check_length(\\n    request: Union[ChatCompletionRequest, CompletionRequest],\\n    prompt: Optional[str] = None,\\n    prompt_ids: Optional[List[int]] = None\\n) -> Tuple[List[int], Optional[JSONResponse]]:\\n    assert (not (prompt is None and prompt_ids is None)\\n            and not (prompt is not None and prompt_ids is not None)\\n            ), \"Either prompt or prompt_ids should be provided.\"\\n    if prompt_ids is not None:\\n        input_ids = prompt_ids\\n    else:\\n        input_ids = tokenizer(prompt).input_ids\\n    token_num = len(input_ids)\\n\\n    if token_num + request.max_tokens > max_model_len:\\n        return input_ids, create_error_response(\\n            HTTPStatus.BAD_REQUEST,\\n            f\"This model\\'s maximum context length is {max_model_len} tokens. \"\\n            f\"However, you requested {request.max_tokens + token_num} tokens \"\\n            f\"({token_num} in the messages, \"\\n            f\"{request.max_tokens} in the completion). \"\\n            f\"Please reduce the length of the messages or completion.\",\\n        )\\n    else:\\n        return input_ids, None\\n\\n\\n@app.get(\"/v1/models\")\\nasync def show_available_models():\\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\\n    model_cards = [\\n        ModelCard(id=served_model,\\n                  root=served_model,\\n                  permission=[ModelPermission()])\\n    ]\\n    return ModelList(data=model_cards)\\n\\n\\ndef create_logprobs(token_ids: List[int],\\n                    id_logprobs: List[Dict[int, float]],\\n                    initial_text_offset: int = 0) -> LogProbs:\\n    \"\"\"Create OpenAI-style logprobs.\"\"\"\\n    logprobs = LogProbs()\\n    last_token_len = 0\\n    for token_id, id_logprob in zip(token_ids, id_logprobs):\\n        token = tokenizer.convert_ids_to_tokens(token_id)\\n        logprobs.tokens.append(token)\\n        logprobs.token_logprobs.append(id_logprob[token_id])\\n        if len(logprobs.text_offset) == 0:\\n            logprobs.text_offset.append(initial_text_offset)\\n        else:\\n            logprobs.text_offset.append(logprobs.text_offset[-1] +\\n                                        last_token_len)\\n        last_token_len = len(token)\\n\\n        logprobs.top_logprobs.append({\\n            tokenizer.convert_ids_to_tokens(i): p\\n            for i, p in id_logprob.items()\\n        })\\n    return logprobs\\n\\n\\n@app.post(\"/v1/chat/completions\")\\nasync def create_chat_completion(request: ChatCompletionRequest,\\n                                 raw_request: Request):\\n    \"\"\"Completion API similar to OpenAI\\'s API.\\n\\n    See  https://platform.openai.com/docs/api-reference/chat/create\\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\\n\\n    NOTE: Currently we do not support the following features:\\n        - function_call (Users should implement this by themselves)\\n        - logit_bias (to be supported by vLLM engine)\\n    \"\"\"\\n    logger.info(f\"Received chat completion request: {request}\")\\n\\n    error_check_ret = await check_model(request)\\n    if error_check_ret is not None:\\n        return error_check_ret\\n\\n    if request.logit_bias is not None and len(request.logit_bias) > 0:\\n        # TODO: support logit_bias in vLLM engine.\\n        return create_error_response(HTTPStatus.BAD_REQUEST,\\n                                     \"logit_bias is not currently supported\")\\n\\n    prompt = await get_gen_prompt(request)\\n    token_ids, error_check_ret = await check_length(request, prompt=prompt)\\n    if error_check_ret is not None:\\n        return error_check_ret\\n\\n    model_name = request.model\\n    request_id = f\"cmpl-{random_uuid()}\"\\n    created_time = int(time.time())\\n    try:\\n        sampling_params = SamplingParams(\\n            n=request.n,\\n            presence_penalty=request.presence_penalty,\\n            frequency_penalty=request.frequency_penalty,\\n            temperature=request.temperature,\\n            top_p=request.top_p,\\n            stop=request.stop,\\n            max_tokens=request.max_tokens,\\n            best_of=request.best_of,\\n            top_k=request.top_k,\\n            ignore_eos=request.ignore_eos,\\n            use_beam_search=request.use_beam_search,\\n        )\\n    except ValueError as e:\\n        return create_error_response(HTTPStatus.BAD_REQUEST, str(e))\\n\\n    result_generator = engine.generate(prompt, sampling_params, request_id,\\n                                       token_ids)\\n\\n    async def abort_request() -> None:\\n        await engine.abort(request_id)\\n\\n    def create_stream_response_json(\\n        index: int,\\n        text: str,\\n        finish_reason: Optional[str] = None,\\n    ) -> str:\\n        choice_data = ChatCompletionResponseStreamChoice(\\n            index=index,\\n            delta=DeltaMessage(content=text),\\n            finish_reason=finish_reason,\\n        )\\n        response = ChatCompletionStreamResponse(\\n            id=request_id,\\n            created=created_time,\\n            model=model_name,\\n            choices=[choice_data],\\n        )\\n        response_json = response.json(ensure_ascii=False)\\n\\n        return response_json\\n\\n    async def completion_stream_generator() -> AsyncGenerator[str, None]:\\n        # First chunk with role\\n        for i in range(request.n):\\n            choice_data = ChatCompletionResponseStreamChoice(\\n                index=i,\\n                delta=DeltaMessage(role=\"assistant\"),\\n                finish_reason=None,\\n            )\\n            chunk = ChatCompletionStreamResponse(id=request_id,\\n                                                 choices=[choice_data],\\n                                                 model=model_name)\\n            data = chunk.json(exclude_unset=True, ensure_ascii=False)\\n            yield f\"data: {data}\\\\n\\\\n\"\\n\\n        previous_texts = [\"\"] * request.n\\n        previous_num_tokens = [0] * request.n\\n        async for res in result_generator:\\n            res: RequestOutput\\n            for output in res.outputs:\\n                i = output.index\\n                delta_text = output.text[len(previous_texts[i]):]\\n                previous_texts[i] = output.text\\n                previous_num_tokens[i] = len(output.token_ids)\\n                response_json = create_stream_response_json(\\n                    index=i,\\n                    text=delta_text,\\n                )\\n                yield f\"data: {response_json}\\\\n\\\\n\"\\n                if output.finish_reason is not None:\\n                    response_json = create_stream_response_json(\\n                        index=i,\\n                        text=\"\",\\n                        finish_reason=output.finish_reason,\\n                    )\\n                    yield f\"data: {response_json}\\\\n\\\\n\"\\n        yield \"data: [DONE]\\\\n\\\\n\"\\n\\n    # Streaming response\\n    if request.stream:\\n        background_tasks = BackgroundTasks()\\n        # Abort the request if the client disconnects.\\n        background_tasks.add_task(abort_request)\\n        return StreamingResponse(completion_stream_generator(),\\n                                 media_type=\"text/event-stream\",\\n                                 background=background_tasks)\\n\\n    # Non-streaming response\\n    final_res: RequestOutput = None\\n    async for res in result_generator:\\n        if await raw_request.is_disconnected():\\n            # Abort the request if the client disconnects.\\n            await abort_request()\\n            return create_error_response(HTTPStatus.BAD_REQUEST,\\n                                         \"Client disconnected\")\\n        final_res = res\\n    assert final_res is not None\\n    choices = []\\n    for output in final_res.outputs:\\n        choice_data = ChatCompletionResponseChoice(\\n            index=output.index,\\n            message=ChatMessage(role=\"assistant\", content=output.text),\\n            finish_reason=output.finish_reason,\\n        )\\n        choices.append(choice_data)\\n\\n    num_prompt_tokens = len(final_res.prompt_token_ids)\\n    num_generated_tokens = sum(\\n        len(output.token_ids) for output in final_res.outputs)\\n    usage = UsageInfo(\\n        prompt_tokens=num_prompt_tokens,\\n        completion_tokens=num_generated_tokens,\\n        total_tokens=num_prompt_tokens + num_generated_tokens,\\n    )\\n    response = ChatCompletionResponse(\\n        id=request_id,\\n        created=created_time,\\n        model=model_name,\\n        choices=choices,\\n        usage=usage,\\n    )\\n\\n    if request.stream:\\n        # When user requests streaming but we don\\'t stream, we still need to\\n        # return a streaming response with a single event.\\n        response_json = response.json(ensure_ascii=False)\\n\\n        async def fake_stream_generator() -> AsyncGenerator[str, None]:\\n            yield f\"data: {response_json}\\\\n\\\\n\"\\n            yield \"data: [DONE]\\\\n\\\\n\"\\n\\n        return StreamingResponse(fake_stream_generator(),\\n                                 media_type=\"text/event-stream\")\\n\\n    return response\\n\\n\\n@app.post(\"/v1/completions\")\\nasync def create_completion(request: CompletionRequest, raw_request: Request):\\n    \"\"\"Completion API similar to OpenAI\\'s API.\\n\\n    See https://platform.openai.com/docs/api-reference/completions/create\\n    for the API specification. This API mimics the OpenAI Completion API.\\n\\n    NOTE: Currently we do not support the following features:\\n        - echo (since the vLLM engine does not currently support\\n          getting the logprobs of prompt tokens)\\n        - suffix (the language models we currently support do not support\\n          suffix)\\n        - logit_bias (to be supported by vLLM engine)\\n    \"\"\"\\n    logger.info(f\"Received completion request: {request}\")\\n\\n    error_check_ret = await check_model(request)\\n    if error_check_ret is not None:\\n        return error_check_ret\\n\\n    if request.echo:\\n        # We do not support echo since the vLLM engine does not\\n        # currently support getting the logprobs of prompt tokens.\\n        return create_error_response(HTTPStatus.BAD_REQUEST,\\n                                     \"echo is not currently supported\")\\n\\n    if request.suffix is not None:\\n        # The language models we currently support do not support suffix.\\n        return create_error_response(HTTPStatus.BAD_REQUEST,\\n                                     \"suffix is not currently supported\")\\n\\n    if request.logit_bias is not None and len(request.logit_bias) > 0:\\n        # TODO: support logit_bias in vLLM engine.\\n        return create_error_response(HTTPStatus.BAD_REQUEST,\\n                                     \"logit_bias is not currently supported\")\\n\\n    model_name = request.model\\n    request_id = f\"cmpl-{random_uuid()}\"\\n\\n    use_token_ids = False\\n    if isinstance(request.prompt, list):\\n        if len(request.prompt) == 0:\\n            return create_error_response(HTTPStatus.BAD_REQUEST,\\n                                         \"please provide at least one prompt\")\\n        first_element = request.prompt[0]\\n        if isinstance(first_element, int):\\n            use_token_ids = True\\n            prompt = request.prompt\\n        elif isinstance(first_element, (str, list)):\\n            # TODO: handles multiple prompt case in list[list[int]]\\n            if len(request.prompt) > 1:\\n                return create_error_response(\\n                    HTTPStatus.BAD_REQUEST,\\n                    \"multiple prompts in a batch is not currently supported\")\\n            use_token_ids = not isinstance(first_element, str)\\n            prompt = request.prompt[0]\\n    else:\\n        prompt = request.prompt\\n\\n    if use_token_ids:\\n        _, error_check_ret = await check_length(request, prompt_ids=prompt)\\n    else:\\n        token_ids, error_check_ret = await check_length(request, prompt=prompt)\\n    if error_check_ret is not None:\\n        return error_check_ret\\n\\n    created_time = int(time.time())\\n    try:\\n        sampling_params = SamplingParams(\\n            n=request.n,\\n            best_of=request.best_of,\\n            presence_penalty=request.presence_penalty,\\n            frequency_penalty=request.frequency_penalty,\\n            temperature=request.temperature,\\n            top_p=request.top_p,\\n            top_k=request.top_k,\\n            stop=request.stop,\\n            ignore_eos=request.ignore_eos,\\n            max_tokens=request.max_tokens,\\n            logprobs=request.logprobs,\\n            use_beam_search=request.use_beam_search,\\n        )\\n    except ValueError as e:\\n        return create_error_response(HTTPStatus.BAD_REQUEST, str(e))\\n\\n    if use_token_ids:\\n        result_generator = engine.generate(None,\\n                                           sampling_params,\\n                                           request_id,\\n                                           prompt_token_ids=prompt)\\n    else:\\n        result_generator = engine.generate(prompt, sampling_params, request_id,\\n                                           token_ids)\\n\\n    # Similar to the OpenAI API, when n != best_of, we do not stream the\\n    # results. In addition, we do not stream the results when use beam search.\\n    stream = (request.stream\\n              and (request.best_of is None or request.n == request.best_of)\\n              and not request.use_beam_search)\\n\\n    async def abort_request() -> None:\\n        await engine.abort(request_id)\\n\\n    def create_stream_response_json(\\n        index: int,\\n        text: str,\\n        logprobs: Optional[LogProbs] = None,\\n        finish_reason: Optional[str] = None,\\n    ) -> str:\\n        choice_data = CompletionResponseStreamChoice(\\n            index=index,\\n            text=text,\\n            logprobs=logprobs,\\n            finish_reason=finish_reason,\\n        )\\n        response = CompletionStreamResponse(\\n            id=request_id,\\n            created=created_time,\\n            model=model_name,\\n            choices=[choice_data],\\n        )\\n        response_json = response.json(ensure_ascii=False)\\n\\n        return response_json\\n\\n    async def completion_stream_generator() -> AsyncGenerator[str, None]:\\n        previous_texts = [\"\"] * request.n\\n        previous_num_tokens = [0] * request.n\\n        async for res in result_generator:\\n            res: RequestOutput\\n            for output in res.outputs:\\n                i = output.index\\n                delta_text = output.text[len(previous_texts[i]):]\\n                if request.logprobs is not None:\\n                    logprobs = create_logprobs(\\n                        output.token_ids[previous_num_tokens[i]:],\\n                        output.logprobs[previous_num_tokens[i]:],\\n                        len(previous_texts[i]))\\n                else:\\n                    logprobs = None\\n                previous_texts[i] = output.text\\n                previous_num_tokens[i] = len(output.token_ids)\\n                response_json = create_stream_response_json(\\n                    index=i,\\n                    text=delta_text,\\n                    logprobs=logprobs,\\n                )\\n                yield f\"data: {response_json}\\\\n\\\\n\"\\n                if output.finish_reason is not None:\\n                    logprobs = (LogProbs()\\n                                if request.logprobs is not None else None)\\n                    response_json = create_stream_response_json(\\n                        index=i,\\n                        text=\"\",\\n                        logprobs=logprobs,\\n                        finish_reason=output.finish_reason,\\n                    )\\n                    yield f\"data: {response_json}\\\\n\\\\n\"\\n        yield \"data: [DONE]\\\\n\\\\n\"\\n\\n    # Streaming response\\n    if stream:\\n        background_tasks = BackgroundTasks()\\n        # Abort the request if the client disconnects.\\n        background_tasks.add_task(abort_request)\\n        return StreamingResponse(completion_stream_generator(),\\n                                 media_type=\"text/event-stream\",\\n                                 background=background_tasks)\\n\\n    # Non-streaming response\\n    final_res: RequestOutput = None\\n    async for res in result_generator:\\n        if await raw_request.is_disconnected():\\n            # Abort the request if the client disconnects.\\n            await abort_request()\\n            return create_error_response(HTTPStatus.BAD_REQUEST,\\n                                         \"Client disconnected\")\\n        final_res = res\\n    assert final_res is not None\\n    choices = []\\n    for output in final_res.outputs:\\n        if request.logprobs is not None:\\n            logprobs = create_logprobs(output.token_ids, output.logprobs)\\n        else:\\n            logprobs = None\\n        choice_data = CompletionResponseChoice(\\n            index=output.index,\\n            text=output.text,\\n            logprobs=logprobs,\\n            finish_reason=output.finish_reason,\\n        )\\n        choices.append(choice_data)\\n\\n    num_prompt_tokens = len(final_res.prompt_token_ids)\\n    num_generated_tokens = sum(\\n        len(output.token_ids) for output in final_res.outputs)\\n    usage = UsageInfo(\\n        prompt_tokens=num_prompt_tokens,\\n        completion_tokens=num_generated_tokens,\\n        total_tokens=num_prompt_tokens + num_generated_tokens,\\n    )\\n    response = CompletionResponse(\\n        id=request_id,\\n        created=created_time,\\n        model=model_name,\\n        choices=choices,\\n        usage=usage,\\n    )\\n\\n    if request.stream:\\n        # When user requests streaming but we don\\'t stream, we still need to\\n        # return a streaming response with a single event.\\n        response_json = response.json(ensure_ascii=False)\\n\\n        async def fake_stream_generator() -> AsyncGenerator[str, None]:\\n            yield f\"data: {response_json}\\\\n\\\\n\"\\n            yield \"data: [DONE]\\\\n\\\\n\"\\n\\n        return StreamingResponse(fake_stream_generator(),\\n                                 media_type=\"text/event-stream\")\\n\\n    return response\\n\\n\\nif __name__ == \"__main__\":\\n    parser = argparse.ArgumentParser(\\n        description=\"vLLM OpenAI-Compatible RESTful API server.\")\\n    parser.add_argument(\"--host\",\\n                        type=str,\\n                        default=\"localhost\",\\n                        help=\"host name\")\\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"port number\")\\n    parser.add_argument(\"--allow-credentials\",\\n                        action=\"store_true\",\\n                        help=\"allow credentials\")\\n    parser.add_argument(\"--allowed-origins\",\\n                        type=json.loads,\\n                        default=[\"*\"],\\n                        help=\"allowed origins\")\\n    parser.add_argument(\"--allowed-methods\",\\n                        type=json.loads,\\n                        default=[\"*\"],\\n                        help=\"allowed methods\")\\n    parser.add_argument(\"--allowed-headers\",\\n                        type=json.loads,\\n                        default=[\"*\"],\\n                        help=\"allowed headers\")\\n    parser.add_argument(\"--served-model-name\",\\n                        type=str,\\n                        default=None,\\n                        help=\"The model name used in the API. If not \"\\n                        \"specified, the model name will be the same as \"\\n                        \"the huggingface name.\")\\n\\n    parser = AsyncEngineArgs.add_cli_args(parser)\\n    args = parser.parse_args()\\n\\n    app.add_middleware(\\n        CORSMiddleware,\\n        allow_origins=args.allowed_origins,\\n        allow_credentials=args.allow_credentials,\\n        allow_methods=args.allowed_methods,\\n        allow_headers=args.allowed_headers,\\n    )\\n\\n    logger.info(f\"args: {args}\")\\n\\n    if args.served_model_name is not None:\\n        served_model = args.served_model_name\\n    else:\\n        served_model = args.model\\n\\n    engine_args = AsyncEngineArgs.from_cli_args(args)\\n    engine = AsyncLLMEngine.from_engine_args(engine_args)\\n    engine_model_config = asyncio.run(engine.get_model_config())\\n    max_model_len = engine_model_config.get_max_model_len()\\n\\n    # A separate tokenizer to map token IDs to strings.\\n    tokenizer = get_tokenizer(engine_args.tokenizer,\\n                              tokenizer_mode=engine_args.tokenizer_mode,\\n                              trust_remote_code=engine_args.trust_remote_code)\\n\\n    uvicorn.run(app,\\n                host=args.host,\\n                port=args.port,\\n                log_level=\"info\",\\n                timeout_keep_alive=TIMEOUT_KEEP_ALIVE)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/transformers_utils/config.py\\n---------\\nContent:\\nfrom typing import Optional\\n\\nfrom transformers import AutoConfig, PretrainedConfig\\n\\nfrom vllm.transformers_utils.configs import *  # pylint: disable=wildcard-import\\n\\n_CONFIG_REGISTRY = {\\n    \"mpt\": MPTConfig,\\n    \"baichuan\": BaiChuanConfig,\\n    \"aquila\": AquilaConfig,\\n    \"qwen\": QWenConfig,\\n    \"RefinedWeb\": RWConfig,  # For tiiuae/falcon-40b(-instruct)\\n    \"RefinedWebModel\": RWConfig,  # For tiiuae/falcon-7b(-instruct)\\n}\\n\\n\\ndef get_config(model: str,\\n               trust_remote_code: bool,\\n               revision: Optional[str] = None) -> PretrainedConfig:\\n    try:\\n        config = AutoConfig.from_pretrained(\\n            model, trust_remote_code=trust_remote_code, revision=revision)\\n    except ValueError as e:\\n        if (not trust_remote_code and\\n                \"requires you to execute the configuration file\" in str(e)):\\n            err_msg = (\\n                \"Failed to load the model config. If the model is a custom \"\\n                \"model not yet available in the HuggingFace transformers \"\\n                \"library, consider setting `trust_remote_code=True` in LLM \"\\n                \"or using the `--trust-remote-code` flag in the CLI.\")\\n            raise RuntimeError(err_msg) from e\\n        else:\\n            raise e\\n    if config.model_type in _CONFIG_REGISTRY:\\n        config_class = _CONFIG_REGISTRY[config.model_type]\\n        config = config_class.from_pretrained(model, revision=revision)\\n    return config\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/transformers_utils/__init__.py\\n---------\\nContent:\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/transformers_utils/tokenizer.py\\n---------\\nContent:\\nfrom typing import List, Optional, Tuple, Union\\n\\nfrom transformers import (AutoTokenizer, PreTrainedTokenizer,\\n                          PreTrainedTokenizerFast)\\n\\nfrom vllm.logger import init_logger\\n\\nlogger = init_logger(__name__)\\n\\n# A fast LLaMA tokenizer with the pre-processed `tokenizer.json` file.\\n_FAST_LLAMA_TOKENIZER = \"hf-internal-testing/llama-tokenizer\"\\n\\n\\ndef get_tokenizer(\\n    tokenizer_name: str,\\n    *args,\\n    tokenizer_mode: str = \"auto\",\\n    trust_remote_code: bool = False,\\n    **kwargs,\\n) -> Union[PreTrainedTokenizer, PreTrainedTokenizerFast]:\\n    \"\"\"Gets a tokenizer for the given model name via Huggingface.\"\"\"\\n    if tokenizer_mode == \"slow\":\\n        if kwargs.get(\"use_fast\", False):\\n            raise ValueError(\\n                \"Cannot use the fast tokenizer in slow tokenizer mode.\")\\n        kwargs[\"use_fast\"] = False\\n\\n    if (\"llama\" in tokenizer_name.lower() and kwargs.get(\"use_fast\", True)\\n            and tokenizer_name != _FAST_LLAMA_TOKENIZER):\\n        logger.info(\\n            \"For some LLaMA V1 models, initializing the fast tokenizer may \"\\n            \"take a long time. To reduce the initialization time, consider \"\\n            f\"using \\'{_FAST_LLAMA_TOKENIZER}\\' instead of the original \"\\n            \"tokenizer.\")\\n    try:\\n        tokenizer = AutoTokenizer.from_pretrained(\\n            tokenizer_name,\\n            *args,\\n            trust_remote_code=trust_remote_code,\\n            **kwargs)\\n    except TypeError as e:\\n        # The LLaMA tokenizer causes a protobuf error in some environments.\\n        err_msg = (\\n            \"Failed to load the tokenizer. If you are using a LLaMA V1 model \"\\n            f\"consider using \\'{_FAST_LLAMA_TOKENIZER}\\' instead of the \"\\n            \"original tokenizer.\")\\n        raise RuntimeError(err_msg) from e\\n    except ValueError as e:\\n        # If the error pertains to the tokenizer class not existing or not\\n        # currently being imported, suggest using the --trust-remote-code flag.\\n        if (not trust_remote_code and\\n            (\"does not exist or is not currently imported.\" in str(e)\\n             or \"requires you to execute the tokenizer file\" in str(e))):\\n            err_msg = (\\n                \"Failed to load the tokenizer. If the tokenizer is a custom \"\\n                \"tokenizer not yet available in the HuggingFace transformers \"\\n                \"library, consider setting `trust_remote_code=True` in LLM \"\\n                \"or using the `--trust-remote-code` flag in the CLI.\")\\n            raise RuntimeError(err_msg) from e\\n        else:\\n            raise e\\n\\n    if not isinstance(tokenizer, PreTrainedTokenizerFast):\\n        logger.warning(\\n            \"Using a slow tokenizer. This might cause a significant \"\\n            \"slowdown. Consider using a fast tokenizer instead.\")\\n    return tokenizer\\n\\n\\ndef _convert_tokens_to_string_with_added_encoders(\\n    tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],\\n    output_tokens: List[str],\\n    skip_special_tokens: bool,\\n) -> str:\\n    # Adapted from\\n    # https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/tokenization_utils.py#L921\\n    # NOTE(woosuk): The following code is slow because it runs a for loop over\\n    # the output_tokens. In Python, running a for loop over a list can be slow\\n    # even when the loop body is very simple.\\n    sub_texts = []\\n    current_sub_text = []\\n    for token in output_tokens:\\n        if skip_special_tokens and token in tokenizer.all_special_tokens:\\n            continue\\n        if token in tokenizer.added_tokens_encoder:\\n            if current_sub_text:\\n                sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\\n                sub_texts.append(sub_text)\\n                current_sub_text = []\\n            sub_texts.append(token)\\n        else:\\n            current_sub_text.append(token)\\n    if current_sub_text:\\n        sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\\n        sub_texts.append(sub_text)\\n    return \" \".join(sub_texts)\\n\\n\\n# Based on\\n# https://github.com/huggingface/text-generation-inference/blob/v0.9.4/server/text_generation_server/models/model.py#L62C9-L62C15\\n# under Apache 2.0 license\\ndef detokenize_incrementally(\\n    tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],\\n    all_input_ids: List[int],\\n    prev_tokens: Optional[List[str]],\\n    prefix_offset: int = 0,\\n    read_offset: int = 0,\\n    skip_special_tokens: bool = False,\\n) -> Tuple[List[str], str, int, int]:\\n    new_token_id = all_input_ids[-1]\\n    # This is the first iteration for this sequence\\n    if prev_tokens is None:\\n        new_tokens = tokenizer.convert_ids_to_tokens(\\n            all_input_ids, skip_special_tokens=skip_special_tokens)\\n        output_tokens = new_tokens\\n        # 5 is an arbitrary value that should work for all\\n        # tokenizers (bigger = more conservative).\\n        # Subtract 1 extra to account for the generated token.\\n        prefix_offset = max(len(output_tokens) - 6, 0)\\n        read_offset = max(len(output_tokens) - 1, 0)\\n    else:\\n        # Put new_token_id in a list so skip_special_tokens is respected\\n        new_tokens = tokenizer.convert_ids_to_tokens(\\n            [new_token_id], skip_special_tokens=skip_special_tokens)\\n        output_tokens = prev_tokens + new_tokens\\n\\n    # The prefix text is necessary only to defeat cleanup algorithms in\\n    # the decode which decide to add a space or not depending on the\\n    # surrounding ids.\\n    if not getattr(tokenizer, \"added_tokens_encoder\", {}):\\n        prefix_text = tokenizer.convert_tokens_to_string(\\n            output_tokens[prefix_offset:read_offset])\\n        new_text = tokenizer.convert_tokens_to_string(\\n            output_tokens[prefix_offset:])\\n    else:\\n        prefix_text = _convert_tokens_to_string_with_added_encoders(\\n            tokenizer,\\n            output_tokens[prefix_offset:read_offset],\\n            skip_special_tokens=skip_special_tokens)\\n        new_text = _convert_tokens_to_string_with_added_encoders(\\n            tokenizer,\\n            output_tokens[prefix_offset:],\\n            skip_special_tokens=skip_special_tokens)\\n\\n    if len(new_text) > len(prefix_text) and not new_text.endswith(\"\"):\\n        # utf-8 char at the end means it\\'s a potential unfinished byte sequence\\n        # from byte fallback tokenization.\\n        # If it\\'s in the middle, it\\'s probably a real invalid id generated\\n        # by the model\\n        new_text = new_text[len(prefix_text):]\\n        return new_tokens, new_text, read_offset, len(output_tokens)\\n    else:\\n        return new_tokens, \"\", prefix_offset, read_offset\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/transformers_utils/configs/aquila.py\\n---------\\nContent:\\n# coding=utf-8\\n# Copyright 2023 EleutherAI and the HuggingFace Inc. team. All rights reserved.\\n#\\n# This code is based on EleutherAI\\'s GPT-NeoX library and the GPT-NeoX\\n# and OPT implementations in this library. It has been modified from its\\n# original forms to accommodate minor architectural differences compared\\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\"\"\" Aquila model configuration\"\"\"\\n\\nfrom transformers import PretrainedConfig\\n\\n\\nclass AquilaConfig(PretrainedConfig):\\n    model_type = \"aquila\"\\n    keys_to_ignore_at_inference = [\"past_key_values\"]\\n\\n    def __init__(\\n        self,\\n        vocab_size=100008,\\n        hidden_size=4096,\\n        intermediate_size=11008,\\n        num_hidden_layers=32,\\n        num_attention_heads=32,\\n        hidden_act=\"silu\",\\n        max_position_embeddings=2048,\\n        initializer_range=0.006,\\n        rms_norm_eps=1e-5,\\n        use_cache=True,\\n        pad_token_id=0,\\n        bos_token_id=1,\\n        eos_token_id=2,\\n        tie_word_embeddings=False,\\n        **kwargs,\\n    ):\\n        self.vocab_size = vocab_size\\n        self.max_position_embeddings = max_position_embeddings\\n        self.hidden_size = hidden_size\\n        self.intermediate_size = intermediate_size\\n        self.num_hidden_layers = num_hidden_layers\\n        self.num_attention_heads = num_attention_heads\\n        self.hidden_act = hidden_act\\n        self.initializer_range = initializer_range\\n        self.rms_norm_eps = rms_norm_eps\\n        self.use_cache = use_cache\\n        super().__init__(\\n            pad_token_id=pad_token_id,\\n            bos_token_id=bos_token_id,\\n            eos_token_id=eos_token_id,\\n            tie_word_embeddings=tie_word_embeddings,\\n            **kwargs,\\n        )\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/transformers_utils/configs/__init__.py\\n---------\\nContent:\\nfrom vllm.transformers_utils.configs.mpt import MPTConfig\\nfrom vllm.transformers_utils.configs.baichuan import BaiChuanConfig\\nfrom vllm.transformers_utils.configs.aquila import AquilaConfig\\nfrom vllm.transformers_utils.configs.qwen import QWenConfig\\n# RWConfig is for the original tiiuae/falcon-40b(-instruct) and\\n# tiiuae/falcon-7b(-instruct) models. Newer Falcon models will use the\\n# `FalconConfig` class from the official HuggingFace transformers library.\\nfrom vllm.transformers_utils.configs.falcon import RWConfig\\n\\n__all__ = [\\n    \"MPTConfig\",\\n    \"BaiChuanConfig\",\\n    \"AquilaConfig\",\\n    \"QWenConfig\",\\n    \"RWConfig\",\\n]\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/transformers_utils/configs/qwen.py\\n---------\\nContent:\\n# Copyright (c) Alibaba Cloud.\\n# LICENSE: https://huggingface.co/Qwen/Qwen-7B/blob/main/LICENSE\\n\\nfrom transformers import PretrainedConfig\\n\\n\\nclass QWenConfig(PretrainedConfig):\\n    model_type = \"qwen\"\\n    keys_to_ignore_at_inference = [\"past_key_values\"]\\n    attribute_map = {\\n        \"hidden_size\": \"n_embd\",\\n        \"num_attention_heads\": \"n_head\",\\n        \"max_position_embeddings\": \"n_positions\",\\n        \"num_hidden_layers\": \"n_layer\",\\n    }\\n\\n    def __init__(\\n        self,\\n        vocab_size=151851,\\n        n_embd=4096,\\n        n_layer=32,\\n        n_head=32,\\n        n_inner=None,\\n        embd_pdrop=0.0,\\n        attn_pdrop=0.0,\\n        layer_norm_epsilon=1e-5,\\n        initializer_range=0.02,\\n        scale_attn_weights=True,\\n        use_cache=True,\\n        eos_token_id=151643,\\n        apply_residual_connection_post_layernorm=False,\\n        bf16=True,\\n        kv_channels=128,\\n        rotary_pct=1.0,\\n        rotary_emb_base=10000,\\n        use_dynamic_ntk=False,\\n        use_logn_attn=False,\\n        use_flash_attn=True,\\n        ffn_hidden_size=22016,\\n        no_bias=True,\\n        tie_word_embeddings=False,\\n        **kwargs,\\n    ):\\n        self.eos_token_id = eos_token_id\\n        super().__init__(eos_token_id=eos_token_id,\\n                         tie_word_embeddings=tie_word_embeddings,\\n                         **kwargs)\\n\\n        self.vocab_size = vocab_size\\n        self.n_embd = n_embd\\n        self.n_layer = n_layer\\n        self.n_head = n_head\\n        self.n_inner = n_inner\\n        self.embd_pdrop = embd_pdrop\\n        self.attn_pdrop = attn_pdrop\\n        self.layer_norm_epsilon = layer_norm_epsilon\\n        self.initializer_range = initializer_range\\n        self.scale_attn_weights = scale_attn_weights\\n        self.use_cache = use_cache\\n        self.apply_residual_connection_post_layernorm = (\\n            apply_residual_connection_post_layernorm)\\n        self.bf16 = bf16\\n        self.kv_channels = kv_channels\\n        self.rotary_pct = rotary_pct\\n        self.rotary_emb_base = rotary_emb_base\\n        self.use_dynamic_ntk = use_dynamic_ntk\\n        self.use_logn_attn = use_logn_attn\\n        self.use_flash_attn = use_flash_attn\\n        self.ffn_hidden_size = ffn_hidden_size\\n        self.no_bias = no_bias\\n        self.tie_word_embeddings = tie_word_embeddings\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/transformers_utils/configs/mpt.py\\n---------\\nContent:\\n# Adapted from\\n# https://huggingface.co/mosaicml/mpt-7b/blob/main/configuration_mpt.py\\nfrom typing import Any, Dict, Optional, Union\\n\\nfrom transformers import PretrainedConfig\\n\\n_ATTN_CONFIG_DEFAULTS = {\\n    \"attn_type\": \"multihead_attention\",\\n    \"attn_pdrop\": 0.0,\\n    \"attn_impl\": \"triton\",\\n    \"qk_ln\": False,\\n    \"clip_qkv\": None,\\n    \"softmax_scale\": None,\\n    \"prefix_lm\": False,\\n    \"attn_uses_sequence_id\": False,\\n    \"alibi\": False,\\n    \"alibi_bias_max\": 8,\\n}\\n\\n\\nclass MPTConfig(PretrainedConfig):\\n    model_type = \"mpt\"\\n    attribute_map = {\\n        \"hidden_size\": \"d_model\",\\n        \"num_attention_heads\": \"n_heads\",\\n        \"num_hidden_layers\": \"n_layers\",\\n    }\\n\\n    def __init__(\\n        self,\\n        d_model: int = 2048,\\n        n_heads: int = 16,\\n        n_layers: int = 24,\\n        expansion_ratio: int = 4,\\n        max_seq_len: int = 2048,\\n        vocab_size: int = 50368,\\n        resid_pdrop: float = 0.0,\\n        emb_pdrop: float = 0.0,\\n        learned_pos_emb: bool = True,\\n        attn_config: Optional[Dict[str, Any]] = None,\\n        init_device: str = \"cpu\",\\n        logit_scale: Optional[Union[float, str]] = None,\\n        no_bias: bool = False,\\n        verbose: int = 0,\\n        embedding_fraction: float = 1.0,\\n        norm_type: str = \"low_precision_layernorm\",\\n        use_cache: bool = False,\\n        **kwargs,\\n    ) -> None:\\n        self.d_model = d_model\\n        self.n_heads = n_heads\\n        self.n_layers = n_layers\\n        self.expansion_ratio = expansion_ratio\\n        self.max_seq_len = max_seq_len\\n        self.vocab_size = vocab_size\\n        self.resid_pdrop = resid_pdrop\\n        self.emb_pdrop = emb_pdrop\\n        self.learned_pos_emb = learned_pos_emb\\n        if attn_config is None:\\n            self.attn_config = _ATTN_CONFIG_DEFAULTS\\n        else:\\n            self.attn_config = attn_config\\n        self.init_device = init_device\\n        self.logit_scale = logit_scale\\n        self.no_bias = no_bias\\n        self.verbose = verbose\\n        self.embedding_fraction = embedding_fraction\\n        self.norm_type = norm_type\\n        self.use_cache = use_cache\\n        if \"name\" in kwargs:\\n            del kwargs[\"name\"]\\n        if \"loss_fn\" in kwargs:\\n            del kwargs[\"loss_fn\"]\\n        super().__init__(**kwargs)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/transformers_utils/configs/baichuan.py\\n---------\\nContent:\\n# coding=utf-8\\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\\n#\\n# This code is based on EleutherAI\\'s GPT-NeoX library and the GPT-NeoX\\n# and OPT implementations in this library. It has been modified from its\\n# original forms to accommodate minor architectural differences compared\\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\nfrom transformers.configuration_utils import PretrainedConfig\\n\\n\\nclass BaiChuanConfig(PretrainedConfig):\\n    model_type = \"baichuan\"\\n    keys_to_ignore_at_inference = [\"past_key_values\"]\\n\\n    def __init__(\\n        self,\\n        vocab_size=64000,\\n        hidden_size=4096,\\n        intermediate_size=11008,\\n        num_hidden_layers=32,\\n        num_attention_heads=32,\\n        hidden_act=\"silu\",\\n        max_position_embeddings=4096,\\n        initializer_range=0.02,\\n        rms_norm_eps=1e-6,\\n        use_cache=True,\\n        pad_token_id=0,\\n        bos_token_id=1,\\n        eos_token_id=2,\\n        tie_word_embeddings=False,\\n        **kwargs,\\n    ):\\n        self.vocab_size = vocab_size\\n        self.max_position_embeddings = max_position_embeddings\\n        self.hidden_size = hidden_size\\n        self.intermediate_size = intermediate_size\\n        self.num_hidden_layers = num_hidden_layers\\n        self.num_attention_heads = num_attention_heads\\n        self.hidden_act = hidden_act\\n        self.initializer_range = initializer_range\\n        self.rms_norm_eps = rms_norm_eps\\n        self.use_cache = use_cache\\n        super().__init__(\\n            pad_token_id=pad_token_id,\\n            bos_token_id=bos_token_id,\\n            eos_token_id=eos_token_id,\\n            tie_word_embeddings=tie_word_embeddings,\\n            **kwargs,\\n        )\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/transformers_utils/configs/falcon.py\\n---------\\nContent:\\n# Adapted from\\n# https://huggingface.co/tiiuae/falcon-7b/blob/main/configuration_RW.py\\n# Copyright 2023 The vLLM team.\\n# Copyright 2022 the Big Science Workshop and HuggingFace Inc. team.\\n# All rights reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\"\"\"Falcon configuration\"\"\"\\nfrom transformers.configuration_utils import PretrainedConfig\\n\\n\\nclass RWConfig(PretrainedConfig):\\n    model_type = \"falcon\"\\n    keys_to_ignore_at_inference = [\"past_key_values\"]\\n    attribute_map = {\\n        \"num_hidden_layers\": \"n_layer\",\\n        \"num_attention_heads\": \"n_head\",\\n        \"num_kv_heads\": \"n_head_kv\",\\n    }\\n\\n    def __init__(\\n        self,\\n        vocab_size=250880,\\n        hidden_size=64,\\n        n_layer=2,\\n        n_head=8,\\n        layer_norm_epsilon=1e-5,\\n        initializer_range=0.02,\\n        use_cache=True,\\n        bos_token_id=1,\\n        eos_token_id=2,\\n        hidden_dropout=0.0,\\n        attention_dropout=0.0,\\n        multi_query=True,\\n        n_head_kv=None,\\n        alibi=False,\\n        bias=False,\\n        parallel_attn=False,\\n        new_decoder_architecture=False,\\n        **kwargs,\\n    ) -> None:\\n        self.vocab_size = vocab_size\\n        # Backward compatibility with n_embed kwarg\\n        n_embed = kwargs.pop(\"n_embed\", None)\\n        self.hidden_size = hidden_size if n_embed is None else n_embed\\n        self.n_layer = n_layer\\n        self.n_head = n_head\\n        self.layer_norm_epsilon = layer_norm_epsilon\\n        self.initializer_range = initializer_range\\n        self.use_cache = use_cache\\n        self.hidden_dropout = hidden_dropout\\n        self.attention_dropout = attention_dropout\\n\\n        self.bos_token_id = bos_token_id\\n        self.eos_token_id = eos_token_id\\n        self.multi_query = multi_query\\n        self.n_head_kv = 1 if n_head_kv is None else n_head_kv\\n        self.alibi = alibi\\n        self.bias = bias\\n        self.parallel_attn = parallel_attn\\n        self.new_decoder_architecture = new_decoder_architecture\\n\\n        if self.hidden_size == 8192:\\n            # Hack for falcon-40b\\n            self.new_decoder_architecture = True\\n\\n        super().__init__(bos_token_id=bos_token_id,\\n                         eos_token_id=eos_token_id,\\n                         **kwargs)\\n\\n    @property\\n    def head_dim(self):\\n        return self.hidden_size // self.n_head\\n\\n    @property\\n    def rotary(self):\\n        return not self.alibi\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/worker/worker.py\\n---------\\nContent:\\n\"\"\"A GPU worker class.\"\"\"\\nimport os\\nfrom typing import Dict, List, Tuple, Optional\\n\\nimport torch\\nimport torch.distributed\\n\\nfrom vllm.config import (CacheConfig, ModelConfig, ParallelConfig,\\n                         SchedulerConfig)\\nfrom vllm.model_executor import get_model, InputMetadata, set_random_seed\\nfrom vllm.model_executor.parallel_utils.parallel_state import (\\n    initialize_model_parallel)\\nfrom vllm.sampling_params import SamplingParams\\nfrom vllm.sequence import SamplerOutput, SequenceData, SequenceGroupMetadata\\nfrom vllm.worker.cache_engine import CacheEngine\\nfrom vllm.utils import get_gpu_memory\\n\\n\\nclass Worker:\\n    \"\"\"A worker class that executes (a partition of) the model on a GPU.\\n\\n    Each worker is associated with a single GPU. The worker is responsible for\\n    maintaining the KV cache and executing the model on the GPU. In case of\\n    distributed inference, each worker is assigned a partition of the model.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        model_config: ModelConfig,\\n        parallel_config: ParallelConfig,\\n        scheduler_config: SchedulerConfig,\\n        rank: Optional[int] = None,\\n        distributed_init_method: Optional[str] = None,\\n    ) -> None:\\n        self.model_config = model_config\\n        self.parallel_config = parallel_config\\n        self.scheduler_config = scheduler_config\\n        self.rank = rank\\n        self.distributed_init_method = distributed_init_method\\n\\n        # Uninitialized cache engine. Will be initialized by\\n        # self.init_cache_engine().\\n        self.cache_config = None\\n        self.block_size = None\\n        self.cache_engine = None\\n        self.cache_events = None\\n        self.gpu_cache = None\\n\\n    def init_model(self):\\n        # This env var set by Ray causes exceptions with graph building.\\n        os.environ.pop(\"NCCL_ASYNC_ERROR_HANDLING\", None)\\n        # Env vars will be set by Ray.\\n        self.rank = self.rank if self.rank is not None else int(\\n            os.getenv(\"RANK\", \"-1\"))\\n        local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\\n        self.device = torch.device(f\"cuda:{local_rank}\")\\n        if self.rank < 0:\\n            raise ValueError(\"Invalid or unspecified rank.\")\\n        torch.cuda.set_device(self.device)\\n\\n        # Initialize the distributed environment.\\n        _init_distributed_environment(self.parallel_config, self.rank,\\n                                      self.distributed_init_method)\\n\\n        # Initialize the model.\\n        set_random_seed(self.model_config.seed)\\n        self.model = get_model(self.model_config)\\n\\n    @torch.inference_mode()\\n    def profile_num_available_blocks(\\n        self,\\n        block_size: int,\\n        gpu_memory_utilization: float,\\n        cpu_swap_space: int,\\n    ) -> Tuple[int, int]:\\n        # Profile the memory usage of the model and get the maximum number of\\n        # cache blocks that can be allocated with the remaining free memory.\\n        torch.cuda.empty_cache()\\n        torch.cuda.reset_peak_memory_stats()\\n\\n        # Profile memory usage with max_num_sequences sequences and the total\\n        # number of tokens equal to max_num_batched_tokens.\\n\\n        # Enable top-k sampling to reflect the accurate memory usage.\\n        vocab_size = self.model.config.vocab_size\\n        sampling_params = SamplingParams(top_p=0.99, top_k=vocab_size - 1)\\n        max_num_batched_tokens = self.scheduler_config.max_num_batched_tokens\\n        max_num_seqs = self.scheduler_config.max_num_seqs\\n        seqs = []\\n        for group_id in range(max_num_seqs):\\n            seq_len = (max_num_batched_tokens // max_num_seqs +\\n                       (group_id < max_num_batched_tokens % max_num_seqs))\\n            seq_data = SequenceData([0] * seq_len)\\n            seq = SequenceGroupMetadata(\\n                request_id=str(group_id),\\n                is_prompt=True,\\n                seq_data={group_id: seq_data},\\n                sampling_params=sampling_params,\\n                block_tables=None,\\n            )\\n            seqs.append(seq)\\n\\n        input_tokens, input_positions, input_metadata = self._prepare_inputs(\\n            seqs)\\n\\n        # Execute the model.\\n        num_layers = self.model_config.get_num_layers(self.parallel_config)\\n        self.model(\\n            input_ids=input_tokens,\\n            positions=input_positions,\\n            kv_caches=[(None, None)] * num_layers,\\n            input_metadata=input_metadata,\\n            cache_events=None,\\n        )\\n\\n        # Calculate the number of blocks that can be allocated with the\\n        # profiled peak memory.\\n        torch.cuda.synchronize()\\n        peak_memory = torch.cuda.max_memory_allocated()\\n        total_gpu_memory = get_gpu_memory()\\n        cache_block_size = CacheEngine.get_cache_block_size(\\n            block_size, self.model_config, self.parallel_config)\\n        num_gpu_blocks = int(\\n            (total_gpu_memory * gpu_memory_utilization - peak_memory) //\\n            cache_block_size)\\n        num_cpu_blocks = int(cpu_swap_space // cache_block_size)\\n        num_gpu_blocks = max(num_gpu_blocks, 0)\\n        num_cpu_blocks = max(num_cpu_blocks, 0)\\n        torch.cuda.empty_cache()\\n\\n        # Reset the seed to ensure that the random state is not affected by\\n        # the model initialization and profiling.\\n        set_random_seed(self.model_config.seed)\\n        return num_gpu_blocks, num_cpu_blocks\\n\\n    def init_cache_engine(self, cache_config: CacheConfig) -> None:\\n        self.cache_config = cache_config\\n        self.block_size = cache_config.block_size\\n        self.cache_engine = CacheEngine(self.cache_config, self.model_config,\\n                                        self.parallel_config)\\n        self.cache_events = self.cache_engine.events\\n        self.gpu_cache = self.cache_engine.gpu_cache\\n\\n    def _prepare_inputs(\\n        self,\\n        seq_group_metadata_list: List[SequenceGroupMetadata],\\n    ) -> Tuple[torch.Tensor, torch.Tensor, InputMetadata]:\\n        seq_groups: List[Tuple[List[int], SamplingParams]] = []\\n        input_tokens: List[int] = []\\n        input_positions: List[int] = []\\n        slot_mapping: List[int] = []\\n\\n        # Add prompt tokens.\\n        prompt_lens: List[int] = []\\n        for seq_group_metadata in seq_group_metadata_list:\\n            if not seq_group_metadata.is_prompt:\\n                continue\\n\\n            seq_ids = list(seq_group_metadata.seq_data.keys())\\n            sampling_params = seq_group_metadata.sampling_params\\n            seq_groups.append((seq_ids, sampling_params))\\n\\n            # Use any sequence in the group.\\n            seq_id = seq_ids[0]\\n\\n            seq_data = seq_group_metadata.seq_data[seq_id]\\n            prompt_tokens = seq_data.get_token_ids()\\n            prompt_len = len(prompt_tokens)\\n            prompt_lens.append(prompt_len)\\n\\n            input_tokens.extend(prompt_tokens)\\n            # NOTE(woosuk): Here we assume that the first token in the prompt\\n            # is always the first token in the sequence.\\n            input_positions.extend(range(len(prompt_tokens)))\\n\\n            if seq_group_metadata.block_tables is None:\\n                # During memory profiling, the block tables are not initialized\\n                # yet. In this case, we just use a dummy slot mapping.\\n                slot_mapping.extend([0] * prompt_len)\\n                continue\\n\\n            # Compute the slot mapping.\\n            block_table = seq_group_metadata.block_tables[seq_id]\\n            for i in range(prompt_len):\\n                block_number = block_table[i // self.block_size]\\n                block_offset = i % self.block_size\\n                slot = block_number * self.block_size + block_offset\\n                slot_mapping.append(slot)\\n\\n        # Add generation tokens.\\n        max_context_len = 0\\n        max_num_blocks_per_seq = 0\\n        context_lens: List[int] = []\\n        generation_block_tables: List[List[int]] = []\\n        for seq_group_metadata in seq_group_metadata_list:\\n            if seq_group_metadata.is_prompt:\\n                continue\\n\\n            seq_ids = list(seq_group_metadata.seq_data.keys())\\n            sampling_params = seq_group_metadata.sampling_params\\n            seq_groups.append((seq_ids, sampling_params))\\n\\n            for seq_id in seq_ids:\\n                seq_data = seq_group_metadata.seq_data[seq_id]\\n                generation_token = seq_data.get_last_token_id()\\n                input_tokens.append(generation_token)\\n\\n                context_len = seq_data.get_len()\\n                position = context_len - 1\\n                input_positions.append(position)\\n\\n                block_table = seq_group_metadata.block_tables[seq_id]\\n                generation_block_tables.append(block_table)\\n\\n                max_context_len = max(max_context_len, context_len)\\n                max_num_blocks_per_seq = max(max_num_blocks_per_seq,\\n                                             len(block_table))\\n                context_lens.append(context_len)\\n\\n                block_number = block_table[position // self.block_size]\\n                block_offset = position % self.block_size\\n                slot = block_number * self.block_size + block_offset\\n                slot_mapping.append(slot)\\n\\n        # Optimization: Pad the input length to be a multiple of 8.\\n        # This is required for utilizing the Tensor Cores in NVIDIA GPUs.\\n        input_tokens = _pad_to_alignment(input_tokens, multiple_of=8)\\n        input_positions = _pad_to_alignment(input_positions, multiple_of=8)\\n\\n        # Convert to tensors.\\n        tokens_tensor = torch.cuda.LongTensor(input_tokens)\\n        positions_tensor = torch.cuda.LongTensor(input_positions)\\n        slot_mapping_tensor = torch.cuda.IntTensor(slot_mapping)\\n        context_lens_tensor = torch.cuda.IntTensor(context_lens)\\n        padded_block_tables = [\\n            _pad_to_max(block_table, max_num_blocks_per_seq)\\n            for block_table in generation_block_tables\\n        ]\\n        block_tables_tensor = torch.cuda.IntTensor(padded_block_tables)\\n\\n        seq_data: Dict[int, SequenceData] = {}\\n        for seq_group_metadata in seq_group_metadata_list:\\n            seq_data.update(seq_group_metadata.seq_data)\\n\\n        input_metadata = InputMetadata(\\n            seq_groups=seq_groups,\\n            seq_data=seq_data,\\n            prompt_lens=prompt_lens,\\n            slot_mapping=slot_mapping_tensor,\\n            context_lens=context_lens_tensor,\\n            max_context_len=max_context_len,\\n            block_tables=block_tables_tensor,\\n        )\\n        return tokens_tensor, positions_tensor, input_metadata\\n\\n    @torch.inference_mode()\\n    def execute_model(\\n        self,\\n        seq_group_metadata_list: List[SequenceGroupMetadata],\\n        blocks_to_swap_in: Dict[int, int],\\n        blocks_to_swap_out: Dict[int, int],\\n        blocks_to_copy: Dict[int, List[int]],\\n    ) -> SamplerOutput:\\n        # Issue cache operations.\\n        issued_cache_op = False\\n        if blocks_to_swap_in:\\n            self.cache_engine.swap_in(blocks_to_swap_in)\\n            issued_cache_op = True\\n        if blocks_to_swap_out:\\n            self.cache_engine.swap_out(blocks_to_swap_out)\\n            issued_cache_op = True\\n        if blocks_to_copy:\\n            self.cache_engine.copy(blocks_to_copy)\\n            issued_cache_op = True\\n\\n        if issued_cache_op:\\n            cache_events = self.cache_events\\n        else:\\n            cache_events = None\\n\\n        # If there is no input, we don\\'t need to execute the model.\\n        if not seq_group_metadata_list:\\n            if cache_events is not None:\\n                for event in cache_events:\\n                    event.wait()\\n            return {}\\n\\n        # Prepare input tensors.\\n        input_tokens, input_positions, input_metadata = self._prepare_inputs(\\n            seq_group_metadata_list)\\n\\n        # Execute the model.\\n        output = self.model(\\n            input_ids=input_tokens,\\n            positions=input_positions,\\n            kv_caches=self.gpu_cache,\\n            input_metadata=input_metadata,\\n            cache_events=cache_events,\\n        )\\n        return output\\n\\n\\ndef _init_distributed_environment(\\n    parallel_config: ParallelConfig,\\n    rank: int,\\n    distributed_init_method: Optional[str] = None,\\n) -> None:\\n    \"\"\"Initialize the distributed environment.\"\"\"\\n    if torch.distributed.is_initialized():\\n        torch_world_size = torch.distributed.get_world_size()\\n        if torch_world_size != parallel_config.world_size:\\n            raise RuntimeError(\\n                \"torch.distributed is already initialized but the torch world \"\\n                \"size does not match parallel_config.world_size \"\\n                f\"({torch_world_size} vs. {parallel_config.world_size}).\")\\n    elif not distributed_init_method:\\n        raise ValueError(\\n            \"distributed_init_method must be set if torch.distributed \"\\n            \"is not already initialized\")\\n    else:\\n        torch.distributed.init_process_group(\\n            backend=\"nccl\",\\n            world_size=parallel_config.world_size,\\n            rank=rank,\\n            init_method=distributed_init_method,\\n        )\\n\\n    # A small all_reduce for warmup.\\n    torch.distributed.all_reduce(torch.zeros(1).cuda())\\n    initialize_model_parallel(parallel_config.tensor_parallel_size,\\n                              parallel_config.pipeline_parallel_size)\\n\\n\\ndef _pad_to_alignment(x: List[int], multiple_of: int) -> List[int]:\\n    return x + [0] * ((-len(x)) % multiple_of)\\n\\n\\ndef _pad_to_max(x: List[int], max_len: int) -> List[int]:\\n    return x + [0] * (max_len - len(x))\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/worker/cache_engine.py\\n---------\\nContent:\\n\"\"\"CacheEngine class for managing the KV cache.\"\"\"\\nfrom typing import Dict, List, Tuple\\n\\nimport torch\\n\\nfrom vllm import cache_ops\\nfrom vllm.config import CacheConfig, ModelConfig, ParallelConfig\\nfrom vllm.logger import init_logger\\nfrom vllm.utils import in_wsl\\n\\nlogger = init_logger(__name__)\\n\\nKVCache = Tuple[torch.Tensor, torch.Tensor]\\n\\n\\nclass CacheEngine:\\n    \"\"\"Manages the KV cache.\\n\\n    This class is responsible for initializing and managing the GPU and CPU KV\\n    caches. It also provides methods for performing KV cache operations, such\\n    as swapping and copying.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        cache_config: CacheConfig,\\n        model_config: ModelConfig,\\n        parallel_config: ParallelConfig,\\n    ) -> None:\\n        self.cache_config = cache_config\\n        self.model_config = model_config\\n        self.parallel_config = parallel_config\\n\\n        self.head_size = model_config.get_head_size()\\n        self.num_layers = model_config.get_num_layers(parallel_config)\\n        self.num_heads = model_config.get_num_heads(parallel_config)\\n        self.dtype = model_config.dtype\\n\\n        self.block_size = cache_config.block_size\\n        self.num_gpu_blocks = cache_config.num_gpu_blocks\\n        self.num_cpu_blocks = cache_config.num_cpu_blocks\\n\\n        # Initialize the cache.\\n        self.gpu_cache = self.allocate_gpu_cache()\\n        self.cpu_cache = self.allocate_cpu_cache()\\n\\n        # Initialize the stream for caching operations.\\n        self.cache_stream = torch.cuda.Stream()\\n        assert self.cache_stream != torch.cuda.current_stream()\\n        # Initialize the events for stream synchronization.\\n        self.events = [torch.cuda.Event() for _ in range(self.num_layers)]\\n\\n    def get_key_block_shape(self) -> Tuple[int, int, int, int]:\\n        element_size = torch.tensor([], dtype=self.dtype).element_size()\\n        x = 16 // element_size\\n        return (\\n            self.num_heads,\\n            self.head_size // x,\\n            self.block_size,\\n            x,\\n        )\\n\\n    def get_value_block_shape(self) -> Tuple[int, int, int]:\\n        return (\\n            self.num_heads,\\n            self.head_size,\\n            self.block_size,\\n        )\\n\\n    def allocate_gpu_cache(self) -> List[KVCache]:\\n        gpu_cache: List[KVCache] = []\\n        key_block_shape = self.get_key_block_shape()\\n        value_block_shape = self.get_value_block_shape()\\n        for _ in range(self.num_layers):\\n            key_blocks = torch.empty(\\n                size=(self.num_gpu_blocks, *key_block_shape),\\n                dtype=self.dtype,\\n                device=\"cuda\",\\n            )\\n            value_blocks = torch.empty(\\n                size=(self.num_gpu_blocks, *value_block_shape),\\n                dtype=self.dtype,\\n                device=\"cuda\",\\n            )\\n            gpu_cache.append((key_blocks, value_blocks))\\n        return gpu_cache\\n\\n    def allocate_cpu_cache(self) -> List[KVCache]:\\n        cpu_cache: List[KVCache] = []\\n        key_block_shape = self.get_key_block_shape()\\n        value_block_shape = self.get_value_block_shape()\\n        pin_memory = not in_wsl()\\n        if not pin_memory:\\n            # Pinning memory in WSL is not supported.\\n            # https://docs.nvidia.com/cuda/wsl-user-guide/index.html#known-limitations-for-linux-cuda-applications\\n            logger.warning(\"Using \\'pin_memory=False\\' as WSL is detected. \"\\n                           \"This may slow down the performance.\")\\n        for _ in range(self.num_layers):\\n            key_blocks = torch.empty(\\n                size=(self.num_cpu_blocks, *key_block_shape),\\n                dtype=self.dtype,\\n                pin_memory=pin_memory,\\n            )\\n            value_blocks = torch.empty(\\n                size=(self.num_cpu_blocks, *value_block_shape),\\n                dtype=self.dtype,\\n                pin_memory=pin_memory,\\n            )\\n            cpu_cache.append((key_blocks, value_blocks))\\n        return cpu_cache\\n\\n    def _swap(\\n        self,\\n        src: List[KVCache],\\n        dst: List[KVCache],\\n        src_to_dst: Dict[int, int],\\n    ) -> None:\\n        with torch.cuda.stream(self.cache_stream):\\n            for i in range(self.num_layers):\\n                src_key_cache, src_value_cache = src[i]\\n                dst_key_cache, dst_value_cache = dst[i]\\n                # Copy the key blocks.\\n                cache_ops.swap_blocks(src_key_cache, dst_key_cache, src_to_dst)\\n                # Copy the value blocks.\\n                cache_ops.swap_blocks(src_value_cache, dst_value_cache,\\n                                      src_to_dst)\\n                event = self.events[i]\\n                event.record(stream=self.cache_stream)\\n\\n    def swap_in(self, src_to_dst: Dict[int, int]) -> None:\\n        self._swap(self.cpu_cache, self.gpu_cache, src_to_dst)\\n\\n    def swap_out(self, src_to_dst: Dict[int, int]) -> None:\\n        self._swap(self.gpu_cache, self.cpu_cache, src_to_dst)\\n\\n    def copy(self, src_to_dsts: Dict[int, List[int]]) -> None:\\n        key_caches = [key_cache for key_cache, _ in self.gpu_cache]\\n        value_caches = [value_cache for _, value_cache in self.gpu_cache]\\n        # NOTE(woosuk): This operation implicitly synchronizes the CPU and GPU.\\n        cache_ops.copy_blocks(key_caches, value_caches, src_to_dsts)\\n\\n    @staticmethod\\n    def get_cache_block_size(\\n        block_size: int,\\n        model_config: ModelConfig,\\n        parallel_config: ParallelConfig,\\n    ) -> int:\\n        head_size = model_config.get_head_size()\\n        num_heads = model_config.get_num_heads(parallel_config)\\n        num_layers = model_config.get_num_layers(parallel_config)\\n\\n        key_cache_block = block_size * num_heads * head_size\\n        value_cache_block = key_cache_block\\n        total = num_layers * (key_cache_block + value_cache_block)\\n        dtype_size = _get_dtype_size(model_config.dtype)\\n        return dtype_size * total\\n\\n\\ndef _get_dtype_size(dtype: torch.dtype) -> int:\\n    return torch.tensor([], dtype=dtype).element_size()\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/worker/__init__.py\\n---------\\nContent:\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/engine/llm_engine.py\\n---------\\nContent:\\nimport copy\\nimport time\\nfrom functools import partial\\nfrom typing import TYPE_CHECKING, Any, Iterable, List, Optional, Tuple, Union\\n\\nfrom vllm.config import (CacheConfig, ModelConfig, ParallelConfig,\\n                         SchedulerConfig)\\nfrom vllm.core.scheduler import Scheduler, SchedulerOutputs\\nfrom vllm.engine.arg_utils import EngineArgs\\nfrom vllm.engine.ray_utils import RayWorker, initialize_cluster, ray\\nfrom vllm.logger import init_logger\\nfrom vllm.outputs import RequestOutput\\nfrom vllm.sampling_params import SamplingParams\\nfrom vllm.sequence import (SamplerOutput, Sequence, SequenceGroup,\\n                           SequenceGroupMetadata, SequenceOutputs,\\n                           SequenceStatus)\\nfrom vllm.transformers_utils.tokenizer import (detokenize_incrementally,\\n                                               get_tokenizer)\\nfrom vllm.utils import Counter\\n\\nif ray:\\n    from ray.air.util.torch_dist import init_torch_dist_process_group\\n    from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy\\n\\nif TYPE_CHECKING:\\n    from ray.util.placement_group import PlacementGroup\\n\\nlogger = init_logger(__name__)\\n\\n_LOGGING_INTERVAL_SEC = 5\\n\\n\\nclass LLMEngine:\\n    \"\"\"An LLM engine that receives requests and generates texts.\\n\\n    This is the main class for the vLLM engine. It receives requests\\n    from clients and generates texts from the LLM. It includes a tokenizer, a\\n    language model (possibly distributed across multiple GPUs), and GPU memory\\n    space allocated for intermediate states (aka KV cache). This class utilizes\\n    iteration-level scheduling and efficient memory management to maximize the\\n    serving throughput.\\n\\n    The `LLM` class wraps this class for offline batched inference and the\\n    `AsyncLLMEngine` class wraps this class for online serving.\\n\\n    NOTE: The config arguments are derived from the `EngineArgs` class. For the\\n    comprehensive list of arguments, see `EngineArgs`.\\n\\n    Args:\\n        model_config: The configuration related to the LLM model.\\n        cache_config: The configuration related to the KV cache memory\\n            management.\\n        parallel_config: The configuration related to distributed execution.\\n        scheduler_config: The configuration related to the request scheduler.\\n        distributed_init_method: The initialization method for distributed\\n            execution. See `torch.distributed.init_process_group` for details.\\n        stage_devices: The list of devices for each stage. Each stage is a list\\n            of (rank, node_resource, device) tuples.\\n        log_stats: Whether to log statistics.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        model_config: ModelConfig,\\n        cache_config: CacheConfig,\\n        parallel_config: ParallelConfig,\\n        scheduler_config: SchedulerConfig,\\n        distributed_init_method: str,\\n        placement_group: Optional[\"PlacementGroup\"],\\n        log_stats: bool,\\n    ) -> None:\\n        logger.info(\\n            \"Initializing an LLM engine with config: \"\\n            f\"model={model_config.model!r}, \"\\n            f\"tokenizer={model_config.tokenizer!r}, \"\\n            f\"tokenizer_mode={model_config.tokenizer_mode}, \"\\n            f\"revision={model_config.revision}, \"\\n            f\"trust_remote_code={model_config.trust_remote_code}, \"\\n            f\"dtype={model_config.dtype}, \"\\n            f\"download_dir={model_config.download_dir!r}, \"\\n            f\"load_format={model_config.load_format}, \"\\n            f\"tensor_parallel_size={parallel_config.tensor_parallel_size}, \"\\n            f\"quantization={model_config.quantization}, \"\\n            f\"seed={model_config.seed})\")\\n        # TODO(woosuk): Print more configs in debug mode.\\n\\n        self.model_config = model_config\\n        self.cache_config = cache_config\\n        self.parallel_config = parallel_config\\n        self.scheduler_config = scheduler_config\\n        self.log_stats = log_stats\\n        self._verify_args()\\n\\n        self.tokenizer = get_tokenizer(\\n            model_config.tokenizer,\\n            tokenizer_mode=model_config.tokenizer_mode,\\n            trust_remote_code=model_config.trust_remote_code,\\n            revision=model_config.revision)\\n        self.seq_counter = Counter()\\n\\n        # Create the parallel GPU workers.\\n        if self.parallel_config.worker_use_ray:\\n            self._init_workers_ray(placement_group)\\n        else:\\n            self._init_workers(distributed_init_method)\\n\\n        # Profile the memory usage and initialize the cache.\\n        self._init_cache()\\n\\n        # Create the scheduler.\\n        self.scheduler = Scheduler(scheduler_config, cache_config)\\n\\n        # Logging.\\n        self.last_logging_time = 0.0\\n        # List of (timestamp, num_tokens)\\n        self.num_prompt_tokens: List[Tuple[float, int]] = []\\n        # List of (timestamp, num_tokens)\\n        self.num_generation_tokens: List[Tuple[float, int]] = []\\n\\n    def _init_workers(self, distributed_init_method: str):\\n        # Lazy import the Worker to avoid importing torch.cuda/xformers\\n        # before CUDA_VISIBLE_DEVICES is set in the Worker\\n        from vllm.worker.worker import Worker  # pylint: disable=import-outside-toplevel\\n\\n        assert self.parallel_config.world_size == 1, (\\n            \"Ray is required if parallel_config.world_size > 1.\")\\n\\n        self.workers: List[Worker] = []\\n        worker = Worker(\\n            self.model_config,\\n            self.parallel_config,\\n            self.scheduler_config,\\n            0,\\n            distributed_init_method,\\n        )\\n        self.workers.append(worker)\\n        self._run_workers(\\n            \"init_model\",\\n            get_all_outputs=True,\\n        )\\n\\n    def _init_workers_ray(self, placement_group: \"PlacementGroup\",\\n                          **ray_remote_kwargs):\\n        # Lazy import the Worker to avoid importing torch.cuda/xformers\\n        # before CUDA_VISIBLE_DEVICES is set in the Worker\\n        from vllm.worker.worker import Worker  # pylint: disable=import-outside-toplevel\\n\\n        self.workers: List[Worker] = []\\n        for bundle in placement_group.bundle_specs:\\n            if not bundle.get(\"GPU\", 0):\\n                continue\\n            worker = ray.remote(\\n                num_cpus=0,\\n                num_gpus=1,\\n                scheduling_strategy=PlacementGroupSchedulingStrategy(\\n                    placement_group=placement_group,\\n                    placement_group_capture_child_tasks=True),\\n                **ray_remote_kwargs,\\n            )(RayWorker).remote(self.model_config.trust_remote_code)\\n            self.workers.append(worker)\\n\\n        # Initialize torch distributed process group for the workers.\\n        init_torch_dist_process_group(self.workers, backend=\"nccl\")\\n        model_config = copy.deepcopy(self.model_config)\\n        parallel_config = copy.deepcopy(self.parallel_config)\\n        scheduler_config = copy.deepcopy(self.scheduler_config)\\n        self._run_workers(\"init_worker\",\\n                          get_all_outputs=True,\\n                          worker_init_fn=lambda: Worker(\\n                              model_config,\\n                              parallel_config,\\n                              scheduler_config,\\n                              None,\\n                              None,\\n                          ))\\n        self._run_workers(\\n            \"init_model\",\\n            get_all_outputs=True,\\n        )\\n\\n    def _verify_args(self) -> None:\\n        self.model_config.verify_with_parallel_config(self.parallel_config)\\n        self.cache_config.verify_with_parallel_config(self.parallel_config)\\n\\n    def _init_cache(self) -> None:\\n        \"\"\"Profiles the memory usage and initializes the KV cache.\"\"\"\\n        # Get the maximum number of blocks that can be allocated on GPU and CPU.\\n        num_blocks = self._run_workers(\\n            \"profile_num_available_blocks\",\\n            get_all_outputs=True,\\n            block_size=self.cache_config.block_size,\\n            gpu_memory_utilization=self.cache_config.gpu_memory_utilization,\\n            cpu_swap_space=self.cache_config.swap_space_bytes,\\n        )\\n\\n        # Since we use a shared centralized controller, we take the minimum\\n        # number of blocks across all workers to make sure all the memory\\n        # operators can be applied to all workers.\\n        num_gpu_blocks = min(b[0] for b in num_blocks)\\n        num_cpu_blocks = min(b[1] for b in num_blocks)\\n        # FIXME(woosuk): Change to debug log.\\n        logger.info(f\"# GPU blocks: {num_gpu_blocks}, \"\\n                    f\"# CPU blocks: {num_cpu_blocks}\")\\n\\n        if num_gpu_blocks <= 0:\\n            raise ValueError(\"No available memory for the cache blocks. \"\\n                             \"Try increasing `gpu_memory_utilization` when \"\\n                             \"initializing the engine.\")\\n\\n        self.cache_config.num_gpu_blocks = num_gpu_blocks\\n        self.cache_config.num_cpu_blocks = num_cpu_blocks\\n\\n        # Initialize the cache.\\n        self._run_workers(\"init_cache_engine\", cache_config=self.cache_config)\\n\\n    @classmethod\\n    def from_engine_args(cls, engine_args: EngineArgs) -> \"LLMEngine\":\\n        \"\"\"Creates an LLM engine from the engine arguments.\"\"\"\\n        # Create the engine configs.\\n        engine_configs = engine_args.create_engine_configs()\\n        parallel_config = engine_configs[2]\\n        # Initialize the cluster.\\n        distributed_init_method, placement_group = initialize_cluster(\\n            parallel_config)\\n        # Create the LLM engine.\\n        engine = cls(*engine_configs,\\n                     distributed_init_method,\\n                     placement_group,\\n                     log_stats=not engine_args.disable_log_stats)\\n        return engine\\n\\n    def add_request(\\n        self,\\n        request_id: str,\\n        prompt: Optional[str],\\n        sampling_params: SamplingParams,\\n        prompt_token_ids: Optional[List[int]] = None,\\n        arrival_time: Optional[float] = None,\\n    ) -> None:\\n        \"\"\"Add a request to the engine\\'s request pool.\\n\\n        The request is added to the request pool and will be processed by the\\n        scheduler as `engine.step()` is called. The exact scheduling policy is\\n        determined by the scheduler.\\n\\n        Args:\\n            request_id: The unique ID of the request.\\n            prompt: The prompt string. Can be None if prompt_token_ids is\\n                provided.\\n            sampling_params: The sampling parameters for text generation.\\n            prompt_token_ids: The token IDs of the prompt. If None, we\\n                use the tokenizer to convert the prompts to token IDs.\\n            arrival_time: The arrival time of the request. If None, we use\\n                the current time.\\n        \"\"\"\\n        if arrival_time is None:\\n            arrival_time = time.time()\\n        if prompt_token_ids is None:\\n            assert prompt is not None\\n            prompt_token_ids = self.tokenizer.encode(prompt)\\n\\n        # Create the sequences.\\n        block_size = self.cache_config.block_size\\n        seq_id = next(self.seq_counter)\\n        seq = Sequence(seq_id, prompt, prompt_token_ids, block_size)\\n\\n        # Create the sequence group.\\n        seq_group = SequenceGroup(request_id, [seq], sampling_params,\\n                                  arrival_time)\\n\\n        # Add the sequence group to the scheduler.\\n        self.scheduler.add_seq_group(seq_group)\\n\\n    def abort_request(self, request_id: Union[str, Iterable[str]]) -> None:\\n        \"\"\"Aborts a request(s) with the given ID.\\n\\n        Args:\\n            request_id: The ID(s) of the request to abort.\\n        \"\"\"\\n        self.scheduler.abort_seq_group(request_id)\\n\\n    def get_model_config(self) -> ModelConfig:\\n        \"\"\"Gets the model configuration.\"\"\"\\n        return self.model_config\\n\\n    def get_num_unfinished_requests(self) -> int:\\n        \"\"\"Gets the number of unfinished requests.\"\"\"\\n        return self.scheduler.get_num_unfinished_seq_groups()\\n\\n    def has_unfinished_requests(self) -> bool:\\n        \"\"\"Returns True if there are unfinished requests.\"\"\"\\n        return self.scheduler.has_unfinished_seqs()\\n\\n    def _schedule(\\n        self\\n    ) -> Tuple[List[SequenceGroupMetadata], SchedulerOutputs,\\n               List[RequestOutput]]:\\n        seq_group_metadata_list, scheduler_outputs = self.scheduler.schedule()\\n        return seq_group_metadata_list, scheduler_outputs, [\\n            RequestOutput.from_seq_group(seq_group)\\n            for seq_group in scheduler_outputs.ignored_seq_groups\\n        ]\\n\\n    def _check_beam_search_early_stopping(\\n        self,\\n        early_stopping: Union[bool, str],\\n        sampling_params: SamplingParams,\\n        best_running_seq: Sequence,\\n        current_worst_seq: Sequence,\\n    ) -> bool:\\n        assert sampling_params.use_beam_search\\n        length_penalty = sampling_params.length_penalty\\n        if early_stopping is True:\\n            return True\\n\\n        current_worst_score = (current_worst_seq.get_beam_search_score(\\n            length_penalty=length_penalty,\\n            eos_token_id=self.tokenizer.eos_token_id))\\n        if early_stopping is False:\\n            highest_attainable_score = (best_running_seq.get_beam_search_score(\\n                length_penalty=length_penalty,\\n                eos_token_id=self.tokenizer.eos_token_id))\\n        else:\\n            assert early_stopping == \"never\"\\n            if length_penalty > 0.0:\\n                # If length_penalty > 0.0, beam search will prefer longer\\n                # sequences. The highest attainable score calculation is\\n                # based on the longest possible sequence length in this case.\\n                max_possible_length = max(\\n                    best_running_seq.get_prompt_len() +\\n                    sampling_params.max_tokens,\\n                    self.scheduler_config.max_model_len)\\n                highest_attainable_score = (\\n                    best_running_seq.get_beam_search_score(\\n                        length_penalty=length_penalty,\\n                        eos_token_id=self.tokenizer.eos_token_id,\\n                        seq_len=max_possible_length))\\n            else:\\n                # Otherwise, beam search will prefer shorter sequences. The\\n                # highest attainable score calculation is based on the current\\n                # sequence length.\\n                highest_attainable_score = (\\n                    best_running_seq.get_beam_search_score(\\n                        length_penalty=length_penalty,\\n                        eos_token_id=self.tokenizer.eos_token_id))\\n        return current_worst_score >= highest_attainable_score\\n\\n    def _process_sequence_group_samples(\\n            self, seq_group: SequenceGroup,\\n            samples: List[SequenceOutputs]) -> None:\\n        parent_seqs = seq_group.get_seqs(status=SequenceStatus.RUNNING)\\n        existing_finished_seqs = seq_group.get_finished_seqs()\\n        parent_child_dict = {\\n            parent_seq.seq_id: []\\n            for parent_seq in parent_seqs\\n        }\\n        for sample in samples:\\n            parent_child_dict[sample.parent_seq_id].append(sample)\\n        # List of (child, parent)\\n        child_seqs: List[Tuple[Sequence, Sequence]] = []\\n\\n        # Process the child samples for each parent sequence\\n        for parent in parent_seqs:\\n            child_samples: List[SequenceOutputs] = parent_child_dict[\\n                parent.seq_id]\\n            if len(child_samples) == 0:\\n                # This parent sequence has no children samples. Remove\\n                # the parent sequence from the sequence group since it will\\n                # not be used in the future iterations.\\n                parent.status = SequenceStatus.FINISHED_ABORTED\\n                seq_group.remove(parent.seq_id)\\n                self.scheduler.free_seq(parent)\\n                continue\\n            # Fork the parent sequence if there are multiple child samples.\\n            for child_sample in child_samples[:-1]:\\n                new_child_seq_id = next(self.seq_counter)\\n                child = parent.fork(new_child_seq_id)\\n                child.append_token_id(child_sample.output_token,\\n                                      child_sample.logprobs)\\n                child_seqs.append((child, parent))\\n            # Continue the parent sequence for the last child sample.\\n            # We reuse the parent sequence here to reduce redundant memory\\n            # copies, especially when using non-beam search sampling methods.\\n            last_child_sample = child_samples[-1]\\n            parent.append_token_id(last_child_sample.output_token,\\n                                   last_child_sample.logprobs)\\n            child_seqs.append((parent, parent))\\n\\n        for seq, _ in child_seqs:\\n            self._decode_sequence(seq)\\n            self._check_stop(seq, seq_group.sampling_params)\\n\\n        # Non-beam search case\\n        if not seq_group.sampling_params.use_beam_search:\\n            # For newly created child sequences, add them to the sequence group\\n            # and fork them in block manager if they are not finished.\\n            for seq, parent in child_seqs:\\n                if seq is not parent:\\n                    seq_group.add(seq)\\n                    if not seq.is_finished():\\n                        self.scheduler.fork_seq(parent, seq)\\n\\n            # Free the finished and selected parent sequences\\' memory in block\\n            # manager. Keep them in the sequence group as candidate output.\\n            # NOTE: we need to fork the new sequences before freeing the\\n            # old sequences.\\n            for seq, parent in child_seqs:\\n                if seq is parent and seq.is_finished():\\n                    self.scheduler.free_seq(seq)\\n            return\\n\\n        # Beam search case\\n        # Select the child sequences to keep in the sequence group.\\n        selected_child_seqs = []\\n        unselected_child_seqs = []\\n        beam_width = seq_group.sampling_params.best_of\\n        length_penalty = seq_group.sampling_params.length_penalty\\n\\n        # Select the newly finished sequences with the highest scores\\n        # to replace existing finished sequences.\\n        # Tuple of (seq, parent, is_new)\\n        existing_finished_seqs = [(seq, None, False)\\n                                  for seq in existing_finished_seqs]\\n        new_finished_seqs = [(seq, parent, True) for seq, parent in child_seqs\\n                             if seq.is_finished()]\\n        all_finished_seqs = existing_finished_seqs + new_finished_seqs\\n        # Sort the finished sequences by their scores.\\n        all_finished_seqs.sort(key=lambda x: x[0].get_beam_search_score(\\n            length_penalty=length_penalty,\\n            eos_token_id=self.tokenizer.eos_token_id),\\n                               reverse=True)\\n        for seq, parent, is_new in all_finished_seqs[:beam_width]:\\n            if is_new:\\n                # A newly generated child sequence finishes and has a high\\n                # score, so we will add it into the sequence group.\\n                selected_child_seqs.append((seq, parent))\\n        for seq, parent, is_new in all_finished_seqs[beam_width:]:\\n            if is_new:\\n                # A newly generated child sequence finishes but has a low\\n                # score, so we will not add it into the sequence group.\\n                # Additionally, if this sequence is a continuation of a\\n                # parent sequence, we will need remove the parent sequence\\n                # from the sequence group.\\n                unselected_child_seqs.append((seq, parent))\\n            else:\\n                # An existing finished sequence has a low score, so we will\\n                # remove it from the sequence group.\\n                seq_group.remove(seq.seq_id)\\n\\n        # select the top beam_width sequences from the running\\n        # sequences for the next iteration to continue the beam\\n        # search.\\n        running_child_seqs = [(seq, parent) for seq, parent in child_seqs\\n                              if not seq.is_finished()]\\n        # Sort the running sequences by their scores.\\n        running_child_seqs.sort(key=lambda x: x[0].get_beam_search_score(\\n            length_penalty=length_penalty,\\n            eos_token_id=self.tokenizer.eos_token_id),\\n                                reverse=True)\\n\\n        # Check if we can stop the beam search.\\n        if len(running_child_seqs) == 0:\\n            # No running sequences, stop the beam search.\\n            stop_beam_search = True\\n        elif len(all_finished_seqs) < beam_width:\\n            # Not enough finished sequences, continue the beam search.\\n            stop_beam_search = False\\n        else:\\n            # Check the early stopping criteria\\n            best_running_seq = running_child_seqs[0][0]\\n            current_worst_seq = all_finished_seqs[beam_width - 1][0]\\n            stop_beam_search = self._check_beam_search_early_stopping(\\n                seq_group.sampling_params.early_stopping,\\n                seq_group.sampling_params, best_running_seq, current_worst_seq)\\n\\n        if stop_beam_search:\\n            # Stop the beam search and remove all the running sequences from\\n            # the sequence group.\\n            unselected_child_seqs.extend(running_child_seqs)\\n        else:\\n            # Continue the beam search and select the top beam_width sequences\\n            # to continue the beam search.\\n            selected_child_seqs.extend(running_child_seqs[:beam_width])\\n            # The remaining running sequences will not be used in the next\\n            # iteration. Again, if these sequences are continuations of\\n            # parent sequences, we will need to remove the parent sequences\\n            # from the sequence group.\\n            unselected_child_seqs.extend(running_child_seqs[beam_width:])\\n\\n        # For newly created child sequences, add them to the sequence group\\n        # and fork them in block manager if they are not finished.\\n        for seq, parent in selected_child_seqs:\\n            if seq is not parent:\\n                seq_group.add(seq)\\n                if not seq.is_finished():\\n                    self.scheduler.fork_seq(parent, seq)\\n\\n        # Free the finished and selected parent sequences\\' memory in block\\n        # manager. Keep them in the sequence group as candidate output.\\n        for seq, parent in selected_child_seqs:\\n            if seq is parent and seq.is_finished():\\n                self.scheduler.free_seq(seq)\\n\\n        # Remove the unselected parent sequences from the sequence group and\\n        # free their memory in block manager.\\n        for seq, parent in unselected_child_seqs:\\n            if seq is parent:\\n                # Remove the parent sequence if it is not selected for next\\n                # iteration\\n                seq_group.remove(seq.seq_id)\\n                self.scheduler.free_seq(seq)\\n\\n    def _process_model_outputs(\\n            self, output: SamplerOutput,\\n            scheduler_outputs: SchedulerOutputs) -> List[RequestOutput]:\\n        # Update the scheduled sequence groups with the model outputs.\\n        scheduled_seq_groups = scheduler_outputs.scheduled_seq_groups\\n        for seq_group, samples in zip(scheduled_seq_groups, output):\\n            self._process_sequence_group_samples(seq_group, samples)\\n\\n        # Free the finished sequence groups.\\n        self.scheduler.free_finished_seq_groups()\\n\\n        # Create the outputs.\\n        request_outputs: List[RequestOutput] = []\\n        for seq_group in (scheduled_seq_groups +\\n                          scheduler_outputs.ignored_seq_groups):\\n            request_output = RequestOutput.from_seq_group(seq_group)\\n            request_outputs.append(request_output)\\n\\n        if self.log_stats:\\n            # Log the system stats.\\n            self._log_system_stats(scheduler_outputs.prompt_run,\\n                                   scheduler_outputs.num_batched_tokens)\\n        return request_outputs\\n\\n    def step(self) -> List[RequestOutput]:\\n        \"\"\"Performs one decoding iteration and returns newly generated results.\\n\\n        This function performs one decoding iteration of the engine. It first\\n        schedules the sequences to be executed in the next iteration and the\\n        token blocks to be swapped in/out/copy. Then, it executes the model\\n        and updates the scheduler with the model outputs. Finally, it decodes\\n        the sequences and returns the newly generated results.\\n        \"\"\"\\n        seq_group_metadata_list, scheduler_outputs, ignored = self._schedule()\\n        if scheduler_outputs.is_empty():\\n            return ignored\\n\\n        # Execute the model.\\n        output = self._run_workers(\\n            \"execute_model\",\\n            seq_group_metadata_list=seq_group_metadata_list,\\n            blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\\n            blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\\n            blocks_to_copy=scheduler_outputs.blocks_to_copy,\\n        )\\n\\n        return self._process_model_outputs(output, scheduler_outputs) + ignored\\n\\n    def _log_system_stats(\\n        self,\\n        prompt_run: bool,\\n        num_batched_tokens: int,\\n    ) -> None:\\n        now = time.time()\\n        # Log the number of batched input tokens.\\n        if prompt_run:\\n            self.num_prompt_tokens.append((now, num_batched_tokens))\\n        else:\\n            self.num_generation_tokens.append((now, num_batched_tokens))\\n\\n        elapsed_time = now - self.last_logging_time\\n        if elapsed_time < _LOGGING_INTERVAL_SEC:\\n            return\\n\\n        # Discard the old stats.\\n        self.num_prompt_tokens = [(t, n) for t, n in self.num_prompt_tokens\\n                                  if now - t < _LOGGING_INTERVAL_SEC]\\n        self.num_generation_tokens = [(t, n)\\n                                      for t, n in self.num_generation_tokens\\n                                      if now - t < _LOGGING_INTERVAL_SEC]\\n\\n        if len(self.num_prompt_tokens) > 1:\\n            total_num_tokens = sum(n for _, n in self.num_prompt_tokens[:-1])\\n            window = now - self.num_prompt_tokens[0][0]\\n            avg_prompt_throughput = total_num_tokens / window\\n        else:\\n            avg_prompt_throughput = 0.0\\n        if len(self.num_generation_tokens) > 1:\\n            total_num_tokens = sum(n\\n                                   for _, n in self.num_generation_tokens[:-1])\\n            window = now - self.num_generation_tokens[0][0]\\n            avg_generation_throughput = total_num_tokens / window\\n        else:\\n            avg_generation_throughput = 0.0\\n\\n        total_num_gpu_blocks = self.cache_config.num_gpu_blocks\\n        num_free_gpu_blocks = (\\n            self.scheduler.block_manager.get_num_free_gpu_blocks())\\n        num_used_gpu_blocks = total_num_gpu_blocks - num_free_gpu_blocks\\n        gpu_cache_usage = num_used_gpu_blocks / total_num_gpu_blocks\\n\\n        total_num_cpu_blocks = self.cache_config.num_cpu_blocks\\n        if total_num_cpu_blocks > 0:\\n            num_free_cpu_blocks = (\\n                self.scheduler.block_manager.get_num_free_cpu_blocks())\\n            num_used_cpu_blocks = total_num_cpu_blocks - num_free_cpu_blocks\\n            cpu_cache_usage = num_used_cpu_blocks / total_num_cpu_blocks\\n        else:\\n            cpu_cache_usage = 0.0\\n\\n        logger.info(\"Avg prompt throughput: \"\\n                    f\"{avg_prompt_throughput:.1f} tokens/s, \"\\n                    \"Avg generation throughput: \"\\n                    f\"{avg_generation_throughput:.1f} tokens/s, \"\\n                    f\"Running: {len(self.scheduler.running)} reqs, \"\\n                    f\"Swapped: {len(self.scheduler.swapped)} reqs, \"\\n                    f\"Pending: {len(self.scheduler.waiting)} reqs, \"\\n                    f\"GPU KV cache usage: {gpu_cache_usage * 100:.1f}%, \"\\n                    f\"CPU KV cache usage: {cpu_cache_usage * 100:.1f}%\")\\n        self.last_logging_time = now\\n\\n    def _decode_sequence(self, seq: Sequence) -> None:\\n        \"\"\"Decodes the new token for a sequence.\"\"\"\\n        (new_tokens, new_output_text, prefix_offset,\\n         read_offset) = detokenize_incrementally(\\n             self.tokenizer,\\n             all_input_ids=seq.get_token_ids(),\\n             prev_tokens=seq.tokens,\\n             prefix_offset=seq.prefix_offset,\\n             read_offset=seq.read_offset,\\n             skip_special_tokens=True,\\n         )\\n        if seq.tokens is None:\\n            seq.tokens = new_tokens\\n        else:\\n            seq.tokens.extend(new_tokens)\\n        seq.prefix_offset = prefix_offset\\n        seq.read_offset = read_offset\\n        seq.output_text += new_output_text\\n\\n    def _check_stop(self, seq: Sequence,\\n                    sampling_params: SamplingParams) -> None:\\n        \"\"\"Stop the finished sequences.\"\"\"\\n        for stop_str in sampling_params.stop:\\n            if seq.output_text.endswith(stop_str):\\n                # Truncate the output text so that the stop string is\\n                # not included in the output.\\n                seq.output_text = seq.output_text[:-len(stop_str)]\\n                seq.status = SequenceStatus.FINISHED_STOPPED\\n                return\\n\\n        # Check if the sequence has reached max_model_len.\\n        if seq.get_len() > self.scheduler_config.max_model_len:\\n            seq.status = SequenceStatus.FINISHED_LENGTH_CAPPED\\n            return\\n\\n        # Check if the sequence has reached max_tokens.\\n        if seq.get_output_len() == sampling_params.max_tokens:\\n            seq.status = SequenceStatus.FINISHED_LENGTH_CAPPED\\n            return\\n\\n        # Check if the sequence has generated the EOS token.\\n        if ((not sampling_params.ignore_eos)\\n                and seq.get_last_token_id() == self.tokenizer.eos_token_id):\\n            seq.status = SequenceStatus.FINISHED_STOPPED\\n            return\\n\\n    def _run_workers(\\n        self,\\n        method: str,\\n        *args,\\n        get_all_outputs: bool = False,\\n        **kwargs,\\n    ) -> Any:\\n        \"\"\"Runs the given method on all workers.\"\"\"\\n        all_outputs = []\\n        for worker in self.workers:\\n            if self.parallel_config.worker_use_ray:\\n                executor = partial(worker.execute_method.remote, method)\\n            else:\\n                executor = getattr(worker, method)\\n\\n            output = executor(*args, **kwargs)\\n            all_outputs.append(output)\\n\\n        if self.parallel_config.worker_use_ray:\\n            all_outputs = ray.get(all_outputs)\\n\\n        if get_all_outputs:\\n            return all_outputs\\n\\n        # Make sure all workers have the same results.\\n        output = all_outputs[0]\\n        for other_output in all_outputs[1:]:\\n            assert output == other_output\\n        return output\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/engine/__init__.py\\n---------\\nContent:\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/engine/ray_utils.py\\n---------\\nContent:\\nimport socket\\nfrom typing import Optional, Tuple, TYPE_CHECKING\\n\\nfrom vllm.config import ParallelConfig\\n\\ntry:\\n    import ray\\n    from ray.air.util.torch_dist import TorchDistributedWorker\\n\\n    class RayWorker(TorchDistributedWorker):\\n        \"\"\"Ray wrapper for vllm.worker.Worker, allowing Worker to be\\n        lazliy initialized after Ray sets CUDA_VISIBLE_DEVICES.\"\"\"\\n\\n        def __init__(self, init_cached_hf_modules=False) -> None:\\n            if init_cached_hf_modules:\\n                # pylint: disable=import-outside-toplevel\\n                from transformers.dynamic_module_utils import init_hf_modules\\n                init_hf_modules()\\n            self.worker = None\\n\\n        def init_worker(self, worker_init_fn):\\n            self.worker = worker_init_fn()\\n\\n        def __getattr__(self, name):\\n            return getattr(self.worker, name)\\n\\n        def execute_method(self, method, *args, **kwargs):\\n            executor = getattr(self, method)\\n            return executor(*args, **kwargs)\\n\\nexcept ImportError:\\n    ray = None\\n    TorchDistributedWorker = None\\n    RayWorker = None  # pylint: disable=invalid-name\\n\\nif TYPE_CHECKING:\\n    from ray.util.placement_group import PlacementGroup\\n\\n\\ndef get_open_port():\\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\\n        s.bind((\"\", 0))\\n        return s.getsockname()[1]\\n\\n\\ndef initialize_cluster(\\n    parallel_config: ParallelConfig,\\n    engine_use_ray: bool = False,\\n    ray_address: Optional[str] = None,\\n) -> Tuple[str, Optional[\"PlacementGroup\"]]:\\n    \"\"\"Initialize the distributed cluster probably with Ray.\\n\\n    Args:\\n        parallel_config: The configurations for parallel execution.\\n        engine_use_ray: Whether to use Ray for async engine.\\n        ray_address: The address of the Ray cluster. If None, uses\\n            the default Ray cluster address.\\n\\n    Returns:\\n        A tuple of (`distributed_init_method`, `all_stage_devices`). The\\n        `distributed_init_method` is the address for initializing the\\n        distributed backend. `all_stage_devices` includes device IDs for\\n        each worker in each pipeline stage. Each device ID is a tuple of\\n        (rank, node resource, device id).\\n    \"\"\"\\n    if parallel_config.worker_use_ray or engine_use_ray:\\n        if ray is None:\\n            raise ImportError(\\n                \"Ray is not installed. Please install Ray to use distributed \"\\n                \"serving.\")\\n        # Connect to a ray cluster.\\n        ray.init(address=ray_address, ignore_reinit_error=True)\\n\\n    if not parallel_config.worker_use_ray:\\n        # Initialize cluster locally.\\n        port = get_open_port()\\n        # We need to setup the distributed init method to make sure\\n        # the distributed megatron code (e.g., get world size) works correctly.\\n        distributed_init_method = f\"tcp://localhost:{port}\"\\n        return distributed_init_method, None\\n\\n    current_placement_group = ray.util.get_current_placement_group()\\n    if current_placement_group:\\n        # We are in a placement group\\n        bundles = current_placement_group.bundle_specs\\n        # Verify that we can use the placement group.\\n        gpu_bundles = 0\\n        for bundle in bundles:\\n            bundle_gpus = bundle.get(\"GPU\", 0)\\n            if bundle_gpus > 1:\\n                raise ValueError(\\n                    \"Placement group bundle cannot have more than 1 GPU.\")\\n            if bundle_gpus:\\n                gpu_bundles += 1\\n        if parallel_config.world_size > gpu_bundles:\\n            raise ValueError(\\n                \"The number of required GPUs exceeds the total number of \"\\n                \"available GPUs in the placement group.\")\\n    else:\\n        num_gpus_in_cluster = ray.cluster_resources().get(\"GPU\", 0)\\n        if parallel_config.world_size > num_gpus_in_cluster:\\n            raise ValueError(\\n                \"The number of required GPUs exceeds the total number of \"\\n                \"available GPUs in the cluster.\")\\n        # Create a new placement group\\n        current_placement_group = ray.util.placement_group([{\\n            \"GPU\": 1\\n        }] * parallel_config.world_size)\\n        # Wait until PG is ready - this will block until all\\n        # requested resources are available, and will timeout\\n        # if they cannot be provisioned.\\n        ray.get(current_placement_group.ready(), timeout=1800)\\n\\n    return None, current_placement_group\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/engine/arg_utils.py\\n---------\\nContent:\\nimport argparse\\nimport dataclasses\\nfrom dataclasses import dataclass\\nfrom typing import Optional, Tuple\\n\\nfrom vllm.config import (CacheConfig, ModelConfig, ParallelConfig,\\n                         SchedulerConfig)\\n\\n\\n@dataclass\\nclass EngineArgs:\\n    \"\"\"Arguments for vLLM engine.\"\"\"\\n    model: str\\n    tokenizer: Optional[str] = None\\n    tokenizer_mode: str = \\'auto\\'\\n    trust_remote_code: bool = False\\n    download_dir: Optional[str] = None\\n    load_format: str = \\'auto\\'\\n    dtype: str = \\'auto\\'\\n    seed: int = 0\\n    max_model_len: Optional[int] = None\\n    worker_use_ray: bool = False\\n    pipeline_parallel_size: int = 1\\n    tensor_parallel_size: int = 1\\n    block_size: int = 16\\n    swap_space: int = 4  # GiB\\n    gpu_memory_utilization: float = 0.90\\n    max_num_batched_tokens: int = 2560\\n    max_num_seqs: int = 256\\n    disable_log_stats: bool = False\\n    revision: Optional[str] = None\\n    quantization: Optional[str] = None\\n\\n    def __post_init__(self):\\n        if self.tokenizer is None:\\n            self.tokenizer = self.model\\n        self.max_num_seqs = min(self.max_num_seqs, self.max_num_batched_tokens)\\n\\n    @staticmethod\\n    def add_cli_args(\\n            parser: argparse.ArgumentParser) -> argparse.ArgumentParser:\\n        \"\"\"Shared CLI arguments for vLLM engine.\"\"\"\\n        # Model arguments\\n        parser.add_argument(\\n            \\'--model\\',\\n            type=str,\\n            default=\\'facebook/opt-125m\\',\\n            help=\\'name or path of the huggingface model to use\\')\\n        parser.add_argument(\\n            \\'--tokenizer\\',\\n            type=str,\\n            default=EngineArgs.tokenizer,\\n            help=\\'name or path of the huggingface tokenizer to use\\')\\n        parser.add_argument(\\n            \\'--revision\\',\\n            type=str,\\n            default=None,\\n            help=\\'the specific model version to use. It can be a branch \\'\\n            \\'name, a tag name, or a commit id. If unspecified, will use \\'\\n            \\'the default version.\\')\\n        parser.add_argument(\\'--tokenizer-mode\\',\\n                            type=str,\\n                            default=EngineArgs.tokenizer_mode,\\n                            choices=[\\'auto\\', \\'slow\\'],\\n                            help=\\'tokenizer mode. \"auto\" will use the fast \\'\\n                            \\'tokenizer if available, and \"slow\" will \\'\\n                            \\'always use the slow tokenizer.\\')\\n        parser.add_argument(\\'--trust-remote-code\\',\\n                            action=\\'store_true\\',\\n                            help=\\'trust remote code from huggingface\\')\\n        parser.add_argument(\\'--download-dir\\',\\n                            type=str,\\n                            default=EngineArgs.download_dir,\\n                            help=\\'directory to download and load the weights, \\'\\n                            \\'default to the default cache dir of \\'\\n                            \\'huggingface\\')\\n        parser.add_argument(\\n            \\'--load-format\\',\\n            type=str,\\n            default=EngineArgs.load_format,\\n            choices=[\\'auto\\', \\'pt\\', \\'safetensors\\', \\'npcache\\', \\'dummy\\'],\\n            help=\\'The format of the model weights to load. \\'\\n            \\'\"auto\" will try to load the weights in the safetensors format \\'\\n            \\'and fall back to the pytorch bin format if safetensors format \\'\\n            \\'is not available. \\'\\n            \\'\"pt\" will load the weights in the pytorch bin format. \\'\\n            \\'\"safetensors\" will load the weights in the safetensors format. \\'\\n            \\'\"npcache\" will load the weights in pytorch format and store \\'\\n            \\'a numpy cache to speed up the loading. \\'\\n            \\'\"dummy\" will initialize the weights with random values, \\'\\n            \\'which is mainly for profiling.\\')\\n        parser.add_argument(\\n            \\'--dtype\\',\\n            type=str,\\n            default=EngineArgs.dtype,\\n            choices=[\\'auto\\', \\'half\\', \\'bfloat16\\', \\'float\\'],\\n            help=\\'data type for model weights and activations. \\'\\n            \\'The \"auto\" option will use FP16 precision \\'\\n            \\'for FP32 and FP16 models, and BF16 precision \\'\\n            \\'for BF16 models.\\')\\n        parser.add_argument(\\'--max-model-len\\',\\n                            type=int,\\n                            default=None,\\n                            help=\\'model context length. If unspecified, \\'\\n                            \\'will be automatically derived from the model.\\')\\n        # Parallel arguments\\n        parser.add_argument(\\'--worker-use-ray\\',\\n                            action=\\'store_true\\',\\n                            help=\\'use Ray for distributed serving, will be \\'\\n                            \\'automatically set when using more than 1 GPU\\')\\n        parser.add_argument(\\'--pipeline-parallel-size\\',\\n                            \\'-pp\\',\\n                            type=int,\\n                            default=EngineArgs.pipeline_parallel_size,\\n                            help=\\'number of pipeline stages\\')\\n        parser.add_argument(\\'--tensor-parallel-size\\',\\n                            \\'-tp\\',\\n                            type=int,\\n                            default=EngineArgs.tensor_parallel_size,\\n                            help=\\'number of tensor parallel replicas\\')\\n        # KV cache arguments\\n        parser.add_argument(\\'--block-size\\',\\n                            type=int,\\n                            default=EngineArgs.block_size,\\n                            choices=[8, 16, 32],\\n                            help=\\'token block size\\')\\n        # TODO(woosuk): Support fine-grained seeds (e.g., seed per request).\\n        parser.add_argument(\\'--seed\\',\\n                            type=int,\\n                            default=EngineArgs.seed,\\n                            help=\\'random seed\\')\\n        parser.add_argument(\\'--swap-space\\',\\n                            type=int,\\n                            default=EngineArgs.swap_space,\\n                            help=\\'CPU swap space size (GiB) per GPU\\')\\n        parser.add_argument(\\'--gpu-memory-utilization\\',\\n                            type=float,\\n                            default=EngineArgs.gpu_memory_utilization,\\n                            help=\\'the percentage of GPU memory to be used for\\'\\n                            \\'the model executor\\')\\n        parser.add_argument(\\'--max-num-batched-tokens\\',\\n                            type=int,\\n                            default=EngineArgs.max_num_batched_tokens,\\n                            help=\\'maximum number of batched tokens per \\'\\n                            \\'iteration\\')\\n        parser.add_argument(\\'--max-num-seqs\\',\\n                            type=int,\\n                            default=EngineArgs.max_num_seqs,\\n                            help=\\'maximum number of sequences per iteration\\')\\n        parser.add_argument(\\'--disable-log-stats\\',\\n                            action=\\'store_true\\',\\n                            help=\\'disable logging statistics\\')\\n        # Quantization settings.\\n        parser.add_argument(\\'--quantization\\',\\n                            \\'-q\\',\\n                            type=str,\\n                            choices=[\\'awq\\', None],\\n                            default=None,\\n                            help=\\'Method used to quantize the weights\\')\\n        return parser\\n\\n    @classmethod\\n    def from_cli_args(cls, args: argparse.Namespace) -> \\'EngineArgs\\':\\n        # Get the list of attributes of this dataclass.\\n        attrs = [attr.name for attr in dataclasses.fields(cls)]\\n        # Set the attributes from the parsed arguments.\\n        engine_args = cls(**{attr: getattr(args, attr) for attr in attrs})\\n        return engine_args\\n\\n    def create_engine_configs(\\n        self,\\n    ) -> Tuple[ModelConfig, CacheConfig, ParallelConfig, SchedulerConfig]:\\n        model_config = ModelConfig(self.model, self.tokenizer,\\n                                   self.tokenizer_mode, self.trust_remote_code,\\n                                   self.download_dir, self.load_format,\\n                                   self.dtype, self.seed, self.revision,\\n                                   self.max_model_len, self.quantization)\\n        cache_config = CacheConfig(self.block_size,\\n                                   self.gpu_memory_utilization,\\n                                   self.swap_space)\\n        parallel_config = ParallelConfig(self.pipeline_parallel_size,\\n                                         self.tensor_parallel_size,\\n                                         self.worker_use_ray)\\n        scheduler_config = SchedulerConfig(self.max_num_batched_tokens,\\n                                           self.max_num_seqs,\\n                                           model_config.get_max_model_len())\\n        return model_config, cache_config, parallel_config, scheduler_config\\n\\n\\n@dataclass\\nclass AsyncEngineArgs(EngineArgs):\\n    \"\"\"Arguments for asynchronous vLLM engine.\"\"\"\\n    engine_use_ray: bool = False\\n    disable_log_requests: bool = False\\n    max_log_len: Optional[int] = None\\n\\n    @staticmethod\\n    def add_cli_args(\\n            parser: argparse.ArgumentParser) -> argparse.ArgumentParser:\\n        parser = EngineArgs.add_cli_args(parser)\\n        parser.add_argument(\\'--engine-use-ray\\',\\n                            action=\\'store_true\\',\\n                            help=\\'use Ray to start the LLM engine in a \\'\\n                            \\'separate process as the server process.\\')\\n        parser.add_argument(\\'--disable-log-requests\\',\\n                            action=\\'store_true\\',\\n                            help=\\'disable logging requests\\')\\n        parser.add_argument(\\'--max-log-len\\',\\n                            type=int,\\n                            default=None,\\n                            help=\\'max number of prompt characters or prompt \\'\\n                            \\'ID numbers being printed in log. \\'\\n                            \\'Default: unlimited.\\')\\n        return parser\\n<<End File>>\\n<<Begin file>>\\nPath:\\nvllm/engine/async_llm_engine.py\\n---------\\nContent:\\nimport asyncio\\nimport time\\nfrom functools import partial\\nfrom typing import (Any, Dict, Iterable, List, Optional, Set, Tuple, Type,\\n                    Union)\\n\\nfrom vllm.config import ModelConfig\\nfrom vllm.engine.arg_utils import AsyncEngineArgs\\nfrom vllm.engine.llm_engine import LLMEngine\\nfrom vllm.engine.ray_utils import initialize_cluster, ray\\nfrom vllm.logger import init_logger\\nfrom vllm.outputs import RequestOutput\\nfrom vllm.sampling_params import SamplingParams\\n\\nlogger = init_logger(__name__)\\n\\n\\nclass AsyncEngineDeadError(RuntimeError):\\n    pass\\n\\n\\ndef _raise_exception_on_finish(task: asyncio.Task,\\n                               request_tracker: \"RequestTracker\") -> None:\\n    msg = (\"Task finished unexpectedly. This should never happen! \"\\n           \"Please open an issue on Github.\")\\n    try:\\n        try:\\n            task.result()\\n        except asyncio.CancelledError:\\n            return\\n        except Exception as exc:\\n            raise AsyncEngineDeadError(\\n                msg + \" See stack trace above for the actual cause.\") from exc\\n        raise AsyncEngineDeadError(msg)\\n    except Exception as exc:\\n        request_tracker.propagate_exception(exc)\\n        raise exc\\n\\n\\nclass AsyncStream:\\n    \"\"\"A stream of RequestOutputs for a request that can be\\n    iterated over asynchronously.\"\"\"\\n\\n    def __init__(self, request_id: str) -> None:\\n        self.request_id = request_id\\n        self._queue = asyncio.Queue()\\n        self._finished = False\\n\\n    def put(self, item: RequestOutput) -> None:\\n        if self._finished:\\n            return\\n        self._queue.put_nowait(item)\\n\\n    def finish(self) -> None:\\n        self._queue.put_nowait(StopIteration)\\n        self._finished = True\\n\\n    @property\\n    def finished(self) -> bool:\\n        return self._finished\\n\\n    def __aiter__(self):\\n        return self\\n\\n    async def __anext__(self) -> RequestOutput:\\n        result = await self._queue.get()\\n        if result is StopIteration:\\n            raise StopAsyncIteration\\n        elif isinstance(result, Exception):\\n            raise result\\n        return result\\n\\n\\nclass RequestTracker:\\n    \"\"\"Synchronous abstraction for tracking requests.\"\"\"\\n\\n    def __init__(self) -> None:\\n        self._request_streams: Dict[str, AsyncStream] = {}\\n        self._finished_requests: asyncio.Queue[str] = asyncio.Queue()\\n        self._new_requests: asyncio.Queue[Tuple[AsyncStream,\\n                                                dict]] = asyncio.Queue()\\n        self.new_requests_event = None\\n\\n    def __contains__(self, item):\\n        return item in self._request_streams\\n\\n    def init_event(self):\\n        self.new_requests_event = asyncio.Event()\\n\\n    def propagate_exception(self,\\n                            exc: Exception,\\n                            request_id: Optional[str] = None) -> None:\\n        \"\"\"Propagate an exception to request streams\\n        (all if request_id is None).\"\"\"\\n        if request_id is not None:\\n            self._request_streams[request_id].put(exc)\\n        else:\\n            for stream in self._request_streams.values():\\n                stream.put(exc)\\n\\n    def process_request_output(self,\\n                               request_output: RequestOutput,\\n                               *,\\n                               verbose: bool = False) -> None:\\n        \"\"\"Process a request output from the engine.\"\"\"\\n        request_id = request_output.request_id\\n\\n        self._request_streams[request_id].put(request_output)\\n        if request_output.finished:\\n            if verbose:\\n                logger.info(f\"Finished request {request_id}.\")\\n            self.abort_request(request_id)\\n\\n    def add_request(self, request_id: str,\\n                    **engine_add_request_kwargs) -> AsyncStream:\\n        \"\"\"Add a request to be sent to the engine on the next background\\n        loop iteration.\"\"\"\\n        if request_id in self._request_streams:\\n            raise KeyError(f\"Request {request_id} already exists.\")\\n\\n        stream = AsyncStream(request_id)\\n        self._new_requests.put_nowait((stream, {\\n            \"request_id\": request_id,\\n            **engine_add_request_kwargs\\n        }))\\n\\n        self.new_requests_event.set()\\n\\n        return stream\\n\\n    def abort_request(self, request_id: str, *, verbose: bool = False) -> None:\\n        \"\"\"Abort a request during next background loop iteration.\"\"\"\\n        if verbose:\\n            logger.info(f\"Aborted request {request_id}.\")\\n\\n        self._finished_requests.put_nowait(request_id)\\n\\n        if request_id not in self._request_streams or self._request_streams[\\n                request_id].finished:\\n            # The request has already finished or been aborted.\\n            return\\n\\n        self._request_streams[request_id].finish()\\n\\n    def get_new_and_finished_requests(self) -> Tuple[List[dict], Set[str]]:\\n        \"\"\"Get the new requests and finished requests to be\\n        sent to the engine.\"\"\"\\n        new_requests: List[dict] = []\\n        finished_requests: Set[str] = set()\\n\\n        while not self._finished_requests.empty():\\n            request_id = self._finished_requests.get_nowait()\\n            finished_requests.add(request_id)\\n            self._request_streams.pop(request_id, None)\\n\\n        while not self._new_requests.empty():\\n            stream, new_request = self._new_requests.get_nowait()\\n            if stream.request_id in finished_requests:\\n                # The request has already been aborted.\\n                stream.finish()\\n                continue\\n            self._request_streams[stream.request_id] = stream\\n            new_requests.append(new_request)\\n\\n        self.new_requests_event.clear()\\n\\n        return new_requests, finished_requests\\n\\n    async def wait_for_new_requests(self):\\n        await self.new_requests_event.wait()\\n\\n\\nclass _AsyncLLMEngine(LLMEngine):\\n    \"\"\"Extension of LLMEngine to add async methods.\"\"\"\\n\\n    async def step_async(self) -> List[RequestOutput]:\\n        \"\"\"Performs one decoding iteration and returns newly generated results.\\n        The workers are ran asynchronously if possible.\\n\\n        This function performs one decoding iteration of the engine. It first\\n        schedules the sequences to be executed in the next iteration and the\\n        token blocks to be swapped in/out/copy. Then, it executes the model\\n        and updates the scheduler with the model outputs. Finally, it decodes\\n        the sequences and returns the newly generated results.\\n        \"\"\"\\n        (seq_group_metadata_list, scheduler_outputs,\\n         early_return) = self._schedule()\\n        if early_return is not None:\\n            return early_return\\n\\n        # Execute the model.\\n        output = await self._run_workers_async(\\n            \"execute_model\",\\n            seq_group_metadata_list=seq_group_metadata_list,\\n            blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\\n            blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\\n            blocks_to_copy=scheduler_outputs.blocks_to_copy,\\n        )\\n\\n        return self._process_model_outputs(output, scheduler_outputs)\\n\\n    async def _run_workers_async(\\n        self,\\n        method: str,\\n        *args,\\n        get_all_outputs: bool = False,\\n        **kwargs,\\n    ) -> Any:\\n        \"\"\"Runs the given method on all workers.\"\"\"\\n        all_outputs = []\\n        for worker in self.workers:\\n            if self.parallel_config.worker_use_ray:\\n                executor = partial(worker.execute_method.remote, method)\\n            else:\\n                executor = getattr(worker, method)\\n\\n            output = executor(*args, **kwargs)\\n            all_outputs.append(output)\\n\\n        if self.parallel_config.worker_use_ray:\\n            all_outputs = await asyncio.gather(*all_outputs)\\n\\n        if get_all_outputs:\\n            return all_outputs\\n\\n        # Make sure all workers have the same results.\\n        output = all_outputs[0]\\n        for other_output in all_outputs[1:]:\\n            assert output == other_output\\n        return output\\n\\n\\nclass AsyncLLMEngine:\\n    \"\"\"An asynchronous wrapper for LLMEngine.\\n\\n    This class is used to wrap the LLMEngine class to make it asynchronous. It\\n    uses asyncio to create a background loop that keeps processing incoming\\n    requests. The LLMEngine is kicked by the generate method when there\\n    are requests in the waiting queue. The generate method yields the outputs\\n    from the LLMEngine to the caller.\\n\\n    NOTE: For the comprehensive list of arguments, see `LLMEngine`.\\n\\n    Args:\\n        worker_use_ray: Whether to use Ray for model workers. Required for\\n            distributed execution. Should be the same as\\n            `parallel_config.worker_use_ray`.\\n        engine_use_ray: Whether to make LLMEngine a Ray actor. If so, the\\n            async frontend will be executed in a separate process as the\\n            model workers.\\n        log_requests: Whether to log the requests.\\n        start_engine_loop: If True, the background task to run the engine\\n            will be automatically started in the generate call.\\n        *args, *kwargs: Arguments for LLMEngine.\\n    \"\"\"\\n\\n    _engine_class: Type[_AsyncLLMEngine] = _AsyncLLMEngine\\n\\n    def __init__(self,\\n                 worker_use_ray: bool,\\n                 engine_use_ray: bool,\\n                 *args,\\n                 log_requests: bool = True,\\n                 max_log_len: Optional[int] = None,\\n                 start_engine_loop: bool = True,\\n                 **kwargs) -> None:\\n        self.worker_use_ray = worker_use_ray\\n        self.engine_use_ray = engine_use_ray\\n        self.log_requests = log_requests\\n        self.max_log_len = max_log_len\\n        self.engine = self._init_engine(*args, **kwargs)\\n\\n        self.background_loop = None\\n        # We need to keep a reference to unshielded\\n        # task as well to prevent it from being garbage\\n        # collected\\n        self._background_loop_unshielded = None\\n        self.start_engine_loop = start_engine_loop\\n        self._request_tracker = RequestTracker()\\n\\n    @property\\n    def is_running(self) -> bool:\\n        return (self.background_loop is not None\\n                and not self.background_loop.done())\\n\\n    def start_background_loop(self) -> None:\\n        \"\"\"Start the background loop.\"\"\"\\n        if self.is_running:\\n            raise RuntimeError(\"Background loop is already running.\")\\n        self._request_tracker.init_event()\\n\\n        self._background_loop_unshielded = asyncio.get_event_loop(\\n        ).create_task(self.run_engine_loop())\\n        self._background_loop_unshielded.add_done_callback(\\n            partial(_raise_exception_on_finish,\\n                    request_tracker=self._request_tracker))\\n        self.background_loop = asyncio.shield(self._background_loop_unshielded)\\n\\n    def _init_engine(self, *args,\\n                     **kwargs) -> Union[_AsyncLLMEngine, \"ray.ObjectRef\"]:\\n        if not self.engine_use_ray:\\n            engine_class = self._engine_class\\n        elif self.worker_use_ray:\\n            engine_class = ray.remote(num_cpus=0)(self._engine_class).remote\\n        else:\\n            engine_class = ray.remote(num_gpus=1)(self._engine_class).remote\\n        return engine_class(*args, **kwargs)\\n\\n    async def engine_step(self) -> bool:\\n        \"\"\"Kick the engine to process the waiting requests.\\n\\n        Returns True if there are in-progress requests.\"\"\"\\n\\n        new_requests, finished_requests = (\\n            self._request_tracker.get_new_and_finished_requests())\\n\\n        for new_request in new_requests:\\n            # Add the request into the vLLM engine\\'s waiting queue.\\n            # TODO: Maybe add add_request_batch to reduce Ray overhead\\n            if self.engine_use_ray:\\n                await self.engine.add_request.remote(**new_request)\\n            else:\\n                self.engine.add_request(**new_request)\\n\\n        if finished_requests:\\n            await self._engine_abort(finished_requests)\\n\\n        if self.engine_use_ray:\\n            request_outputs = await self.engine.step.remote()\\n        else:\\n            request_outputs = await self.engine.step_async()\\n\\n        # Put the outputs into the corresponding streams.\\n        for request_output in request_outputs:\\n            self._request_tracker.process_request_output(\\n                request_output, verbose=self.log_requests)\\n\\n        return len(request_outputs) > 0\\n\\n    async def _engine_abort(self, request_ids: Iterable[str]):\\n        if self.engine_use_ray:\\n            await self.engine.abort_request.remote(request_ids)\\n        else:\\n            self.engine.abort_request(request_ids)\\n\\n    async def run_engine_loop(self):\\n        # Initialize the RequestTracker here so it uses the right event loop.\\n        has_requests_in_progress = False\\n        while True:\\n            if not has_requests_in_progress:\\n                await self._request_tracker.wait_for_new_requests()\\n            has_requests_in_progress = await self.engine_step()\\n            await asyncio.sleep(0)\\n\\n    async def add_request(\\n        self,\\n        request_id: str,\\n        prompt: Optional[str],\\n        sampling_params: SamplingParams,\\n        prompt_token_ids: Optional[List[int]] = None,\\n        arrival_time: Optional[float] = None,\\n    ) -> AsyncStream:\\n        if self.log_requests:\\n            shortened_prompt = prompt\\n            shortened_token_ids = prompt_token_ids\\n            if self.max_log_len is not None:\\n                if shortened_prompt is not None:\\n                    shortened_prompt = shortened_prompt[:self.max_log_len]\\n                if shortened_token_ids is not None:\\n                    shortened_token_ids = shortened_token_ids[:self.\\n                                                              max_log_len]\\n            logger.info(f\"Received request {request_id}: \"\\n                        f\"prompt: {shortened_prompt!r}, \"\\n                        f\"sampling params: {sampling_params}, \"\\n                        f\"prompt token ids: {shortened_token_ids}.\")\\n\\n        if not self.is_running:\\n            if self.start_engine_loop:\\n                self.start_background_loop()\\n            else:\\n                raise AsyncEngineDeadError(\\n                    \"Background loop is not running. If it was running, \"\\n                    \"inspect the output to find the stacktrace of the \"\\n                    \"error that caused the background loop to stop \"\\n                    \"(AsyncEngineDeadError).\")\\n\\n        stream = self._request_tracker.add_request(\\n            request_id,\\n            prompt=prompt,\\n            sampling_params=sampling_params,\\n            prompt_token_ids=prompt_token_ids,\\n            arrival_time=arrival_time)\\n\\n        return stream\\n\\n    async def generate(\\n            self,\\n            prompt: Optional[str],\\n            sampling_params: SamplingParams,\\n            request_id: str,\\n            prompt_token_ids: Optional[List[int]] = None) -> RequestOutput:\\n        \"\"\"Generate outputs for a request.\\n\\n        Generate outputs for a request. This method is a coroutine. It adds the\\n        request into the waiting queue of the LLMEngine and streams the outputs\\n        from the LLMEngine to the caller.\\n\\n        Args:\\n            prompt: The prompt string. Can be None if prompt_token_ids is\\n                provided.\\n            sampling_params: The sampling parameters of the request.\\n            request_id: The unique id of the request.\\n            prompt_token_ids: The token IDs of the prompt. If None, we\\n                use the tokenizer to convert the prompts to token IDs.\\n\\n        Yields:\\n            The output `RequestOutput` objects from the LLMEngine for the\\n            request.\\n        \"\"\"\\n        # Preprocess the request.\\n        arrival_time = time.time()\\n\\n        try:\\n            stream = await self.add_request(request_id,\\n                                            prompt,\\n                                            sampling_params,\\n                                            prompt_token_ids=prompt_token_ids,\\n                                            arrival_time=arrival_time)\\n\\n            async for request_output in stream:\\n                yield request_output\\n        except (Exception, asyncio.CancelledError) as e:\\n            # If there is an exception or coroutine is cancelled, abort the\\n            # request.\\n            self._abort(request_id)\\n            raise e\\n\\n    async def abort(self, request_id: str) -> None:\\n        \"\"\"Abort a request.\\n\\n        Abort a submitted request. If the request is finished or not found,\\n        this method will be a no-op.\\n\\n        Args:\\n            request_id: The unique id of the request.\\n        \"\"\"\\n        if not self.is_running:\\n            raise AsyncEngineDeadError(\\n                \"Background loop is not running. If it was running, \"\\n                \"inspect the output to find the stacktrace of the \"\\n                \"error that caused the background loop to stop \"\\n                \"(AsyncEngineDeadError).\")\\n\\n        return self._abort(request_id)\\n\\n    def _abort(self, request_id: str) -> None:\\n        \"\"\"Abort a request.\\n\\n        Abort a submitted request. If the request is finished or not found,\\n        this method will be a no-op.\\n\\n        Args:\\n            request_id: The unique id of the request.\\n        \"\"\"\\n        self._request_tracker.abort_request(request_id,\\n                                            verbose=self.log_requests)\\n\\n    async def get_model_config(self) -> ModelConfig:\\n        \"\"\"Get the model configuration of the vLLM engine.\"\"\"\\n        if self.engine_use_ray:\\n            return await self.engine.get_model_config.remote()\\n        else:\\n            return self.engine.get_model_config()\\n\\n    @classmethod\\n    def from_engine_args(cls,\\n                         engine_args: AsyncEngineArgs,\\n                         start_engine_loop: bool = True) -> \"AsyncLLMEngine\":\\n        \"\"\"Creates an async LLM engine from the engine arguments.\"\"\"\\n        # Create the engine configs.\\n        engine_configs = engine_args.create_engine_configs()\\n        parallel_config = engine_configs[2]\\n        # Initialize the cluster.\\n        distributed_init_method, placement_group = initialize_cluster(\\n            parallel_config, engine_args.engine_use_ray)\\n        # Create the async LLM engine.\\n        engine = cls(engine_args.worker_use_ray,\\n                     engine_args.engine_use_ray,\\n                     *engine_configs,\\n                     distributed_init_method,\\n                     placement_group,\\n                     log_requests=not engine_args.disable_log_requests,\\n                     log_stats=not engine_args.disable_log_stats,\\n                     max_log_len=engine_args.max_log_len,\\n                     start_engine_loop=start_engine_loop)\\n        return engine\\n<<End File>>\\n<<Begin file>>\\nPath:\\ndocs/source/conf.py\\n---------\\nContent:\\n# Configuration file for the Sphinx documentation builder.\\n#\\n# This file only contains a selection of the most common options. For a full\\n# list see the documentation:\\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\\n\\n# -- Path setup --------------------------------------------------------------\\n\\n# If extensions (or modules to document with autodoc) are in another directory,\\n# add these directories to sys.path here. If the directory is relative to the\\n# documentation root, use os.path.abspath to make it absolute, like shown here.\\n#\\n# import os\\n# import sys\\n# sys.path.insert(0, os.path.abspath(\\'.\\'))\\n\\n\\n# -- Project information -----------------------------------------------------\\n\\nproject = \\'vLLM\\'\\ncopyright = \\'2023, vLLM Team\\'\\nauthor = \\'the vLLM Team\\'\\n\\n\\n# -- General configuration ---------------------------------------------------\\n\\n# Add any Sphinx extension module names here, as strings. They can be\\n# extensions coming with Sphinx (named \\'sphinx.ext.*\\') or your custom\\n# ones.\\nextensions = [\\n    \"sphinx.ext.napoleon\",\\n    \"sphinx.ext.viewcode\",\\n    \"sphinx.ext.intersphinx\",\\n    \"sphinx_copybutton\",\\n]\\n\\n# Add any paths that contain templates here, relative to this directory.\\ntemplates_path = [\\'_templates\\']\\n\\n# List of patterns, relative to source directory, that match files and\\n# directories to ignore when looking for source files.\\n# This pattern also affects html_static_path and html_extra_path.\\nexclude_patterns = []\\n\\n# Exclude the prompt \"$\" when copying code\\ncopybutton_prompt_text = r\"\\\\$ \"\\ncopybutton_prompt_is_regexp = True\\n\\n# -- Options for HTML output -------------------------------------------------\\n\\n# The theme to use for HTML and HTML Help pages.  See the documentation for\\n# a list of builtin themes.\\n#\\nhtml_title = project\\nhtml_theme = \\'sphinx_book_theme\\'\\nhtml_logo = \\'assets/logos/vllm-logo-text-light.png\\'\\nhtml_theme_options = {\\n    \\'logo_only\\': True,\\n    \\'path_to_docs\\': \\'docs/source\\',\\n    \\'repository_url\\': \\'https://github.com/vllm-project/vllm\\',\\n    \\'use_repository_button\\': True,\\n}\\n\\n# Add any paths that contain custom static files (such as style sheets) here,\\n# relative to this directory. They are copied after the builtin static files,\\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\\nhtml_static_path = [\\'_static\\']\\n<<End File>>\\n<<Begin file>>\\nPath:\\nexamples/gradio_webserver.py\\n---------\\nContent:\\nimport argparse\\nimport json\\n\\nimport gradio as gr\\nimport requests\\n\\n\\ndef http_bot(prompt):\\n    headers = {\"User-Agent\": \"vLLM Client\"}\\n    pload = {\\n        \"prompt\": prompt,\\n        \"stream\": True,\\n        \"max_tokens\": 128,\\n    }\\n    response = requests.post(args.model_url,\\n                             headers=headers,\\n                             json=pload,\\n                             stream=True)\\n\\n    for chunk in response.iter_lines(chunk_size=8192,\\n                                     decode_unicode=False,\\n                                     delimiter=b\"\\\\0\"):\\n        if chunk:\\n            data = json.loads(chunk.decode(\"utf-8\"))\\n            output = data[\"text\"][0]\\n            yield output\\n\\n\\ndef build_demo():\\n    with gr.Blocks() as demo:\\n        gr.Markdown(\"# vLLM text completion demo\\\\n\")\\n        inputbox = gr.Textbox(label=\"Input\",\\n                              placeholder=\"Enter text and press ENTER\")\\n        outputbox = gr.Textbox(label=\"Output\",\\n                               placeholder=\"Generated result from the model\")\\n        inputbox.submit(http_bot, [inputbox], [outputbox])\\n    return demo\\n\\n\\nif __name__ == \"__main__\":\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\\n    parser.add_argument(\"--port\", type=int, default=8001)\\n    parser.add_argument(\"--model-url\",\\n                        type=str,\\n                        default=\"http://localhost:8000/generate\")\\n    args = parser.parse_args()\\n\\n    demo = build_demo()\\n    demo.queue(concurrency_count=100).launch(server_name=args.host,\\n                                             server_port=args.port,\\n                                             share=True)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nexamples/offline_inference.py\\n---------\\nContent:\\nfrom vllm import LLM, SamplingParams\\n\\n# Sample prompts.\\nprompts = [\\n    \"Hello, my name is\",\\n    \"The president of the United States is\",\\n    \"The capital of France is\",\\n    \"The future of AI is\",\\n]\\n# Create a sampling params object.\\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\\n\\n# Create an LLM.\\nllm = LLM(model=\"facebook/opt-125m\")\\n# Generate texts from the prompts. The output is a list of RequestOutput objects\\n# that contain the prompt, generated text, and other information.\\noutputs = llm.generate(prompts, sampling_params)\\n# Print the outputs.\\nfor output in outputs:\\n    prompt = output.prompt\\n    generated_text = output.outputs[0].text\\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\\n<<End File>>\\n<<Begin file>>\\nPath:\\nexamples/openai_chatcompletion_client.py\\n---------\\nContent:\\nimport openai\\n\\n# Modify OpenAI\\'s API key and API base to use vLLM\\'s API server.\\nopenai.api_key = \"EMPTY\"\\nopenai.api_base = \"http://localhost:8000/v1\"\\n\\n# List models API\\nmodels = openai.Model.list()\\nprint(\"Models:\", models)\\n\\nmodel = models[\"data\"][0][\"id\"]\\n\\n# Chat completion API\\nchat_completion = openai.ChatCompletion.create(\\n    model=model,\\n    messages=[{\\n        \"role\": \"system\",\\n        \"content\": \"You are a helpful assistant.\"\\n    }, {\\n        \"role\": \"user\",\\n        \"content\": \"Who won the world series in 2020?\"\\n    }, {\\n        \"role\":\\n        \"assistant\",\\n        \"content\":\\n        \"The Los Angeles Dodgers won the World Series in 2020.\"\\n    }, {\\n        \"role\": \"user\",\\n        \"content\": \"Where was it played?\"\\n    }])\\n\\nprint(\"Chat completion results:\")\\nprint(chat_completion)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nexamples/llm_engine_example.py\\n---------\\nContent:\\nimport argparse\\n\\nfrom vllm import EngineArgs, LLMEngine, SamplingParams\\n\\n\\ndef main(args: argparse.Namespace):\\n    # Parse the CLI argument and initialize the engine.\\n    engine_args = EngineArgs.from_cli_args(args)\\n    engine = LLMEngine.from_engine_args(engine_args)\\n\\n    # Test the following prompts.\\n    test_prompts = [\\n        (\"A robot may not injure a human being\",\\n         SamplingParams(temperature=0.0)),\\n        (\"To be or not to be,\",\\n         SamplingParams(temperature=0.8, top_k=5, presence_penalty=0.2)),\\n        (\"What is the meaning of life?\",\\n         SamplingParams(n=2,\\n                        best_of=5,\\n                        temperature=0.8,\\n                        top_p=0.95,\\n                        frequency_penalty=0.1)),\\n        (\"It is only with the heart that one can see rightly\",\\n         SamplingParams(n=3, best_of=3, use_beam_search=True,\\n                        temperature=0.0)),\\n    ]\\n\\n    # Run the engine by calling `engine.step()` manually.\\n    request_id = 0\\n    while True:\\n        # To test continuous batching, we add one request at each step.\\n        if test_prompts:\\n            prompt, sampling_params = test_prompts.pop(0)\\n            engine.add_request(str(request_id), prompt, sampling_params)\\n            request_id += 1\\n\\n        request_outputs = engine.step()\\n        for request_output in request_outputs:\\n            if request_output.finished:\\n                print(request_output)\\n\\n        if not (engine.has_unfinished_requests() or test_prompts):\\n            break\\n\\n\\nif __name__ == \\'__main__\\':\\n    parser = argparse.ArgumentParser(\\n        description=\\'Demo on using the LLMEngine class directly\\')\\n    parser = EngineArgs.add_cli_args(parser)\\n    args = parser.parse_args()\\n    main(args)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nexamples/openai_completion_client.py\\n---------\\nContent:\\nimport openai\\n\\n# Modify OpenAI\\'s API key and API base to use vLLM\\'s API server.\\nopenai.api_key = \"EMPTY\"\\nopenai.api_base = \"http://localhost:8000/v1\"\\n\\n# List models API\\nmodels = openai.Model.list()\\nprint(\"Models:\", models)\\n\\nmodel = models[\"data\"][0][\"id\"]\\n\\n# Completion API\\nstream = False\\ncompletion = openai.Completion.create(\\n    model=model,\\n    prompt=\"A robot may not injure a human being\",\\n    echo=False,\\n    n=2,\\n    stream=stream,\\n    logprobs=3)\\n\\nprint(\"Completion results:\")\\nif stream:\\n    for c in completion:\\n        print(c)\\nelse:\\n    print(completion)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nexamples/api_client.py\\n---------\\nContent:\\n\"\"\"Example Python client for vllm.entrypoints.api_server\"\"\"\\n\\nimport argparse\\nimport json\\nfrom typing import Iterable, List\\n\\nimport requests\\n\\n\\ndef clear_line(n: int = 1) -> None:\\n    LINE_UP = \\'\\\\033[1A\\'\\n    LINE_CLEAR = \\'\\\\x1b[2K\\'\\n    for _ in range(n):\\n        print(LINE_UP, end=LINE_CLEAR, flush=True)\\n\\n\\ndef post_http_request(prompt: str,\\n                      api_url: str,\\n                      n: int = 1,\\n                      stream: bool = False) -> requests.Response:\\n    headers = {\"User-Agent\": \"Test Client\"}\\n    pload = {\\n        \"prompt\": prompt,\\n        \"n\": n,\\n        \"use_beam_search\": True,\\n        \"temperature\": 0.0,\\n        \"max_tokens\": 16,\\n        \"stream\": stream,\\n    }\\n    response = requests.post(api_url, headers=headers, json=pload, stream=True)\\n    return response\\n\\n\\ndef get_streaming_response(response: requests.Response) -> Iterable[List[str]]:\\n    for chunk in response.iter_lines(chunk_size=8192,\\n                                     decode_unicode=False,\\n                                     delimiter=b\"\\\\0\"):\\n        if chunk:\\n            data = json.loads(chunk.decode(\"utf-8\"))\\n            output = data[\"text\"]\\n            yield output\\n\\n\\ndef get_response(response: requests.Response) -> List[str]:\\n    data = json.loads(response.content)\\n    output = data[\"text\"]\\n    return output\\n\\n\\nif __name__ == \"__main__\":\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\\n    parser.add_argument(\"--port\", type=int, default=8000)\\n    parser.add_argument(\"--n\", type=int, default=4)\\n    parser.add_argument(\"--prompt\", type=str, default=\"San Francisco is a\")\\n    parser.add_argument(\"--stream\", action=\"store_true\")\\n    args = parser.parse_args()\\n    prompt = args.prompt\\n    api_url = f\"http://{args.host}:{args.port}/generate\"\\n    n = args.n\\n    stream = args.stream\\n\\n    print(f\"Prompt: {prompt!r}\\\\n\", flush=True)\\n    response = post_http_request(prompt, api_url, n, stream)\\n\\n    if stream:\\n        num_printed_lines = 0\\n        for h in get_streaming_response(response):\\n            clear_line(num_printed_lines)\\n            num_printed_lines = 0\\n            for i, line in enumerate(h):\\n                num_printed_lines += 1\\n                print(f\"Beam candidate {i}: {line!r}\", flush=True)\\n    else:\\n        output = get_response(response)\\n        for i, line in enumerate(output):\\n            print(f\"Beam candidate {i}: {line!r}\", flush=True)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nbenchmarks/benchmark_throughput.py\\n---------\\nContent:\\n\"\"\"Benchmark offline inference throughput.\"\"\"\\nimport argparse\\nimport json\\nimport random\\nimport time\\nfrom typing import List, Optional, Tuple\\n\\nimport torch\\nfrom transformers import AutoModelForCausalLM, PreTrainedTokenizerBase\\nfrom tqdm import tqdm\\n\\nfrom vllm import LLM, SamplingParams\\nfrom vllm.transformers_utils.tokenizer import get_tokenizer\\n\\n\\ndef sample_requests(\\n    dataset_path: str,\\n    num_requests: int,\\n    tokenizer: PreTrainedTokenizerBase,\\n) -> List[Tuple[str, int, int]]:\\n    # Load the dataset.\\n    with open(dataset_path) as f:\\n        dataset = json.load(f)\\n    # Filter out the conversations with less than 2 turns.\\n    dataset = [data for data in dataset if len(data[\"conversations\"]) >= 2]\\n    # Only keep the first two turns of each conversation.\\n    dataset = [(data[\"conversations\"][0][\"value\"],\\n                data[\"conversations\"][1][\"value\"]) for data in dataset]\\n\\n    # Tokenize the prompts and completions.\\n    prompts = [prompt for prompt, _ in dataset]\\n    prompt_token_ids = tokenizer(prompts).input_ids\\n    completions = [completion for _, completion in dataset]\\n    completion_token_ids = tokenizer(completions).input_ids\\n    tokenized_dataset = []\\n    for i in range(len(dataset)):\\n        output_len = len(completion_token_ids[i])\\n        tokenized_dataset.append((prompts[i], prompt_token_ids[i], output_len))\\n\\n    # Filter out too long sequences.\\n    filtered_dataset: List[Tuple[str, int, int]] = []\\n    for prompt, prompt_token_ids, output_len in tokenized_dataset:\\n        prompt_len = len(prompt_token_ids)\\n        if prompt_len < 4 or output_len < 4:\\n            # Prune too short sequences.\\n            continue\\n        if prompt_len > 1024 or prompt_len + output_len > 2048:\\n            # Prune too long sequences.\\n            continue\\n        filtered_dataset.append((prompt, prompt_len, output_len))\\n\\n    # Sample the requests.\\n    sampled_requests = random.sample(filtered_dataset, num_requests)\\n    return sampled_requests\\n\\n\\ndef run_vllm(\\n    requests: List[Tuple[str, int, int]],\\n    model: str,\\n    tokenizer: str,\\n    quantization: Optional[str],\\n    tensor_parallel_size: int,\\n    seed: int,\\n    n: int,\\n    use_beam_search: bool,\\n    trust_remote_code: bool,\\n) -> float:\\n    llm = LLM(\\n        model=model,\\n        tokenizer=tokenizer,\\n        quantization=quantization,\\n        tensor_parallel_size=tensor_parallel_size,\\n        seed=seed,\\n        trust_remote_code=trust_remote_code,\\n    )\\n\\n    # Add the requests to the engine.\\n    for prompt, _, output_len in requests:\\n        sampling_params = SamplingParams(\\n            n=n,\\n            temperature=0.0 if use_beam_search else 1.0,\\n            top_p=1.0,\\n            use_beam_search=use_beam_search,\\n            ignore_eos=True,\\n            max_tokens=output_len,\\n        )\\n        # FIXME(woosuk): Do not use internal method.\\n        llm._add_request(\\n            prompt=prompt,\\n            prompt_token_ids=None,\\n            sampling_params=sampling_params,\\n        )\\n\\n    start = time.time()\\n    # FIXME(woosuk): Do use internal method.\\n    llm._run_engine(use_tqdm=True)\\n    end = time.time()\\n    return end - start\\n\\n\\ndef run_hf(\\n    requests: List[Tuple[str, int, int]],\\n    model: str,\\n    tokenizer: PreTrainedTokenizerBase,\\n    n: int,\\n    use_beam_search: bool,\\n    max_batch_size: int,\\n    trust_remote_code: bool,\\n) -> float:\\n    assert not use_beam_search\\n    llm = AutoModelForCausalLM.from_pretrained(\\n        model, torch_dtype=torch.float16, trust_remote_code=trust_remote_code)\\n    if llm.config.model_type == \"llama\":\\n        # To enable padding in the HF backend.\\n        tokenizer.pad_token = tokenizer.eos_token\\n    llm = llm.cuda()\\n\\n    pbar = tqdm(total=len(requests))\\n    start = time.time()\\n    batch: List[str] = []\\n    max_prompt_len = 0\\n    max_output_len = 0\\n    for i in range(len(requests)):\\n        prompt, prompt_len, output_len = requests[i]\\n        # Add the prompt to the batch.\\n        batch.append(prompt)\\n        max_prompt_len = max(max_prompt_len, prompt_len)\\n        max_output_len = max(max_output_len, output_len)\\n        if len(batch) < max_batch_size and i != len(requests) - 1:\\n            # Check if we can add more requests to the batch.\\n            _, next_prompt_len, next_output_len = requests[i + 1]\\n            if (max(max_prompt_len, next_prompt_len) +\\n                    max(max_output_len, next_output_len)) <= 2048:\\n                # We can add more requests to the batch.\\n                continue\\n\\n        # Generate the sequences.\\n        input_ids = tokenizer(batch, return_tensors=\"pt\",\\n                              padding=True).input_ids\\n        llm_outputs = llm.generate(\\n            input_ids=input_ids.cuda(),\\n            do_sample=not use_beam_search,\\n            num_return_sequences=n,\\n            temperature=1.0,\\n            top_p=1.0,\\n            use_cache=True,\\n            max_new_tokens=max_output_len,\\n        )\\n        # Include the decoding time.\\n        tokenizer.batch_decode(llm_outputs, skip_special_tokens=True)\\n        pbar.update(len(batch))\\n\\n        # Clear the batch.\\n        batch = []\\n        max_prompt_len = 0\\n        max_output_len = 0\\n    end = time.time()\\n    return end - start\\n\\n\\ndef main(args: argparse.Namespace):\\n    print(args)\\n    random.seed(args.seed)\\n\\n    # Sample the requests.\\n    tokenizer = get_tokenizer(args.tokenizer,\\n                              trust_remote_code=args.trust_remote_code)\\n    requests = sample_requests(args.dataset, args.num_prompts, tokenizer)\\n\\n    if args.backend == \"vllm\":\\n        elapsed_time = run_vllm(requests, args.model, args.tokenizer,\\n                                args.quantization, args.tensor_parallel_size,\\n                                args.seed, args.n, args.use_beam_search,\\n                                args.trust_remote_code)\\n    elif args.backend == \"hf\":\\n        assert args.tensor_parallel_size == 1\\n        elapsed_time = run_hf(requests, args.model, tokenizer, args.n,\\n                              args.use_beam_search, args.hf_max_batch_size,\\n                              args.trust_remote_code)\\n    else:\\n        raise ValueError(f\"Unknown backend: {args.backend}\")\\n    total_num_tokens = sum(prompt_len + output_len\\n                           for _, prompt_len, output_len in requests)\\n    print(f\"Throughput: {len(requests) / elapsed_time:.2f} requests/s, \"\\n          f\"{total_num_tokens / elapsed_time:.2f} tokens/s\")\\n\\n\\nif __name__ == \"__main__\":\\n    parser = argparse.ArgumentParser(description=\"Benchmark the throughput.\")\\n    parser.add_argument(\"--backend\",\\n                        type=str,\\n                        choices=[\"vllm\", \"hf\"],\\n                        default=\"vllm\")\\n    parser.add_argument(\"--dataset\",\\n                        type=str,\\n                        required=True,\\n                        help=\"Path to the dataset.\")\\n    parser.add_argument(\"--model\", type=str, default=\"facebook/opt-125m\")\\n    parser.add_argument(\"--tokenizer\", type=str, default=None)\\n    parser.add_argument(\\'--quantization\\',\\n                        \\'-q\\',\\n                        choices=[\\'awq\\', None],\\n                        default=None)\\n    parser.add_argument(\"--tensor-parallel-size\", \"-tp\", type=int, default=1)\\n    parser.add_argument(\"--n\",\\n                        type=int,\\n                        default=1,\\n                        help=\"Number of generated sequences per prompt.\")\\n    parser.add_argument(\"--use-beam-search\", action=\"store_true\")\\n    parser.add_argument(\"--num-prompts\",\\n                        type=int,\\n                        default=1000,\\n                        help=\"Number of prompts to process.\")\\n    parser.add_argument(\"--seed\", type=int, default=0)\\n    parser.add_argument(\"--hf-max-batch-size\",\\n                        type=int,\\n                        default=None,\\n                        help=\"Maximum batch size for HF backend.\")\\n    parser.add_argument(\\'--trust-remote-code\\',\\n                        action=\\'store_true\\',\\n                        help=\\'trust remote code from huggingface\\')\\n    args = parser.parse_args()\\n\\n    if args.backend == \"vllm\":\\n        if args.hf_max_batch_size is not None:\\n            raise ValueError(\"HF max batch size is only for HF backend.\")\\n    elif args.backend == \"hf\":\\n        if args.hf_max_batch_size is None:\\n            raise ValueError(\"HF max batch size is required for HF backend.\")\\n        if args.quantization is not None:\\n            raise ValueError(\"Quantization is only for vLLM backend.\")\\n    if args.tokenizer is None:\\n        args.tokenizer = args.model\\n\\n    main(args)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nbenchmarks/benchmark_latency.py\\n---------\\nContent:\\n\"\"\"Benchmark the latency of processing a single batch of requests.\"\"\"\\nimport argparse\\nimport time\\n\\nimport numpy as np\\nimport torch\\nfrom tqdm import tqdm\\n\\nfrom vllm import LLM, SamplingParams\\n\\n\\ndef main(args: argparse.Namespace):\\n    print(args)\\n\\n    # Process all the requests in a single batch if possible.\\n    # NOTE(woosuk): If the request cannot be processed in a single batch,\\n    # the engine will automatically process the request in multiple batches.\\n    llm = LLM(\\n        model=args.model,\\n        tokenizer=args.tokenizer,\\n        quantization=args.quantization,\\n        tensor_parallel_size=args.tensor_parallel_size,\\n        max_num_seqs=args.batch_size,\\n        max_num_batched_tokens=args.batch_size * args.input_len,\\n        trust_remote_code=args.trust_remote_code,\\n    )\\n\\n    sampling_params = SamplingParams(\\n        n=args.n,\\n        temperature=0.0 if args.use_beam_search else 1.0,\\n        top_p=1.0,\\n        use_beam_search=args.use_beam_search,\\n        ignore_eos=True,\\n        max_tokens=args.output_len,\\n    )\\n    print(sampling_params)\\n    dummy_prompt_token_ids = [[0] * args.input_len] * args.batch_size\\n\\n    def run_to_completion(profile: bool = False):\\n        if profile:\\n            torch.cuda.cudart().cudaProfilerStart()\\n        start_time = time.time()\\n\\n        llm.generate(prompt_token_ids=dummy_prompt_token_ids,\\n                     sampling_params=sampling_params,\\n                     use_tqdm=False)\\n\\n        end_time = time.time()\\n        latency = end_time - start_time\\n        if profile:\\n            torch.cuda.cudart().cudaProfilerStop()\\n        return latency\\n\\n    print(\"Warming up...\")\\n    run_to_completion(profile=False)\\n\\n    # Benchmark.\\n    latencies = []\\n    for _ in tqdm(range(args.num_iters), desc=\"Profiling iterations\"):\\n        latencies.append(run_to_completion(profile=False))\\n    print(f\\'Avg latency: {np.mean(latencies)} seconds\\')\\n\\n\\nif __name__ == \\'__main__\\':\\n    parser = argparse.ArgumentParser(\\n        description=\\'Benchmark the latency of processing a single batch of \\'\\n        \\'requests till completion.\\')\\n    parser.add_argument(\\'--model\\', type=str, default=\\'facebook/opt-125m\\')\\n    parser.add_argument(\\'--tokenizer\\', type=str, default=None)\\n    parser.add_argument(\\'--quantization\\',\\n                        \\'-q\\',\\n                        choices=[\\'awq\\', None],\\n                        default=None)\\n    parser.add_argument(\\'--tensor-parallel-size\\', \\'-tp\\', type=int, default=1)\\n    parser.add_argument(\\'--input-len\\', type=int, default=32)\\n    parser.add_argument(\\'--output-len\\', type=int, default=128)\\n    parser.add_argument(\\'--batch-size\\', type=int, default=8)\\n    parser.add_argument(\\'--n\\',\\n                        type=int,\\n                        default=1,\\n                        help=\\'Number of generated sequences per prompt.\\')\\n    parser.add_argument(\\'--use-beam-search\\', action=\\'store_true\\')\\n    parser.add_argument(\\'--num-iters\\',\\n                        type=int,\\n                        default=3,\\n                        help=\\'Number of iterations to run.\\')\\n    parser.add_argument(\\'--trust-remote-code\\',\\n                        action=\\'store_true\\',\\n                        help=\\'trust remote code from huggingface\\')\\n    args = parser.parse_args()\\n    main(args)\\n<<End File>>\\n<<Begin file>>\\nPath:\\nbenchmarks/benchmark_serving.py\\n---------\\nContent:\\n\"\"\"Benchmark online serving throughput.\\n\\nOn the server side, run one of the following commands:\\n    (vLLM backend)\\n    python -m vllm.entrypoints.api_server \\\\\\n        --model <your_model> --swap-space 16 \\\\\\n        --disable-log-requests\\n\\n    (TGI backend)\\n    ./launch_hf_server.sh <your_model>\\n\\nOn the client side, run:\\n    python benchmarks/benchmark_serving.py \\\\\\n        --backend <backend> \\\\\\n        --tokenizer <your_model> --dataset <target_dataset> \\\\\\n        --request-rate <request_rate>\\n\"\"\"\\nimport argparse\\nimport asyncio\\nimport json\\nimport random\\nimport time\\nfrom typing import AsyncGenerator, List, Tuple\\n\\nimport aiohttp\\nimport numpy as np\\nfrom transformers import PreTrainedTokenizerBase\\nfrom vllm.transformers_utils.tokenizer import get_tokenizer\\n\\n# (prompt len, output len, latency)\\nREQUEST_LATENCY: List[Tuple[int, int, float]] = []\\n\\n\\ndef sample_requests(\\n    dataset_path: str,\\n    num_requests: int,\\n    tokenizer: PreTrainedTokenizerBase,\\n) -> List[Tuple[str, int, int]]:\\n    # Load the dataset.\\n    with open(dataset_path) as f:\\n        dataset = json.load(f)\\n    # Filter out the conversations with less than 2 turns.\\n    dataset = [\\n        data for data in dataset\\n        if len(data[\"conversations\"]) >= 2\\n    ]\\n    # Only keep the first two turns of each conversation.\\n    dataset = [\\n        (data[\"conversations\"][0][\"value\"], data[\"conversations\"][1][\"value\"])\\n        for data in dataset\\n    ]\\n\\n    # Tokenize the prompts and completions.\\n    prompts = [prompt for prompt, _ in dataset]\\n    prompt_token_ids = tokenizer(prompts).input_ids\\n    completions = [completion for _, completion in dataset]\\n    completion_token_ids = tokenizer(completions).input_ids\\n    tokenized_dataset = []\\n    for i in range(len(dataset)):\\n        output_len = len(completion_token_ids[i])\\n        tokenized_dataset.append((prompts[i], prompt_token_ids[i], output_len))\\n\\n    # Filter out too long sequences.\\n    filtered_dataset: List[Tuple[str, int, int]] = []\\n    for prompt, prompt_token_ids, output_len in tokenized_dataset:\\n        prompt_len = len(prompt_token_ids)\\n        if prompt_len < 4 or output_len < 4:\\n            # Prune too short sequences.\\n            # This is because TGI causes errors when the input or output length\\n            # is too short.\\n            continue\\n        if prompt_len > 1024 or prompt_len + output_len > 2048:\\n            # Prune too long sequences.\\n            continue\\n        filtered_dataset.append((prompt, prompt_len, output_len))\\n\\n    # Sample the requests.\\n    sampled_requests = random.sample(filtered_dataset, num_requests)\\n    return sampled_requests\\n\\n\\nasync def get_request(\\n    input_requests: List[Tuple[str, int, int]],\\n    request_rate: float,\\n) -> AsyncGenerator[Tuple[str, int, int], None]:\\n    input_requests = iter(input_requests)\\n    for request in input_requests:\\n        yield request\\n\\n        if request_rate == float(\"inf\"):\\n            # If the request rate is infinity, then we don\\'t need to wait.\\n            continue\\n        # Sample the request interval from the exponential distribution.\\n        interval = np.random.exponential(1.0 / request_rate)\\n        # The next request will be sent after the interval.\\n        await asyncio.sleep(interval)\\n\\n\\nasync def send_request(\\n    backend: str,\\n    api_url: str,\\n    prompt: str,\\n    prompt_len: int,\\n    output_len: int,\\n    best_of: int,\\n    use_beam_search: bool,\\n) -> None:\\n    request_start_time = time.time()\\n\\n    headers = {\"User-Agent\": \"Benchmark Client\"}\\n    if backend == \"vllm\":\\n        pload = {\\n            \"prompt\": prompt,\\n            \"n\": 1,\\n            \"best_of\": best_of,\\n            \"use_beam_search\": use_beam_search,\\n            \"temperature\": 0.0 if use_beam_search else 1.0,\\n            \"top_p\": 1.0,\\n            \"max_tokens\": output_len,\\n            \"ignore_eos\": True,\\n            \"stream\": False,\\n        }\\n    elif backend == \"tgi\":\\n        assert not use_beam_search\\n        params = {\\n            \"best_of\": best_of,\\n            \"max_new_tokens\": output_len,\\n            \"do_sample\": True,\\n        }\\n        pload = {\\n            \"inputs\": prompt,\\n            \"parameters\": params,\\n        }\\n    else:\\n        raise ValueError(f\"Unknown backend: {backend}\")\\n\\n    timeout = aiohttp.ClientTimeout(total=3 * 3600)\\n    async with aiohttp.ClientSession(timeout=timeout) as session:\\n        while True:\\n            async with session.post(api_url, headers=headers, json=pload) as response:\\n                chunks = []\\n                async for chunk, _ in response.content.iter_chunks():\\n                    chunks.append(chunk)\\n            output = b\"\".join(chunks).decode(\"utf-8\")\\n            output = json.loads(output)\\n\\n            # Re-send the request if it failed.\\n            if \"error\" not in output:\\n                break\\n\\n    request_end_time = time.time()\\n    request_latency = request_end_time - request_start_time\\n    REQUEST_LATENCY.append((prompt_len, output_len, request_latency))\\n\\n\\nasync def benchmark(\\n    backend: str,\\n    api_url: str,\\n    input_requests: List[Tuple[str, int, int]],\\n    best_of: int,\\n    use_beam_search: bool,\\n    request_rate: float,\\n) -> None:\\n    tasks: List[asyncio.Task] = []\\n    async for request in get_request(input_requests, request_rate):\\n        prompt, prompt_len, output_len = request\\n        task = asyncio.create_task(send_request(backend, api_url, prompt,\\n                                                prompt_len, output_len,\\n                                                best_of, use_beam_search))\\n        tasks.append(task)\\n    await asyncio.gather(*tasks)\\n\\n\\ndef main(args: argparse.Namespace):\\n    print(args)\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n\\n    api_url = f\"http://{args.host}:{args.port}/generate\"\\n    tokenizer = get_tokenizer(args.tokenizer, trust_remote_code=args.trust_remote_code)\\n    input_requests = sample_requests(args.dataset, args.num_prompts, tokenizer)\\n\\n    benchmark_start_time = time.time()\\n    asyncio.run(benchmark(args.backend, api_url, input_requests, args.best_of,\\n                          args.use_beam_search, args.request_rate))\\n    benchmark_end_time = time.time()\\n    benchmark_time = benchmark_end_time - benchmark_start_time\\n    print(f\"Total time: {benchmark_time:.2f} s\")\\n    print(f\"Throughput: {args.num_prompts / benchmark_time:.2f} requests/s\")\\n\\n    # Compute the latency statistics.\\n    avg_latency = np.mean([latency for _, _, latency in REQUEST_LATENCY])\\n    print(f\"Average latency: {avg_latency:.2f} s\")\\n    avg_per_token_latency = np.mean([\\n        latency / (prompt_len + output_len)\\n        for prompt_len, output_len, latency in REQUEST_LATENCY\\n    ])\\n    print(f\"Average latency per token: {avg_per_token_latency:.2f} s\")\\n    avg_per_output_token_latency = np.mean([\\n        latency / output_len\\n        for _, output_len, latency in REQUEST_LATENCY\\n    ])\\n    print(\"Average latency per output token: \"\\n          f\"{avg_per_output_token_latency:.2f} s\")\\n\\n\\nif __name__ == \"__main__\":\\n    parser = argparse.ArgumentParser(\\n        description=\"Benchmark the online serving throughput.\")\\n    parser.add_argument(\"--backend\", type=str, default=\"vllm\",\\n                        choices=[\"vllm\", \"tgi\"])\\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\\n    parser.add_argument(\"--port\", type=int, default=8000)\\n    parser.add_argument(\"--dataset\", type=str, required=True,\\n                        help=\"Path to the dataset.\")\\n    parser.add_argument(\"--tokenizer\", type=str, required=True,\\n                        help=\"Name or path of the tokenizer.\")\\n    parser.add_argument(\"--best-of\", type=int, default=1,\\n                        help=\"Generates `best_of` sequences per prompt and \"\\n                             \"returns the best one.\")\\n    parser.add_argument(\"--use-beam-search\", action=\"store_true\")\\n    parser.add_argument(\"--num-prompts\", type=int, default=1000,\\n                        help=\"Number of prompts to process.\")\\n    parser.add_argument(\"--request-rate\", type=float, default=float(\"inf\"),\\n                        help=\"Number of requests per second. If this is inf, \"\\n                             \"then all the requests are sent at time 0. \"\\n                             \"Otherwise, we use Poisson process to synthesize \"\\n                             \"the request arrival times.\")\\n    parser.add_argument(\"--seed\", type=int, default=0)\\n    parser.add_argument(\\'--trust-remote-code\\', action=\\'store_true\\',\\n                        help=\\'trust remote code from huggingface\\')\\n    args = parser.parse_args()\\n    main(args)\\n<<End File>>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92c3d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenizer.encode(raw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7773e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLLM .py Total tokens: 0.183235M\n"
     ]
    }
   ],
   "source": [
    "tokens = len(tokenized_data)\n",
    "print(f\"VLLM .py Total tokens: {tokens/1_000_000}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1241354b",
   "metadata": {},
   "source": [
    "That's not a lot of tokens :P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c487645d",
   "metadata": {},
   "source": [
    "## Save to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa2b4dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tcapelle/work/vllm_llm/wandb/run-20230918_212420-i4lcpbvk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/capecape/vllm_llm/runs/i4lcpbvk' target=\"_blank\">golden-water-6</a></strong> to <a href='https://wandb.ai/capecape/vllm_llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/capecape/vllm_llm' target=\"_blank\">https://wandb.ai/capecape/vllm_llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/capecape/vllm_llm/runs/i4lcpbvk' target=\"_blank\">https://wandb.ai/capecape/vllm_llm/runs/i4lcpbvk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682e4042e68d4751a31e72cc6ab1bcaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.645 MB of 0.660 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.978074"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">golden-water-6</strong> at: <a href='https://wandb.ai/capecape/vllm_llm/runs/i4lcpbvk' target=\"_blank\">https://wandb.ai/capecape/vllm_llm/runs/i4lcpbvk</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230918_212420-i4lcpbvk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "with wandb.init(project=\"vllm_llm\"):\n",
    "    at = wandb.Artifact(name=\"vllm_python\", \n",
    "                        description=\"The .py files from the vllm library\",\n",
    "                        type=\"dataset\",\n",
    "                        metadata={\n",
    "                            \"url\": \"https://github.com/vllm-project/vllm.git\",\n",
    "                            \"commit\":last_commit,\n",
    "                            \"remote\": vllm_repo.remote().url,\n",
    "                            \"tokens\": tokens})\n",
    "    at.add_file(\"vllm_python.jsonl\")\n",
    "    \n",
    "    wandb.log_artifact(at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3fd120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
